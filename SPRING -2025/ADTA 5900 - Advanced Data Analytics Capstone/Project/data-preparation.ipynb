{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Dict, List, Tuple\n",
    "import ta\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFinancialPreprocessor:\n",
    "    def __init__(self, sequence_length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with your dataset's fields plus additional features\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scalers = {}\n",
    "        \n",
    "        # Feature groups based on your actual data fields\n",
    "        self.feature_groups = {\n",
    "            'price': [\n",
    "                'Open', 'High', 'Low', 'Close', 'Adj Close',\n",
    "                'Returns', 'True_Range', 'ATR'\n",
    "            ],\n",
    "            \n",
    "            'moving_averages': [\n",
    "                'MA20',\n",
    "                'MA50',  # Will be calculated\n",
    "                'MA200', # Will be calculated\n",
    "                'EMA20', # Will be calculated\n",
    "            ],\n",
    "            \n",
    "            'volatility': [\n",
    "                '20dSTD',\n",
    "                'Upper_Band',\n",
    "                'Lower_Band',\n",
    "                'BB_Width',    # Will be calculated\n",
    "                'Volatility',\n",
    "                'ATR',         # Will be calculated\n",
    "            ],\n",
    "            \n",
    "            'momentum': [\n",
    "                'RSI',\n",
    "                'MACD',\n",
    "                'Signal_Line',\n",
    "                'MACD_Histogram',  # Will be calculated\n",
    "                'ROC',            # Will be calculated\n",
    "                'Momentum'        # Will be calculated\n",
    "            ],\n",
    "            \n",
    "            'volume': [\n",
    "                'Volume',\n",
    "                'Volume_MA',\n",
    "                'Volume_Ratio',\n",
    "                'OBV'            # Will be calculated\n",
    "            ],\n",
    "            \n",
    "            'events': [\n",
    "                'Dividends',\n",
    "                'Stock Splits'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def calculate_additional_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate additional technical indicators not in original data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            \n",
    "            # 1. Additional Moving Averages\n",
    "            df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "            df['MA200'] = df['Close'].rolling(window=200).mean()\n",
    "            df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "            \n",
    "            # 2. Bollinger Band Width\n",
    "            df['BB_Width'] = (df['Upper_Band'] - df['Lower_Band']) / df['MA20']\n",
    "            \n",
    "            # 3. Average True Range (ATR)\n",
    "            high_low = df['High'] - df['Low']\n",
    "            high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "            low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "            df['True_Range'] = np.maximum(high_low, np.maximum(high_close, low_close))\n",
    "            df['ATR'] = df['True_Range'].rolling(window=14).mean()\n",
    "            \n",
    "            # 4. MACD Histogram\n",
    "            df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "            \n",
    "            # 5. Rate of Change (ROC)\n",
    "            df['ROC'] = ((df['Close'] - df['Close'].shift(10)) / \n",
    "                        df['Close'].shift(10)) * 100\n",
    "            \n",
    "            # 6. Momentum\n",
    "            df['Momentum'] = df['Close'] - df['Close'].shift(10)\n",
    "            \n",
    "            # 7. On Balance Volume (OBV)\n",
    "            df['OBV'] = np.where(df['Close'] > df['Close'].shift(),\n",
    "                                df['Volume'],\n",
    "                                np.where(df['Close'] < df['Close'].shift(),\n",
    "                                        -df['Volume'], 0)).cumsum()\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating additional features: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalize features based on their characteristics\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Price normalization\n",
    "        price_features = self.feature_groups['price']\n",
    "        self.scalers['price'] = MinMaxScaler()\n",
    "        df[price_features] = self.scalers['price'].fit_transform(df[price_features])\n",
    "        \n",
    "        # Volume normalization (log transform then scale)\n",
    "        volume_features = ['Volume', 'Volume_MA', 'OBV']\n",
    "        df[volume_features] = np.log1p(df[volume_features])\n",
    "        self.scalers['volume'] = MinMaxScaler()\n",
    "        df[volume_features] = self.scalers['volume'].fit_transform(df[volume_features])\n",
    "        \n",
    "        # Note: RSI, MACD, etc. are already normalized\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, df: pd.DataFrame, train_split: float = 0.8) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete data preparation pipeline\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Ensure date is index\n",
    "\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df = df.set_index('Date')\n",
    "            \n",
    "            # 2. Calculate additional features\n",
    "            df = self.calculate_additional_features(df)\n",
    "            \n",
    "            # 3. Handle missing values\n",
    "            df = self.handle_missing_values(df)\n",
    "            \n",
    "            # 4. Normalize features\n",
    "            df = self.normalize_features(df)\n",
    "            \n",
    "            # 5. Create sequences\n",
    "            X, y = self.create_sequences(df)\n",
    "            \n",
    "            # 6. Train/validation split\n",
    "            split_idx = int(len(X) * train_split)\n",
    "            \n",
    "            return {\n",
    "                'train': {\n",
    "                    'X': X[:split_idx],\n",
    "                    'y': y[:split_idx]\n",
    "                },\n",
    "                'val': {\n",
    "                    'X': X[split_idx:],\n",
    "                    'y': y[split_idx:]\n",
    "                },\n",
    "                'scalers': self.scalers,\n",
    "                'feature_groups': self.feature_groups\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in data preparation: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values based on feature type\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Forward fill price and MA data\n",
    "        price_ma_features = (self.feature_groups['price'] + \n",
    "                           self.feature_groups['moving_averages'])\n",
    "        df[price_ma_features] = df[price_ma_features].fillna(method='ffill')\n",
    "        \n",
    "        # Zero fill volume data\n",
    "        df[self.feature_groups['volume']] = df[self.feature_groups['volume']].fillna(0)\n",
    "        \n",
    "        # Forward fill other indicators\n",
    "        df = df.fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sequences for CNN-LSTM model\n",
    "        \"\"\"\n",
    "        # Get all features except events\n",
    "        features = [feat for group, feats in self.feature_groups.items() \n",
    "                   if group != 'events' for feat in feats]\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(df) - self.sequence_length):\n",
    "            sequence = df[features].iloc[i:(i + self.sequence_length)]\n",
    "            target = df['Returns'].iloc[i + self.sequence_length]\n",
    "            \n",
    "            X.append(sequence.values)\n",
    "            y.append(target)\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_data():\n",
    "    \"\"\"\n",
    "    Load and analyze the SP500 master data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        logger.info(\"Loading SP500 master data...\")\n",
    "        df = pd.read_csv('sp500_master_data.csv')\n",
    "        \n",
    "        # Convert date to datetime and handle timezone\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "        \n",
    "        # Basic data analysis\n",
    "        logger.info(\"\\nDataset Overview:\")\n",
    "        logger.info(f\"Total rows: {len(df)}\")\n",
    "        logger.info(f\"Total columns: {len(df.columns)}\")\n",
    "        logger.info(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "        logger.info(f\"Number of unique stocks: {df['Symbol'].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def prepare_stock_data(df: pd.DataFrame, symbol: str = None):\n",
    "    \"\"\"\n",
    "    Prepare data for a single stock or all stocks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter for specific stock if provided\n",
    "        if symbol:\n",
    "            df = df[df['Symbol'] == symbol].copy()\n",
    "            logger.info(f\"\\nPreparing data for {symbol}\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                raise ValueError(f\"No data found for symbol {symbol}\")\n",
    "        \n",
    "        # Ensure date is timezone-naive\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('Date')\n",
    "        \n",
    "        # Initialize preprocessor\n",
    "        preprocessor = EnhancedFinancialPreprocessor(sequence_length=20)\n",
    "        \n",
    "        # Prepare data\n",
    "        prepared_data = preprocessor.prepare_data(df)\n",
    "        \n",
    "        # Log preparation summary\n",
    "        logger.info(\"\\nData Preparation Summary:\")\n",
    "        logger.info(f\"Training sequences shape: {prepared_data['train']['X'].shape}\")\n",
    "        logger.info(f\"Validation sequences shape: {prepared_data['val']['X'].shape}\")\n",
    "        \n",
    "        return prepared_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def visualize_features(df: pd.DataFrame, symbol: str):\n",
    "    \"\"\"\n",
    "    Create visualization of key features for a stock\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for the specified stock\n",
    "        stock_data = df[df['Symbol'] == symbol].copy()\n",
    "        \n",
    "        if len(stock_data) == 0:\n",
    "            raise ValueError(f\"No data found for symbol {symbol}\")\n",
    "            \n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.tz_localize(None)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Price and Moving Averages\n",
    "        axes[0].plot(stock_data['Date'], stock_data['Close'], label='Close')\n",
    "        axes[0].plot(stock_data['Date'], stock_data['MA20'], label='MA20')\n",
    "        axes[0].set_title(f'{symbol} - Price and Moving Averages')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Technical Indicators\n",
    "        axes[1].plot(stock_data['Date'], stock_data['RSI'], label='RSI')\n",
    "        axes[1].plot(stock_data['Date'], stock_data['MACD'], label='MACD')\n",
    "        axes[1].set_title('Technical Indicators')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Volume\n",
    "        axes[2].bar(stock_data['Date'], stock_data['Volume'])\n",
    "        axes[2].set_title('Volume')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating visualization: {str(e)}\")\n",
    "        raise\n",
    "    \"\"\"\n",
    "    Create visualization of key features for a stock\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for the specified stock\n",
    "        stock_data = df[df['Symbol'] == Symbol].copy()\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Price and Moving Averages\n",
    "        axes[0].plot(stock_data['Date'], stock_data['Close'], label='Close')\n",
    "        axes[0].plot(stock_data['Date'], stock_data['MA_20'], label='MA_20')\n",
    "        axes[0].plot(stock_data['Date'], stock_data['MA_50'], label='MA_50')\n",
    "        axes[0].set_title(f'{Symbol} - Price and Moving Averages')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Technical Indicators\n",
    "        axes[1].plot(stock_data['Date'], stock_data['RSI_14'], label='RSI_14')\n",
    "        axes[1].plot(stock_data['Date'], stock_data['MACD'], label='MACD')\n",
    "        axes[1].set_title('Technical Indicators')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Volume\n",
    "        axes[2].bar(stock_data['Date'], stock_data['Volume'])\n",
    "        axes[2].set_title('Volume')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating visualization: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading SP500 master data...\n",
      "C:\\Users\\17034\\AppData\\Local\\Temp\\ipykernel_14496\\2262027172.py:11: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
      "ERROR:__main__:Error loading data: Can only use .dt accessor with datetimelike values\n",
      "ERROR:__main__:Error in main execution: Can only use .dt accessor with datetimelike values\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Load and analyze data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         df \u001b[38;5;241m=\u001b[39m load_and_analyze_data()\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Example: Prepare data for a single stock (e.g., AAPL)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         aapl_data \u001b[38;5;241m=\u001b[39m prepare_stock_data(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mload_and_analyze_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp500_master_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert date to datetime and handle timezone\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Basic data analysis\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset Overview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\17034\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32mc:\\Users\\17034\\anaconda3\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\17034\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:608\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load and analyze data\n",
    "        df = load_and_analyze_data()\n",
    "        \n",
    "        # Example: Prepare data for a single stock (e.g., AAPL)\n",
    "        aapl_data = prepare_stock_data(df, 'AAPL')\n",
    "        \n",
    "        # Visualize features\n",
    "        visualize_features(df, 'AAPL')\n",
    "        \n",
    "        # Optional: Prepare data for all stocks\n",
    "        # all_stocks_data = prepare_stock_data(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
