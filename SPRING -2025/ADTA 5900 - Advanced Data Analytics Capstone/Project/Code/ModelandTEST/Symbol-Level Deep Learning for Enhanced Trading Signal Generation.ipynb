{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, Multiply, Lambda, RepeatVector, Permute\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import ta\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_correlation_features(symbol_data):\n",
    "    \"\"\"Add correlation-based features specific to this symbol\"\"\"\n",
    "    # Calculate correlation between returns and volume\n",
    "    symbol_data['Returns_Volume_Corr'] = symbol_data['Returns'].rolling(20).corr(symbol_data['Volume'])\n",
    "    \n",
    "    # Calculate correlation between symbol returns and market returns\n",
    "    if 'Market_Return' in symbol_data.columns:\n",
    "        symbol_data['Market_Corr_20d'] = symbol_data['Returns'].rolling(20).corr(symbol_data['Market_Return'])\n",
    "        symbol_data['Market_Corr_60d'] = symbol_data['Returns'].rolling(60).corr(symbol_data['Market_Return'])\n",
    "    \n",
    "    # Calculate correlation between price and volatility\n",
    "    symbol_data['Price_Volatility_Corr'] = symbol_data['Close'].rolling(30).corr(\n",
    "        symbol_data['Returns'].rolling(20).std())\n",
    "    \n",
    "    # Add price momentum correlation\n",
    "    symbol_data['Mom_Corr'] = symbol_data['Returns'].rolling(10).corr(\n",
    "        symbol_data['Returns'].shift(10).rolling(10).mean())\n",
    "    \n",
    "    return symbol_data\n",
    "def detect_technical_pattern(data, pattern_type):\n",
    "    \"\"\"Detect technical chart patterns in price data\"\"\"\n",
    "    result = np.zeros(len(data))\n",
    "    \n",
    "    if pattern_type == 'Double_Bottom':\n",
    "        # Simple double bottom detection\n",
    "        for i in range(30, len(data)):\n",
    "            window = data['Low'].iloc[i-30:i]\n",
    "            # Find two significant lows with a higher point between them\n",
    "            if len(window) >= 20:\n",
    "                min_idx = window.nsmallest(2).index\n",
    "                if len(min_idx) >= 2 and abs(min_idx[0] - min_idx[1]) >= 10:\n",
    "                    mid_point = window.iloc[min(min_idx):max(min_idx)].max()\n",
    "                    left_low, right_low = window.iloc[min_idx]\n",
    "                    # If middle point is significantly higher and lows are at similar levels\n",
    "                    if mid_point > left_low * 1.03 and abs(left_low - right_low) / left_low < 0.03:\n",
    "                        result[i] = 1\n",
    "    \n",
    "    elif pattern_type == 'Head_Shoulders':\n",
    "        # Simple head and shoulders detection\n",
    "        for i in range(40, len(data)):\n",
    "            if i+20 < len(data):\n",
    "                window = data['High'].iloc[i-40:i+20]\n",
    "                if len(window) >= 60:\n",
    "                    # Find 3 peaks\n",
    "                    peaks = window.nlargest(3).index.tolist()\n",
    "                    if len(peaks) == 3:\n",
    "                        peaks.sort()\n",
    "                        if len(peaks) == 3 and peaks[1] - peaks[0] >= 10 and peaks[2] - peaks[1] >= 10:\n",
    "                            # Check if middle peak is higher\n",
    "                            left, middle, right = window.iloc[peaks]\n",
    "                            if middle > left * 1.02 and middle > right * 1.02 and abs(left - right) / left < 0.05:\n",
    "                                result[i] = 1\n",
    "    \n",
    "    elif pattern_type == 'Triangle':\n",
    "        # Simple triangle pattern detection\n",
    "        for i in range(30, len(data)):\n",
    "            window_high = data['High'].iloc[i-30:i]\n",
    "            window_low = data['Low'].iloc[i-30:i]\n",
    "            \n",
    "            if len(window_high) >= 20 and len(window_low) >= 20:\n",
    "                # Check decreasing highs\n",
    "                highs = window_high.rolling(5).max().dropna()\n",
    "                # Check increasing lows\n",
    "                lows = window_low.rolling(5).min().dropna()\n",
    "                \n",
    "                if len(highs) >= 3 and len(lows) >= 3:\n",
    "                    high_slope = np.polyfit(range(len(highs)), highs.values, 1)[0]\n",
    "                    low_slope = np.polyfit(range(len(lows)), lows.values, 1)[0]\n",
    "                    \n",
    "                    # Triangle patterns: highs slope down, lows slope up\n",
    "                    if high_slope < -0.001 and low_slope > 0.001:\n",
    "                        result[i] = 1\n",
    "    \n",
    "    return result\n",
    "def calculate_mutual_information(X, y):\n",
    "    \"\"\"Calculate mutual information between features and target\"\"\"\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    \n",
    "    # Convert target to class labels if one-hot encoded\n",
    "    if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "        y_labels = np.argmax(y, axis=1)\n",
    "    else:\n",
    "        y_labels = y\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_scores = mutual_info_classif(X, y_labels)\n",
    "    \n",
    "    # Create list of (feature, score) tuples\n",
    "    mi_features = [(X.columns[i], score) for i, score in enumerate(mi_scores)]\n",
    "    \n",
    "    # Sort by mutual information score in descending order\n",
    "    mi_features.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return mi_features\n",
    "def prepare_prediction_data(symbol_data, sequence_length, features):\n",
    "    \"\"\"Prepare sequence data for prediction\"\"\"\n",
    "    # Extract features\n",
    "    data = symbol_data[features].values\n",
    "    \n",
    "    # Create sequences\n",
    "    X = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "    \n",
    "    return np.array(X)\n",
    "def rebalance_portfolio(portfolio, target_allocations, df, day):\n",
    "    \"\"\"Rebalance portfolio based on target allocations\"\"\"\n",
    "    # Calculate current portfolio value\n",
    "    current_value = calculate_portfolio_value(portfolio, df, day)\n",
    "    \n",
    "    # Get current positions\n",
    "    current_positions = portfolio['positions'].copy()\n",
    "    \n",
    "    # Calculate trades needed\n",
    "    trades = {}\n",
    "    for symbol, target_alloc in target_allocations.items():\n",
    "        target_value = current_value * abs(target_alloc)\n",
    "        target_direction = 1 if target_alloc > 0 else -1\n",
    "        \n",
    "        # Get current position\n",
    "        current_position = current_positions.get(symbol, {'shares': 0, 'direction': 0})\n",
    "        current_shares = current_position['shares']\n",
    "        current_direction = current_position['direction']\n",
    "        \n",
    "        # Get current price\n",
    "        symbol_data = df[(df['Symbol'] == symbol) & (df.index == day)]\n",
    "        if len(symbol_data) == 0:\n",
    "            continue  # Skip if no price data for today\n",
    "            \n",
    "        current_price = symbol_data['Close'].values[0]\n",
    "        \n",
    "        # Calculate target shares\n",
    "        target_shares = target_value / current_price\n",
    "        \n",
    "        # Handle direction change\n",
    "        if current_direction != target_direction and current_shares > 0:\n",
    "            # Close current position\n",
    "            portfolio['cash'] += current_shares * current_price * current_direction\n",
    "            # Remove position\n",
    "            if symbol in portfolio['positions']:\n",
    "                del portfolio['positions'][symbol]\n",
    "            # Reset current shares\n",
    "            current_shares = 0\n",
    "            current_direction = 0\n",
    "        \n",
    "        # Calculate shares to trade\n",
    "        if target_direction == current_direction or current_shares == 0:\n",
    "            shares_to_trade = target_shares - current_shares\n",
    "            if abs(shares_to_trade) < 1:  # Skip small trades\n",
    "                continue\n",
    "                \n",
    "            # Execute trade\n",
    "            cost = shares_to_trade * current_price\n",
    "            if shares_to_trade > 0:  # Buy\n",
    "                if portfolio['cash'] >= cost:\n",
    "                    portfolio['cash'] -= cost\n",
    "                    # Update position\n",
    "                    portfolio['positions'][symbol] = {\n",
    "                        'shares': current_shares + shares_to_trade,\n",
    "                        'direction': target_direction,\n",
    "                        'entry_price': current_price\n",
    "                    }\n",
    "            else:  # Sell\n",
    "                portfolio['cash'] += abs(cost)\n",
    "                # Update position\n",
    "                if current_shares - abs(shares_to_trade) <= 0:\n",
    "                    if symbol in portfolio['positions']:\n",
    "                        del portfolio['positions'][symbol]\n",
    "                else:\n",
    "                    portfolio['positions'][symbol] = {\n",
    "                        'shares': current_shares - abs(shares_to_trade),\n",
    "                        'direction': current_direction,\n",
    "                        'entry_price': current_position['entry_price']\n",
    "                    }\n",
    "    \n",
    "    return portfolio\n",
    "def calculate_portfolio_value(portfolio, df, day):\n",
    "    \"\"\"Calculate total portfolio value\"\"\"\n",
    "    # Start with cash\n",
    "    total_value = portfolio['cash']\n",
    "    \n",
    "    # Add position values\n",
    "    for symbol, position in portfolio['positions'].items():\n",
    "        # Get current price\n",
    "        symbol_data = df[(df['Symbol'] == symbol) & (df.index == day)]\n",
    "        if len(symbol_data) == 0:\n",
    "            continue  # Skip if no price data for today\n",
    "            \n",
    "        current_price = symbol_data['Close'].values[0]\n",
    "        \n",
    "        # Add position value\n",
    "        position_value = position['shares'] * current_price\n",
    "        total_value += position_value\n",
    "    \n",
    "    return total_value\n",
    "def calculate_performance_metrics(portfolio):\n",
    "    \"\"\"Calculate performance metrics for the portfolio\"\"\"\n",
    "    # Get portfolio value history\n",
    "    values = [entry['value'] for entry in portfolio['value_history']]\n",
    "    dates = [entry['date'] for entry in portfolio['value_history']]\n",
    "    \n",
    "    # Calculate returns\n",
    "    returns = np.diff(values) / values[:-1]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = (values[-1] / values[0]) - 1\n",
    "    annual_return = (1 + total_return) ** (252 / len(values)) - 1\n",
    "    volatility = np.std(returns) * np.sqrt(252)\n",
    "    sharpe_ratio = annual_return / volatility if volatility > 0 else 0\n",
    "    \n",
    "    # Calculate drawdowns\n",
    "    peak = values[0]\n",
    "    drawdowns = []\n",
    "    for value in values:\n",
    "        if value > peak:\n",
    "            peak = value\n",
    "        drawdown = (peak - value) / peak\n",
    "        drawdowns.append(drawdown)\n",
    "    \n",
    "    max_drawdown = max(drawdowns)\n",
    "    \n",
    "    # Calculate win rate for trades\n",
    "    # This would require a transaction history that we don't have\n",
    "    # in this simplified implementation\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "def visualize_symbol_performance(symbol_performances, top_n=10):\n",
    "    \"\"\"Visualize performance of top and bottom symbols\"\"\"\n",
    "    # Sort symbols by total return\n",
    "    sorted_symbols = sorted(symbol_performances.keys(), \n",
    "                           key=lambda s: symbol_performances[s]['total_return'],\n",
    "                           reverse=True)\n",
    "    \n",
    "    # Get top N and bottom N performers\n",
    "    top_performers = sorted_symbols[:top_n]\n",
    "    bottom_performers = sorted_symbols[-top_n:]\n",
    "    \n",
    "    # Plot top performers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    for symbol in top_performers:\n",
    "        perf = symbol_performances[symbol]\n",
    "        plt.bar(symbol, perf['total_return'] * 100)\n",
    "    \n",
    "    plt.title(f'Top {top_n} Performing Symbols')\n",
    "    plt.ylabel('Total Return (%)')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot bottom performers\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    for symbol in bottom_performers:\n",
    "        perf = symbol_performances[symbol]\n",
    "        plt.bar(symbol, perf['total_return'] * 100)\n",
    "    \n",
    "    plt.title(f'Bottom {top_n} Performing Symbols')\n",
    "    plt.ylabel('Total Return (%)')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def visualize_sector_performance(symbol_performances):\n",
    "    \"\"\"Visualize performance by sector\"\"\"\n",
    "    # Group performances by sector\n",
    "    sector_performances = {}\n",
    "    \n",
    "    for symbol, perf in symbol_performances.items():\n",
    "        sector = get_sector_for_symbol(symbol)\n",
    "        if sector not in sector_performances:\n",
    "            sector_performances[sector] = []\n",
    "        \n",
    "        sector_performances[sector].append(perf['total_return'])\n",
    "    \n",
    "    # Calculate average performance by sector\n",
    "    sector_avg_performance = {}\n",
    "    for sector, performances in sector_performances.items():\n",
    "        sector_avg_performance[sector] = np.mean(performances)\n",
    "    \n",
    "    # Plot sector performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sectors = list(sector_avg_performance.keys())\n",
    "    performances = [sector_avg_performance[s] * 100 for s in sectors]\n",
    "    \n",
    "    plt.bar(sectors, performances)\n",
    "    plt.title('Average Performance by Sector')\n",
    "    plt.ylabel('Average Return (%)')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_portfolio_performance(portfolio, benchmark_returns=None):\n",
    "    \"\"\"Plot portfolio performance over time\"\"\"\n",
    "    # Get portfolio value history\n",
    "    values = [entry['value'] for entry in portfolio['value_history']]\n",
    "    dates = [entry['date'] for entry in portfolio['value_history']]\n",
    "    \n",
    "    # Calculate portfolio returns\n",
    "    portfolio_returns = [values[i] / values[0] - 1 for i in range(len(values))]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot portfolio returns\n",
    "    plt.plot(dates, portfolio_returns, label='Portfolio')\n",
    "    \n",
    "    # Plot benchmark if provided\n",
    "    if benchmark_returns is not None:\n",
    "        plt.plot(dates, benchmark_returns, label='Benchmark')\n",
    "    \n",
    "    plt.title('Portfolio Performance')\n",
    "    plt.ylabel('Return (%)')\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def analyze_prediction_accuracy(all_signals, df):\n",
    "    \"\"\"Analyze prediction accuracy for all symbols\"\"\"\n",
    "    accuracy_by_symbol = {}\n",
    "    \n",
    "    for symbol, signals in all_signals.items():\n",
    "        # Get actual returns\n",
    "        symbol_data = df[df['Symbol'] == symbol]\n",
    "        actual_returns = symbol_data['Returns'].shift(-1)  # Next day returns\n",
    "        \n",
    "        # Align signals with actual returns\n",
    "        aligned_data = pd.concat([signals['predicted_class'], actual_returns], axis=1)\n",
    "        aligned_data.columns = ['predicted_class', 'next_return']\n",
    "        aligned_data = aligned_data.dropna()\n",
    "        \n",
    "        if len(aligned_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_predictions = 0\n",
    "        total_predictions = len(aligned_data)\n",
    "        \n",
    "        for i, row in aligned_data.iterrows():\n",
    "            predicted = row['predicted_class']\n",
    "            actual_return = row['next_return']\n",
    "            \n",
    "            # Check if prediction was correct\n",
    "            if (predicted == 2 and actual_return > 0) or \\\n",
    "               (predicted == 0 and actual_return < 0) or \\\n",
    "               (predicted == 1 and -0.003 <= actual_return <= 0.003):\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        accuracy_by_symbol[symbol] = accuracy\n",
    "    \n",
    "    return accuracy_by_symbol\n",
    "def select_symbol_features(symbol_data, base_features, top_n=25):\n",
    "    \"\"\"Select most predictive features for this specific symbol\"\"\"\n",
    "    features = base_features.copy()\n",
    "    \n",
    "    # Calculate feature importance for this symbol\n",
    "    X = symbol_data[features].fillna(0)\n",
    "    y = symbol_data['target']\n",
    "    \n",
    "    # Use mutual information to determine feature importance\n",
    "    importance = calculate_mutual_information(X, y)\n",
    "    \n",
    "    # Select top features specific to this symbol\n",
    "    top_features = [f[0] for f in importance[:top_n]]\n",
    "    \n",
    "    # Always include certain key features regardless of importance\n",
    "    essential_features = ['Returns', 'Volatility_20d', 'Market_Return', 'RSI_14']\n",
    "    for feature in essential_features:\n",
    "        if feature in features and feature not in top_features:\n",
    "            top_features.append(feature)\n",
    "    \n",
    "    return top_features\n",
    "def get_optimal_hyperparameters(symbol):\n",
    "    \"\"\"Retrieve optimal hyperparameters for this symbol\"\"\"\n",
    "    \n",
    "    # Get sector for this symbol\n",
    "    sector = get_sector_for_symbol(symbol)\n",
    "    \n",
    "    # Get symbol volatility profile\n",
    "    volatility = get_symbol_volatility(symbol)\n",
    "    \n",
    "    # Define base hyperparameters\n",
    "    base_params = {\n",
    "        'cnn_filters_1': 64,\n",
    "        'kernel_size_1': 3,\n",
    "        'cnn_filters_2': 128,\n",
    "        'kernel_size_2': 3,\n",
    "        'pool_size': 2,\n",
    "        'lstm_units_1': 64,\n",
    "        'lstm_units_2': 32,\n",
    "        'dense_units': 32,\n",
    "        'dropout_1': 0.2,\n",
    "        'dropout_2': 0.3,\n",
    "        'dropout_lstm_1': 0.3,\n",
    "        'dropout_lstm_2': 0.4,\n",
    "        'dropout_dense': 0.4,\n",
    "        'use_second_cnn': True,\n",
    "        'use_bidirectional': True,\n",
    "        'activation': 'relu',\n",
    "        'learning_rate': 0.001,\n",
    "        'loss_function': 'categorical_crossentropy'\n",
    "    }\n",
    "    \n",
    "    # Adjust hyperparameters based on sector\n",
    "    sector_params = {\n",
    "        'Technology': {'cnn_filters_1': 128, 'lstm_units_1': 128},\n",
    "        'Financial': {'lstm_units_1': 96, 'dropout_lstm_1': 0.4},\n",
    "        'Healthcare': {'use_bidirectional': True, 'dropout_dense': 0.5},\n",
    "        'Energy': {'kernel_size_1': 5, 'learning_rate': 0.0005},\n",
    "        'Consumer': {'dense_units': 64, 'dropout_2': 0.25}\n",
    "    }\n",
    "    \n",
    "    # Adjust hyperparameters based on volatility\n",
    "    if volatility == 'high':\n",
    "        volatility_params = {'dropout_1': 0.3, 'dropout_lstm_1': 0.4}\n",
    "    elif volatility == 'medium':\n",
    "        volatility_params = {'dropout_1': 0.2, 'dropout_lstm_1': 0.3}\n",
    "    else:\n",
    "        volatility_params = {'dropout_1': 0.1, 'dropout_lstm_1': 0.2}\n",
    "    \n",
    "    # Update base parameters with sector-specific parameters\n",
    "    if sector in sector_params:\n",
    "        base_params.update(sector_params[sector])\n",
    "    \n",
    "    # Update with volatility-specific parameters\n",
    "    base_params.update(volatility_params)\n",
    "    \n",
    "    return base_params\n",
    "def train_symbol_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train model with adaptive learning rate and early stopping\"\"\"\n",
    "    \n",
    "    # Convert y_train to integer labels if one-hot encoded\n",
    "    if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "        y_train_labels = np.argmax(y_train, axis=1)\n",
    "    else:\n",
    "        y_train_labels = y_train.copy()\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_counts = np.bincount(y_train_labels)\n",
    "    total_samples = len(y_train_labels)\n",
    "    class_weights = {\n",
    "        i: (total_samples / (len(np.unique(y_train_labels)) * count)) \n",
    "        for i, count in enumerate(class_counts)\n",
    "    }\n",
    "    \n",
    "    # Enhanced callbacks for symbol-specific training\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "        ModelCheckpoint(f'models/{symbol}_best_model.h5', monitor='val_loss', \n",
    "                         save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    # Train with adaptive batch size based on dataset size\n",
    "    batch_size = min(32, len(X_train) // 20)  # Ensure at least 20 batches\n",
    "    batch_size = max(8, batch_size)  # Ensure batch size is at least 8\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,  # Use early stopping to determine actual epochs\n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0  # Reduce verbosity for many symbols\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "def generate_symbol_signals(symbol_models, df, sequence_length, symbol_features):\n",
    "    \"\"\"Generate trading signals for all symbols\"\"\"\n",
    "    symbols = list(symbol_models.keys())\n",
    "    all_signals = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        # Get model and features for this symbol\n",
    "        model = symbol_models[symbol]\n",
    "        features = symbol_features[symbol]\n",
    "        \n",
    "        # Get data for this symbol\n",
    "        symbol_data = df[df['Symbol'] == symbol].copy()\n",
    "        \n",
    "        # Prepare sequence data\n",
    "        X = prepare_prediction_data(symbol_data, sequence_length, features)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        prediction_probs = np.max(predictions, axis=1)\n",
    "        \n",
    "        # Create signals DataFrame\n",
    "        signals = pd.DataFrame(index=symbol_data.index[-len(predictions):])\n",
    "        signals['predicted_class'] = predicted_classes\n",
    "        signals['confidence'] = prediction_probs\n",
    "        signals['symbol'] = symbol\n",
    "        \n",
    "        # Add signal strength based on confidence\n",
    "        signals['signal_strength'] = 0.0\n",
    "        for i, cls in enumerate(predicted_classes):\n",
    "            if cls == 2:  # Up signal\n",
    "                signals['signal_strength'].iloc[i] = prediction_probs[i]\n",
    "            elif cls == 0:  # Down signal\n",
    "                signals['signal_strength'].iloc[i] = -prediction_probs[i]\n",
    "        \n",
    "        # Store signals\n",
    "        all_signals[symbol] = signals\n",
    "    \n",
    "    return all_signals\n",
    "def optimize_portfolio(all_signals, date, max_positions=20, max_allocation=0.2):\n",
    "    \"\"\"Create optimized portfolio based on signal strength\"\"\"\n",
    "    # Get signals for the specific date\n",
    "    date_signals = {}\n",
    "    for symbol, signals in all_signals.items():\n",
    "        if date in signals.index:\n",
    "            date_signals[symbol] = signals.loc[date]\n",
    "    \n",
    "    # Sort symbols by signal strength (absolute value)\n",
    "    symbols_ranked = sorted(date_signals.keys(), \n",
    "                           key=lambda s: abs(date_signals[s]['signal_strength']),\n",
    "                           reverse=True)\n",
    "    \n",
    "    # Take top N signals\n",
    "    top_symbols = symbols_ranked[:max_positions]\n",
    "    \n",
    "    # Calculate allocation based on signal strength\n",
    "    total_strength = sum(abs(date_signals[s]['signal_strength']) for s in top_symbols)\n",
    "    allocations = {}\n",
    "    \n",
    "    for symbol in top_symbols:\n",
    "        # Determine position direction (long/short)\n",
    "        direction = 1 if date_signals[symbol]['signal_strength'] > 0 else -1\n",
    "        \n",
    "        # Calculate allocation percentage (proportional to signal strength)\n",
    "        allocation = (abs(date_signals[symbol]['signal_strength']) / total_strength) \n",
    "        \n",
    "        # Cap at maximum allocation\n",
    "        allocation = min(allocation, max_allocation)\n",
    "        \n",
    "        # Store allocation with direction\n",
    "        allocations[symbol] = allocation * direction\n",
    "    \n",
    "    return allocations\n",
    "def implement_symbol_strategy(symbol_models, df, sequence_length, symbol_features, \n",
    "                             start_date, end_date, initial_capital=1000000):\n",
    "    \"\"\"Implement the full trading strategy across all symbols\"\"\"\n",
    "    # Get unique dates in the specified range\n",
    "    trading_days = df[(df.index >= start_date) & (df.index <= end_date)].index.unique()\n",
    "    \n",
    "    # Initialize portfolio\n",
    "    portfolio = {\n",
    "        'cash': initial_capital,\n",
    "        'positions': {},\n",
    "        'value_history': []\n",
    "    }\n",
    "    \n",
    "    # Generate all signals\n",
    "    all_signals = generate_symbol_signals(symbol_models, df, sequence_length, symbol_features)\n",
    "    \n",
    "    # Iterate through trading days\n",
    "    for day in trading_days:\n",
    "        # Optimize portfolio for this day\n",
    "        target_allocations = optimize_portfolio(all_signals, day)\n",
    "        \n",
    "        # Rebalance portfolio\n",
    "        portfolio = rebalance_portfolio(portfolio, target_allocations, df, day)\n",
    "        \n",
    "        # Record portfolio value\n",
    "        portfolio_value = calculate_portfolio_value(portfolio, df, day)\n",
    "        portfolio['value_history'].append({\n",
    "            'date': day,\n",
    "            'value': portfolio_value\n",
    "        })\n",
    "    \n",
    "    return portfolio\n",
    "def calculate_risk_parameters(symbol, df):\n",
    "    \"\"\"Calculate sector-specific risk parameters\"\"\"\n",
    "    # Get sector\n",
    "    sector = get_sector_for_symbol(symbol)\n",
    "    \n",
    "    # Get last 250 trading days of data\n",
    "    symbol_data = df[df['Symbol'] == symbol].tail(250)\n",
    "    \n",
    "    # Calculate volatility\n",
    "    volatility = symbol_data['Returns'].std() * np.sqrt(252)\n",
    "    \n",
    "    # Determine appropriate stop-loss based on sector and volatility\n",
    "    sector_multipliers = {\n",
    "        'Technology': 2.0,\n",
    "        'Financial': 1.5,\n",
    "        'Healthcare': 1.8,\n",
    "        'Energy': 2.2,\n",
    "        'Consumer': 1.7,\n",
    "        'Utilities': 1.2,\n",
    "        'Materials': 2.0,\n",
    "        'Industrial': 1.8,\n",
    "        'Real Estate': 1.5,\n",
    "        'Communication': 1.8\n",
    "    }\n",
    "    \n",
    "    # Get appropriate multiplier with default of 1.8\n",
    "    multiplier = sector_multipliers.get(sector, 1.8)\n",
    "    \n",
    "    # Calculate stop-loss percentage\n",
    "    stop_loss_pct = volatility * multiplier\n",
    "    \n",
    "    # Ensure minimum and maximum stop-loss values\n",
    "    stop_loss_pct = max(0.05, min(0.25, stop_loss_pct))\n",
    "    \n",
    "    # Calculate trailing stop parameters\n",
    "    trailing_stop_activation = 0.05  # Activate trailing stop after 5% profit\n",
    "    trailing_stop_distance = volatility * multiplier * 0.5  # Half the stop-loss distance\n",
    "    \n",
    "    return {\n",
    "        'stop_loss_pct': stop_loss_pct,\n",
    "        'trailing_stop_activation': trailing_stop_activation,\n",
    "        'trailing_stop_distance': trailing_stop_distance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_symbol_model(model, X_test, y_test, test_data):\n",
    "    \"\"\"Evaluate model performance and return metrics\"\"\"\n",
    "    # Make predictions on test data\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Convert y_test to integer class labels if one-hot encoded\n",
    "    if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_test_labels = y_test.copy()\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
    "    \n",
    "    # Calculate profit-based metrics\n",
    "    # Assuming labels: 0=down, 1=neutral, 2=up\n",
    "    # Map predictions to actual returns\n",
    "    pred_returns = np.zeros(len(y_pred))\n",
    "    \n",
    "    # Set the predicted returns based on the actual returns in test_data\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 2:  # Predicted up\n",
    "            pred_returns[i] = test_data['Returns'].iloc[i]\n",
    "        elif y_pred[i] == 0:  # Predicted down\n",
    "            pred_returns[i] = -test_data['Returns'].iloc[i]  # Reverse for short\n",
    "        # Neutral predictions (1) are left as 0 return\n",
    "    \n",
    "    # Calculate financial metrics\n",
    "    total_return = np.sum(pred_returns)\n",
    "    win_rate = np.mean(pred_returns > 0)\n",
    "    avg_win = np.mean(pred_returns[pred_returns > 0]) if any(pred_returns > 0) else 0\n",
    "    avg_loss = np.mean(pred_returns[pred_returns < 0]) if any(pred_returns < 0) else 0\n",
    "    \n",
    "    # Profit factor = gross profit / gross loss\n",
    "    gross_profit = np.sum(pred_returns[pred_returns > 0]) if any(pred_returns > 0) else 0\n",
    "    gross_loss = abs(np.sum(pred_returns[pred_returns < 0])) if any(pred_returns < 0) else 0\n",
    "    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'total_return': total_return,\n",
    "        'win_rate': win_rate,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'profit_factor': profit_factor\n",
    "    }\n",
    "\n",
    "    \"\"\"Build and train individualized models for each symbol\"\"\"\n",
    "    symbol_models = {}\n",
    "    symbol_features = {}\n",
    "    symbol_performances = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        print(f\"Processing {symbol}...\")\n",
    "        \n",
    "        # 1. Get symbol-specific data\n",
    "        symbol_data = df[df['Symbol'] == symbol].copy()\n",
    "        if len(symbol_data) < 500:  # Skip symbols with insufficient data\n",
    "            continue\n",
    "            \n",
    "        # 2. Feature extraction and engineering for this specific symbol\n",
    "        symbol_data = add_symbol_specific_features(symbol_data, base_features)\n",
    "        \n",
    "        # 3. Correlation analysis to find most relevant features for this symbol\n",
    "        symbol_features[symbol] = select_symbol_features(symbol_data, base_features)\n",
    "        \n",
    "        # 4. Prepare sequence data\n",
    "        X, y = prepare_sequence_data(symbol_data, sequence_length, 'target', symbol_features[symbol])\n",
    "        \n",
    "        # 5. Train/test split (time-based)\n",
    "        train_size = int(0.7 * len(X))\n",
    "        val_size = int(0.15 * len(X))\n",
    "        \n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "        X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "        \n",
    "        # 6. Build and train model with optimized hyperparameters for this symbol\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = build_optimized_model_for_symbol(symbol, input_shape)\n",
    "        \n",
    "        # 7. Train with early stopping\n",
    "        history, model = train_symbol_model(model, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # 8. Evaluate performance\n",
    "        performance = evaluate_symbol_model(model, X_test, y_test, symbol_data.iloc[train_size+val_size+sequence_length:])\n",
    "        symbol_performances[symbol] = performance\n",
    "        \n",
    "        # 9. Save model\n",
    "        symbol_models[symbol] = model\n",
    "        \n",
    "    return symbol_models, symbol_features, symbol_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility_regime(symbol_data):\n",
    "    \"\"\"Determine volatility regime (high, medium, low) for the symbol\"\"\"\n",
    "    if 'Volatility_20d' not in symbol_data.columns:\n",
    "        return 'medium'  # Default if volatility not calculated\n",
    "    \n",
    "    # Get recent volatility (last 20 days)\n",
    "    recent_volatility = symbol_data['Volatility_20d'].iloc[-20:].mean()\n",
    "    \n",
    "    # Get historical volatility (full period)\n",
    "    historical_volatility = symbol_data['Volatility_20d'].mean()\n",
    "    historical_std = symbol_data['Volatility_20d'].std()\n",
    "    \n",
    "    # Determine regime based on z-score\n",
    "    z_score = (recent_volatility - historical_volatility) / historical_std\n",
    "    \n",
    "    if z_score > 1.0:\n",
    "        return 'high'\n",
    "    elif z_score < -1.0:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'medium'\n",
    "def get_days_since_earnings(symbol_data):\n",
    "    \"\"\"Calculate days since last earnings announcement\"\"\"\n",
    "    # This is a placeholder - in a real implementation, you would need\n",
    "    # an external data source for earnings dates\n",
    "    # Here we use a cyclic pattern to approximate quarterly earnings\n",
    "    \n",
    "    trading_days = np.arange(len(symbol_data))\n",
    "    # Assuming earnings every ~63 trading days (quarterly)\n",
    "    days_since = trading_days % 63\n",
    "    \n",
    "    return days_since\n",
    "def get_sector_for_symbol(symbol):\n",
    "    \"\"\"Get sector classification for a symbol\"\"\"\n",
    "    # This is a simplified version - in real implementation, \n",
    "    # this would use an external data source for sector information\n",
    "    \n",
    "    # Example sector mapping for common symbols\n",
    "    sector_map = {\n",
    "        # Technology\n",
    "        'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology',\n",
    "        'AMZN': 'Technology', 'META': 'Technology', 'NVDA': 'Technology',\n",
    "        'INTC': 'Technology', 'AMD': 'Technology', 'CSCO': 'Technology',\n",
    "        \n",
    "        # Financial\n",
    "        'JPM': 'Financial', 'BAC': 'Financial', 'WFC': 'Financial',\n",
    "        'GS': 'Financial', 'MS': 'Financial', 'AXP': 'Financial',\n",
    "        'V': 'Financial', 'MA': 'Financial', 'BLK': 'Financial',\n",
    "        \n",
    "        # Healthcare\n",
    "        'JNJ': 'Healthcare', 'PFE': 'Healthcare', 'MRK': 'Healthcare',\n",
    "        'UNH': 'Healthcare', 'ABT': 'Healthcare', 'LLY': 'Healthcare',\n",
    "        \n",
    "        # Energy\n",
    "        'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy',\n",
    "        'SLB': 'Energy', 'EOG': 'Energy', 'PSX': 'Energy',\n",
    "        \n",
    "        # Consumer\n",
    "        'PG': 'Consumer', 'KO': 'Consumer', 'PEP': 'Consumer',\n",
    "        'WMT': 'Consumer', 'MCD': 'Consumer', 'SBUX': 'Consumer',\n",
    "        \n",
    "        # Industrial\n",
    "        'GE': 'Industrial', 'BA': 'Industrial', 'CAT': 'Industrial',\n",
    "        'MMM': 'Industrial', 'HON': 'Industrial', 'UPS': 'Industrial',\n",
    "        \n",
    "        # Utilities\n",
    "        'NEE': 'Utilities', 'DUK': 'Utilities', 'SO': 'Utilities',\n",
    "        \n",
    "        # Communication\n",
    "        'T': 'Communication', 'VZ': 'Communication', 'CMCSA': 'Communication',\n",
    "        \n",
    "        # Materials\n",
    "        'DD': 'Materials', 'DOW': 'Materials', 'FCX': 'Materials',\n",
    "        \n",
    "        # Real Estate\n",
    "        'AMT': 'Real Estate', 'PLD': 'Real Estate', 'CCI': 'Real Estate'\n",
    "    }\n",
    "    \n",
    "    return sector_map.get(symbol, 'Other')\n",
    "def get_sector_returns(sector):\n",
    "    \"\"\"Get sector index returns\"\"\"\n",
    "    # This is a placeholder function\n",
    "    # In a real implementation, you would use external data for sector indices\n",
    "    \n",
    "    # Placeholder return value - would normally fetch sector index\n",
    "    return 0.001  # Placeholder 0.1% daily return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_symbol_specific_features(symbol_data, base_features):\n",
    "    \"\"\"Add features specific to symbol characteristics\"\"\"\n",
    "    \n",
    "    # Calculate sector-specific metrics\n",
    "    sector = get_sector_for_symbol(symbol_data['Symbol'].iloc[0])\n",
    "    \n",
    "    # Add volatility regime features\n",
    "    symbol_data['Volatility_Regime'] = get_volatility_regime(symbol_data)\n",
    "    \n",
    "    # Add earnings seasonality\n",
    "    symbol_data['Days_Since_Earnings'] = get_days_since_earnings(symbol_data)\n",
    "    \n",
    "    # Add symbol-specific technical patterns\n",
    "    for pattern in ['Double_Bottom', 'Head_Shoulders', 'Triangle']:\n",
    "        symbol_data[f'Pattern_{pattern}'] = detect_technical_pattern(symbol_data, pattern)\n",
    "    \n",
    "    # Add relative strength compared to sector and market\n",
    "    symbol_data['Sector_RS'] = symbol_data['Returns_20d'] / get_sector_returns(sector)\n",
    "    symbol_data['Market_RS'] = symbol_data['Returns_20d'] / symbol_data['Market_Return_20d']\n",
    "    \n",
    "    # Add symbol-specific correlation features\n",
    "    symbol_data = add_correlation_features(symbol_data)\n",
    "    \n",
    "    return symbol_data\n",
    "def prepare_sequence_data(df, sequence_length, target_column, feature_columns):\n",
    "    \"\"\"\n",
    "    Prepare sequence data for time series forecasting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input dataframe containing features and target\n",
    "    sequence_length : int\n",
    "        Number of time steps in each sequence\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "    feature_columns : list\n",
    "        List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array of shape (n_samples, sequence_length, n_features)\n",
    "        The feature sequences\n",
    "    y : numpy array of shape (n_samples,)\n",
    "        The target values\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Extract features and target\n",
    "    data = df[feature_columns].values\n",
    "    targets = df[target_column].values\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(targets[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "def build_optimized_model_for_symbol(symbol, input_shape, num_classes=3):\n",
    "    \"\"\"Build model with hyperparameters optimized for this symbol\"\"\"\n",
    "    \n",
    "    # Load optimal hyperparameters for this symbol (or sector)\n",
    "    hyperparams = get_optimal_hyperparameters(symbol)\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers with optimized parameters\n",
    "    x = Conv1D(filters=hyperparams['cnn_filters_1'], \n",
    "                kernel_size=hyperparams['kernel_size_1'], \n",
    "                padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(hyperparams['activation'])(x)\n",
    "    x = MaxPooling1D(pool_size=hyperparams['pool_size'])(x)\n",
    "    x = Dropout(hyperparams['dropout_1'])(x)\n",
    "    \n",
    "    # Optional second CNN layer based on symbol complexity\n",
    "    if hyperparams['use_second_cnn']:\n",
    "        x = Conv1D(filters=hyperparams['cnn_filters_2'], \n",
    "                    kernel_size=hyperparams['kernel_size_2'], \n",
    "                    padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(hyperparams['activation'])(x)\n",
    "        x = MaxPooling1D(pool_size=hyperparams['pool_size'])(x)\n",
    "        x = Dropout(hyperparams['dropout_2'])(x)\n",
    "    \n",
    "    # LSTM layers\n",
    "    if hyperparams['use_bidirectional']:\n",
    "        x = Bidirectional(LSTM(units=hyperparams['lstm_units_1'], \n",
    "                               return_sequences=True))(x)\n",
    "    else:\n",
    "        x = LSTM(units=hyperparams['lstm_units_1'], \n",
    "                 return_sequences=True)(x)\n",
    "    \n",
    "    x = Dropout(hyperparams['dropout_lstm_1'])(x)\n",
    "    \n",
    "    if hyperparams['use_bidirectional']:\n",
    "        x = Bidirectional(LSTM(units=hyperparams['lstm_units_2'], \n",
    "                               return_sequences=False))(x)\n",
    "    else:\n",
    "        x = LSTM(units=hyperparams['lstm_units_2'], \n",
    "                 return_sequences=False)(x)\n",
    "    \n",
    "    x = Dropout(hyperparams['dropout_lstm_2'])(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(hyperparams['dense_units'], activation=hyperparams['activation'])(x)\n",
    "    x = Dropout(hyperparams['dropout_dense'])(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile with optimized learning rate\n",
    "    optimizer = Adam(learning_rate=hyperparams['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=hyperparams['loss_function'],\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbol_specific_models(df,symbols, base_features, sequence_length=30):\n",
    "    \"\"\"Build and train individualized models for each symbol\"\"\"\n",
    "    symbol_models = {}\n",
    "    symbol_features = {}\n",
    "    symbol_performances = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        print(f\"Processing {symbol}...\")\n",
    "        \n",
    "        # 1. Get symbol-specific data\n",
    "        symbol_data = df[df['Symbol'] == symbol].copy()\n",
    "        if len(symbol_data) < 500:  # Skip symbols with insufficient data\n",
    "            continue\n",
    "            \n",
    "        # 2. Feature extraction and engineering for this specific symbol\n",
    "        symbol_data = add_symbol_specific_features(symbol_data, base_features)\n",
    "        \n",
    "        # 3. Correlation analysis to find most relevant features for this symbol\n",
    "        symbol_features[symbol] = select_symbol_features(symbol_data, base_features)\n",
    "        \n",
    "        # 4. Prepare sequence data\n",
    "        X, y = prepare_sequence_data(symbol_data, sequence_length, 'target', symbol_features[symbol])\n",
    "        \n",
    "        # 5. Train/test split (time-based)\n",
    "        train_size = int(0.7 * len(X))\n",
    "        val_size = int(0.15 * len(X))\n",
    "        \n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "        X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "        \n",
    "        # 6. Build and train model with optimized hyperparameters for this symbol\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = build_optimized_model_for_symbol(symbol, input_shape)\n",
    "        \n",
    "        # 7. Train with early stopping\n",
    "        history, model = train_symbol_model(model, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # 8. Evaluate performance\n",
    "        performance = evaluate_symbol_model(model, X_test, y_test, symbol_data.iloc[train_size+val_size+sequence_length:])\n",
    "        symbol_performances[symbol] = performance\n",
    "        \n",
    "        # 9. Save model\n",
    "        symbol_models[symbol] = model\n",
    "        \n",
    "    return symbol_models, symbol_features, symbol_performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building models for 501 symbols...\n",
      "Processing MMM...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'Timedelta' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[29], line 21\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding models for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(symbols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m symbols...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build symbol-specific models\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m symbol_models, symbol_features, symbol_performances \u001b[38;5;241m=\u001b[39m build_symbol_specific_models(\n\u001b[0;32m     22\u001b[0m     df,symbols, base_features, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating signals and implementing strategy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Implement trading strategy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m, in \u001b[0;36mbuild_symbol_specific_models\u001b[1;34m(df, symbols, base_features, sequence_length)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Feature extraction and engineering for this specific symbol\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m symbol_data \u001b[38;5;241m=\u001b[39m add_symbol_specific_features(symbol_data, base_features)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 3. Correlation analysis to find most relevant features for this symbol\u001b[39;00m\n\u001b[0;32m     19\u001b[0m symbol_features[symbol] \u001b[38;5;241m=\u001b[39m select_symbol_features(symbol_data, base_features)\n",
      "Cell \u001b[1;32mIn[19], line 564\u001b[0m, in \u001b[0;36madd_symbol_specific_features\u001b[1;34m(symbol_data, base_features)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Add symbol-specific technical patterns\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDouble_Bottom\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHead_Shoulders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTriangle\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 564\u001b[0m     symbol_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPattern_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m detect_technical_pattern(symbol_data, pattern)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# Add relative strength compared to sector and market\u001b[39;00m\n\u001b[0;32m    567\u001b[0m symbol_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSector_RS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m symbol_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturns_20d\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m get_sector_returns(sector)\n",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m, in \u001b[0;36mdetect_technical_pattern\u001b[1;34m(data, pattern_type)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(window) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m:\n\u001b[0;32m     30\u001b[0m     min_idx \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39mnsmallest(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(min_idx) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(min_idx[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m min_idx[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m     32\u001b[0m         mid_point \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mmin\u001b[39m(min_idx):\u001b[38;5;28mmax\u001b[39m(min_idx)]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m     33\u001b[0m         left_low, right_low \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39miloc[min_idx]\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'Timedelta' and 'int'"
     ]
    }
   ],
   "source": [
    "# Global variable declaration at the module level\n",
    "df = None\n",
    "def main():\n",
    "    \"\"\"Main function to run the full pipeline\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # Load the S&P 500 data - replace with your data loading code\n",
    "    df = pd.read_csv('sp500_master_data.csv', parse_dates=['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Get list of symbols\n",
    "    symbols = df['Symbol'].unique().tolist()\n",
    "    \n",
    "    # Define base features\n",
    "    base_features = [\n",
    "        'Close', 'Returns', 'Log_Returns', 'Volatility_20d', \n",
    "        'RSI_14', 'MACD', 'MFI_14', 'Market_Return', 'VIX'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Building models for {len(symbols)} symbols...\")\n",
    "    # Build symbol-specific models\n",
    "    symbol_models, symbol_features, symbol_performances = build_symbol_specific_models(\n",
    "        df,symbols, base_features, sequence_length=30\n",
    "    )\n",
    "    \n",
    "    print(\"Generating signals and implementing strategy...\")\n",
    "    # Implement trading strategy\n",
    "    portfolio = implement_symbol_strategy(\n",
    "        symbol_models, df, 30, symbol_features,\n",
    "        start_date='2020-01-01', end_date='2021-12-31',\n",
    "        initial_capital=1000000\n",
    "    )\n",
    "    \n",
    "    print(\"Analyzing results...\")\n",
    "    # Calculate performance metrics\n",
    "    performance_metrics = calculate_performance_metrics(portfolio)\n",
    "    print(\"Performance Metrics:\")\n",
    "    for metric, value in performance_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Analyze prediction accuracy\n",
    "    all_signals = generate_symbol_signals(symbol_models, df, 30, symbol_features)\n",
    "    accuracy_by_symbol = analyze_prediction_accuracy(all_signals, df)\n",
    "    \n",
    "    avg_accuracy = np.mean(list(accuracy_by_symbol.values()))\n",
    "    print(f\"Average prediction accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_symbol_performance(symbol_performances)\n",
    "    visualize_sector_performance(symbol_performances)\n",
    "    plot_portfolio_performance(portfolio)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
