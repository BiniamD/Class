{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:46:53.458276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-23 18:46:53.667300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-23 18:46:53.721802: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 18:46:54.138363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-23 18:46:55.873141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, add\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, Multiply, Lambda, RepeatVector, Permute\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Enhanced Data Preprocessing\n",
    "def preprocess_stock_data(df):\n",
    "    \"\"\"Comprehensive preprocessing for stock data\"\"\"\n",
    "    # 1. Forward-fill and backward-fill missing values\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(0, inplace=True)  # Any remaining NaNs\n",
    "    \n",
    "    # 2. Create target variable with adaptive threshold\n",
    "    # Use a fixed threshold for simplicity in this implementation\n",
    "    threshold = 0.003  # Can be adjusted based on market volatility\n",
    "    df['target'] = np.where(df['Returns'].shift(-1) > threshold, 2,\n",
    "                           np.where(df['Returns'].shift(-1) < -threshold, 0, 1))\n",
    "    \n",
    "    # Remove rows with NaN target\n",
    "    df = df.dropna(subset=['target'])\n",
    "    df['target'] = df['target'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Feature Selection with Mutual Information\n",
    "def select_features_with_mi(df, features, target_col='target', n_select=30):\n",
    "    \"\"\"Select features using mutual information\"\"\"\n",
    "    # Drop rows with missing target values\n",
    "    data = df.dropna(subset=[target_col])\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_scores = mutual_info_classif(data[features], data[target_col])\n",
    "    \n",
    "    # Create DataFrame of features and scores\n",
    "    mi_df = pd.DataFrame({'Feature': features, 'MI Score': mi_scores})\n",
    "    mi_df = mi_df.sort_values('MI Score', ascending=False)\n",
    "    \n",
    "    # Print top features and scores\n",
    "    print(\"Top 15 features by mutual information:\")\n",
    "    print(mi_df.head(15))\n",
    "    \n",
    "    # Return top n_select features\n",
    "    return mi_df.head(n_select)['Feature'].tolist()\n",
    "\n",
    "# 3. Create correlation tensor features\n",
    "def create_correlation_tensor(df, technical_indicators, window_size=30):\n",
    "    \"\"\"Create correlation tensors from technical indicators\"\"\"\n",
    "    n_samples = len(df) - window_size + 1\n",
    "    n_features = len(technical_indicators)\n",
    "    correlation_tensor = np.zeros((n_samples, n_features, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        window_data = df.iloc[i:i+window_size][technical_indicators]\n",
    "        correlation_matrix = window_data.corr().fillna(0).values\n",
    "        correlation_tensor[i] = correlation_matrix\n",
    "    \n",
    "    return correlation_tensor\n",
    "\n",
    "# 4. Sequence data preparation\n",
    "def prepare_sequence_data(df, sequence_length, target_column, feature_columns):\n",
    "    \"\"\"Prepare sequence data for time series forecasting\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Extract features and target\n",
    "    data = df[feature_columns].values\n",
    "    targets = df[target_column].values\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(targets[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 5. Build model with parameters for hyperparameter tuning\n",
    "def build_model_with_params(input_shape, filters, lstm_units, dropout_rates, learning_rate, num_classes=3):\n",
    "    \"\"\"Build model with configurable parameters for hyperparameter tuning\"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN Block 1\n",
    "    conv1 = Conv1D(filters=filters[0], kernel_size=3, padding='same')(inputs)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    act1 = Activation('relu')(bn1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(act1)\n",
    "    drop1 = Dropout(dropout_rates[0])(pool1)\n",
    "    \n",
    "    # CNN Block 2\n",
    "    conv2 = Conv1D(filters=filters[1], kernel_size=3, padding='same')(drop1)\n",
    "    bn2 = BatchNormalization()(conv2)\n",
    "    act2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(act2)\n",
    "    drop2 = Dropout(dropout_rates[1])(pool2)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    lstm1 = Bidirectional(LSTM(units=lstm_units[0], return_sequences=True))(drop2)\n",
    "    drop_lstm1 = Dropout(dropout_rates[2])(lstm1)\n",
    "    lstm2 = Bidirectional(LSTM(units=lstm_units[1], return_sequences=False))(drop_lstm1)\n",
    "    drop_lstm2 = Dropout(dropout_rates[2])(lstm2)\n",
    "    \n",
    "    # Final dense layers\n",
    "    dense1 = Dense(64, activation='relu')(drop_lstm2)\n",
    "    drop_dense = Dropout(dropout_rates[2])(dense1)\n",
    "    outputs = Dense(num_classes, activation='softmax')(drop_dense)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Use a lower learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Enhanced CNN-BiLSTM Model with Attention\n",
    "def build_enhanced_cnn_bilstm_model(input_shape, num_classes=3):\n",
    "    \"\"\"Build enhanced CNN-BiLSTM model with attention and residual connections\"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN Block 1\n",
    "    conv1 = Conv1D(filters=128, kernel_size=3, padding='same')(inputs)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    act1 = Activation('relu')(bn1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(act1)\n",
    "    drop1 = Dropout(0.2)(pool1)\n",
    "    \n",
    "    # CNN Block 2\n",
    "    conv2 = Conv1D(filters=256, kernel_size=3, padding='same')(drop1)\n",
    "    bn2 = BatchNormalization()(conv2)\n",
    "    act2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(act2)\n",
    "    drop2 = Dropout(0.3)(pool2)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    lstm1 = Bidirectional(LSTM(units=128, return_sequences=True))(drop2)\n",
    "    drop_lstm1 = Dropout(0.4)(lstm1)\n",
    "    lstm2 = Bidirectional(LSTM(units=64, return_sequences=False))(drop_lstm1)\n",
    "    drop_lstm2 = Dropout(0.4)(lstm2)\n",
    "    \n",
    "    # Final dense layers\n",
    "    dense1 = Dense(64, activation='relu')(drop_lstm2)\n",
    "    drop_dense = Dropout(0.5)(dense1)\n",
    "    outputs = Dense(num_classes, activation='softmax')(drop_dense)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Use a lower learning rate\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. Dynamic Class Weight Callback\n",
    "class DynamicClassWeightCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to dynamically adjust class weights during training\"\"\"\n",
    "    def __init__(self, training_data, training_targets, initial_weights=None, adjustment_factor=0.05):\n",
    "        super(DynamicClassWeightCallback, self).__init__()\n",
    "        self.X_train = training_data\n",
    "        self.y_train = training_targets\n",
    "        self.adjustment_factor = adjustment_factor\n",
    "        self.class_weights = initial_weights if initial_weights else {}\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Adjust weights based on validation performance\"\"\"\n",
    "        # Get current validation loss\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        # If validation loss is improving, keep current weights\n",
    "        if current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "            return\n",
    "        \n",
    "        # Otherwise, adjust weights for minority classes\n",
    "        if epoch % 5 == 0:  # Adjust every 5 epochs\n",
    "            y_pred = self.model.predict(self.X_train)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            y_true_classes = np.argmax(self.y_train, axis=1)\n",
    "            \n",
    "            # Calculate class-wise accuracy\n",
    "            class_accuracy = {}\n",
    "            for cls in range(len(self.class_weights)):\n",
    "                cls_indices = (y_true_classes == cls)\n",
    "                if np.sum(cls_indices) > 0:\n",
    "                    class_accuracy[cls] = np.mean(y_pred_classes[cls_indices] == cls)\n",
    "                else:\n",
    "                    class_accuracy[cls] = 0\n",
    "            \n",
    "            # Adjust weights - increase weight for classes with lower accuracy\n",
    "            for cls in self.class_weights:\n",
    "                if class_accuracy[cls] < np.mean(list(class_accuracy.values())):\n",
    "                    self.class_weights[cls] *= (1 + self.adjustment_factor)\n",
    "                else:\n",
    "                    self.class_weights[cls] *= (1 - self.adjustment_factor * 0.5)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch}: Updated class weights: {self.class_weights}\")\n",
    "\n",
    "# 7. Enhanced Training Function\n",
    "def train_model_with_advanced_techniques(model, X_train, y_train, X_val, y_val, epochs=150, batch_size=64):\n",
    "    \"\"\"Train model with enhanced class balancing techniques\"\"\"\n",
    "    # Convert y_train to integer labels (from one-hot encoded)\n",
    "    if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "        y_train_labels = np.argmax(y_train, axis=1)\n",
    "    else:\n",
    "        y_train_labels = y_train.copy()\n",
    "    \n",
    "    # Compute class weights with more emphasis on minority classes\n",
    "    class_counts = np.bincount(y_train_labels)\n",
    "    total_samples = len(y_train_labels)\n",
    "    class_weights = {\n",
    "        i: (total_samples / (len(np.unique(y_train_labels)) * count)) * 1.5 \n",
    "        if i != 1 else (total_samples / (len(np.unique(y_train_labels)) * count))\n",
    "        for i, count in enumerate(class_counts)\n",
    "    }\n",
    "    \n",
    "    # Dynamic class weight callback\n",
    "    dynamic_weights = DynamicClassWeightCallback(\n",
    "        X_train, y_train, \n",
    "        initial_weights=class_weights,\n",
    "        adjustment_factor=0.05\n",
    "    )\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n",
    "        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        dynamic_weights\n",
    "    ]\n",
    "    \n",
    "    # Train with more epochs and larger batch size\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# 8. Advanced Trading Signal Generation\n",
    "def generate_advanced_trading_signals(model, X_test, df_test, confidence_threshold=0.6):\n",
    "    \"\"\"Generate trading signals with confidence filtering and position sizing\"\"\"\n",
    "    # Get predictions with probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df_test.index[:len(y_pred_proba)])\n",
    "    signals['pred_down_prob'] = y_pred_proba[:, 0]\n",
    "    signals['pred_neutral_prob'] = y_pred_proba[:, 1]\n",
    "    signals['pred_up_prob'] = y_pred_proba[:, 2]\n",
    "    \n",
    "    # Apply confidence threshold\n",
    "    signals['predicted_class'] = np.argmax(y_pred_proba, axis=1)\n",
    "    signals['confidence'] = np.max(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Initialize position column\n",
    "    signals['position'] = 0  # Default is no position\n",
    "    \n",
    "    # Apply confidence threshold for taking positions\n",
    "    signals.loc[(signals['predicted_class'] == 2) & \n",
    "                (signals['confidence'] > confidence_threshold), 'position'] = 1  # Long\n",
    "    \n",
    "    signals.loc[(signals['predicted_class'] == 0) & \n",
    "                (signals['confidence'] > confidence_threshold), 'position'] = -1  # Short\n",
    "    \n",
    "    # Position sizing based on confidence (scale between 0.5 and 1.0)\n",
    "    signals['position_size'] = signals['position'] * (signals['confidence'] - 0.5) * 2\n",
    "    signals.loc[signals['position_size'] < 0, 'position_size'] = signals.loc[signals['position_size'] < 0, 'position_size'] * -1\n",
    "    \n",
    "    # Add price data\n",
    "    signals['price'] = df_test['Close'].values[:len(signals)]\n",
    "    \n",
    "    # Add market volatility for risk adjustment\n",
    "    if 'Volatility_20d' in df_test.columns:\n",
    "        signals['volatility'] = df_test['Volatility_20d'].values[:len(signals)]\n",
    "        # Adjust position size inversely to volatility (smaller positions in high volatility)\n",
    "        signals['position_size'] = signals['position_size'] / (1 + signals['volatility'] * 10)\n",
    "    \n",
    "    # Calculate returns\n",
    "    signals['market_return'] = np.log(signals['price'] / signals['price'].shift(1))\n",
    "    signals['strategy_return'] = signals['position'].shift(1) * signals['market_return']\n",
    "    signals['sized_strategy_return'] = signals['position_size'].shift(1) * signals['market_return']\n",
    "    \n",
    "    # Calculate drawdown\n",
    "    signals['cumulative_market_return'] = np.exp(signals['market_return'].cumsum()) - 1\n",
    "    signals['cumulative_strategy_return'] = np.exp(signals['sized_strategy_return'].cumsum()) - 1\n",
    "    \n",
    "    signals['drawdown'] = signals['cumulative_strategy_return'] - signals['cumulative_strategy_return'].cummax()\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = np.exp(signals['sized_strategy_return'].sum()) - 1\n",
    "    annual_return = np.exp(signals['sized_strategy_return'].mean() * 252) - 1\n",
    "    sharpe_ratio = np.sqrt(252) * signals['sized_strategy_return'].mean() / signals['sized_strategy_return'].std()\n",
    "    max_drawdown = signals['drawdown'].min()\n",
    "    win_rate = len(signals[signals['sized_strategy_return'] > 0]) / len(signals[signals['sized_strategy_return'] != 0])\n",
    "    \n",
    "    # Calculate profit factor\n",
    "    gross_profits = signals.loc[signals['sized_strategy_return'] > 0, 'sized_strategy_return'].sum()\n",
    "    gross_losses = abs(signals.loc[signals['sized_strategy_return'] < 0, 'sized_strategy_return'].sum())\n",
    "    profit_factor = gross_profits / gross_losses if gross_losses != 0 else float('inf')\n",
    "    \n",
    "    performance_metrics = {\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "        'profit_factor': profit_factor\n",
    "    }\n",
    "    \n",
    "    return signals, performance_metrics\n",
    "\n",
    "# 9. Enhanced Performance Visualization\n",
    "def visualize_trading_performance(signals, performance_metrics, symbol_name):\n",
    "    \"\"\"Create comprehensive visualizations of trading performance\"\"\"\n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Cumulative returns comparison\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    signals['cumulative_market_return'].plot(ax=ax1, label=f'{symbol_name} Return', color='blue', alpha=0.7)\n",
    "    signals['cumulative_strategy_return'].plot(ax=ax1, label='Strategy Return', color='green')\n",
    "    ax1.set_title(f'Cumulative Returns Comparison - {symbol_name}')\n",
    "    ax1.set_ylabel('Return (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot 2: Drawdown\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    signals['drawdown'].plot(ax=ax2, color='red')\n",
    "    ax2.set_title('Strategy Drawdown')\n",
    "    ax2.set_ylabel('Drawdown (%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot 3: Position and price\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.plot(signals.index, signals['price'], color='black', alpha=0.7)\n",
    "    \n",
    "    # Add buy/sell markers\n",
    "    buy_signals = signals[signals['position'].diff() > 0]\n",
    "    sell_signals = signals[signals['position'].diff() < 0]\n",
    "    \n",
    "    ax3.scatter(buy_signals.index, buy_signals['price'], marker='^', color='green', s=100, label='Buy')\n",
    "    ax3.scatter(sell_signals.index, sell_signals['price'], marker='v', color='red', s=100, label='Sell')\n",
    "    \n",
    "    ax3.set_title(f'Trading Signals - {symbol_name}')\n",
    "    ax3.set_ylabel('Price')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add performance metrics as text\n",
    "    plt.figtext(0.01, 0.01, f\"\"\"\n",
    "    {symbol_name} Performance Metrics:\n",
    "    - Total Return: {performance_metrics['total_return']*100:.2f}%\n",
    "    - Annual Return: {performance_metrics['annual_return']*100:.2f}%\n",
    "    - Sharpe Ratio: {performance_metrics['sharpe_ratio']:.2f}\n",
    "    - Max Drawdown: {performance_metrics['max_drawdown']*100:.2f}%\n",
    "    - Win Rate: {performance_metrics['win_rate']*100:.2f}%\n",
    "    - Profit Factor: {performance_metrics['profit_factor']:.2f}\n",
    "    \"\"\", fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.savefig(f\"{symbol_name}_trading_performance.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Main Execution Function for Single Symbol\n",
    "def run_trading_system_for_symbol(data_path, symbol, confidence_threshold=0.6):\n",
    "    \"\"\"Run complete trading system pipeline for a single stock symbol\"\"\"\n",
    "    # Load data\n",
    "    print(f\"Loading data for {symbol}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Filter data for the specific symbol\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "        print(f\"Filtered data for {symbol}. Count of rows: {len(df)}\")\n",
    "    else:\n",
    "        print(f\"No Symbol column found. Assuming data is already for {symbol}.\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"No data found for symbol {symbol}.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Convert Date column to datetime if it exists\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    df = preprocess_stock_data(df)\n",
    "    print(f\"Missing values after imputation: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    # Define feature categories\n",
    "    price_indicators = ['Close', 'Returns', 'Log_Returns', 'Price_Range', 'Price_Range_Pct']\n",
    "    moving_averages = [col for col in df.columns if col.startswith('MA_') or \n",
    "                        col.startswith('EMA_') or col.startswith('Returns_')]\n",
    "    volatility_metrics = [col for col in df.columns if col.startswith('Volatility_') or \n",
    "                            col.startswith('Volume_MA_') or col.startswith('BB_Width_')]\n",
    "    technical_indicators = ['RSI_9', 'RSI_14', 'RSI_25', 'MACD', 'Signal_Line', 'MACD_Histogram',\n",
    "                            'Momentum_14', 'ROC_14', 'MFI_14', 'MFI_28'] + \\\n",
    "                            [col for col in df.columns if col.startswith('Channel_Width_')]\n",
    "    volume_indicators = ['OBV', 'Volume_Ratio', 'Volume_StdDev']\n",
    "    fundamental_features = ['PE_Ratio', 'PB_Ratio', 'Dividend_Yield', 'Profit_Margin', \n",
    "                            'Beta', 'Enterprise_Value', 'Forward_EPS', 'Trailing_EPS']\n",
    "    market_features = ['Market_Return', 'Market_Volatility', 'Rolling_Beta', 'VIX', 'VIX_MA_10']\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = price_indicators + moving_averages + volatility_metrics + \\\n",
    "                    technical_indicators + volume_indicators + \\\n",
    "                    fundamental_features + market_features\n",
    "    \n",
    "    # Remove any features not in the dataframe\n",
    "    features = [f for f in all_features if f in df.columns]\n",
    "    print(f\"Using {len(features)} features\")\n",
    "    \n",
    "    # Feature selection with mutual information\n",
    "    selected_features = select_features_with_mi(df, features, 'target', n_select=30)\n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "    # Create correlation tensor features\n",
    "    tensor_indicators = [ind for ind in [\n",
    "        'RSI_14', 'MACD', 'Momentum_14', 'ROC_14', 'MFI_14', \n",
    "        'Volume_Ratio', 'Returns', 'Volatility_20d'\n",
    "    ] if ind in df.columns]\n",
    "    \n",
    "    # Prepare sequence data\n",
    "    print(\"Preparing sequence data...\")\n",
    "    sequence_length = 30  # 30 days of historical data\n",
    "    X, y = prepare_sequence_data(df, sequence_length, 'target', selected_features)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # Convert target to one-hot encoding\n",
    "    y_onehot = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "    \n",
    "    # Split data using time series split\n",
    "    print(\"Splitting data...\")\n",
    "    train_size = int(0.7 * len(X))\n",
    "    val_size = int(0.15 * len(X))\n",
    "    \n",
    "    X_train, y_train = X[:train_size], y_onehot[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size+val_size], y_onehot[train_size:train_size+val_size]\n",
    "    X_test, y_test = X[train_size+val_size:], y_onehot[train_size+val_size:]\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = np.sum(y_train, axis=0)\n",
    "    print(f\"Class distribution in training set:\")\n",
    "    print(f\"Down: {class_counts[0]}, Neutral: {class_counts[1]}, Up: {class_counts[2]}\")\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_enhanced_cnn_bilstm_model(input_shape, num_classes=3)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history, model = train_model_with_advanced_techniques(\n",
    "        model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluating model...\")\n",
    "    # Convert one-hot encoded targets back to class indices\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Get model predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Down', 'Neutral', 'Up'],\n",
    "                yticklabels=['Down', 'Neutral', 'Up'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {symbol}')\n",
    "    plt.savefig(f\"{symbol}_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model Accuracy - {symbol}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model Loss - {symbol}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{symbol}_training_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate trading signals\n",
    "    print(\"Generating trading signals...\")\n",
    "    df_test = df.iloc[train_size+val_size+sequence_length:]\n",
    "    signals, performance_metrics = generate_advanced_trading_signals(model, X_test, df_test, confidence_threshold)\n",
    "    \n",
    "    # Visualize trading performance\n",
    "    visualize_trading_performance(signals, performance_metrics, symbol)\n",
    "    \n",
    "    # Save model and data\n",
    "    model.save(f\"{symbol}_model.keras\")\n",
    "    np.save(f\"{symbol}_X_test.npy\", X_test)\n",
    "    np.save(f\"{symbol}_y_test.npy\", y_test)\n",
    "    signals.to_csv(f\"{symbol}_signals.csv\")\n",
    "    \n",
    "    return model, signals, performance_metrics\n",
    "# Enhanced feature importance analysis\n",
    "def analyze_feature_importance_for_symbol(model, X_test, y_test, feature_names, symbol):\n",
    "    \"\"\"Analyze feature importance using permutation importance\"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Convert model predictions to class labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate baseline accuracy\n",
    "    baseline_accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    # Reshape X_test for sklearn compatibility\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(\n",
    "        lambda x: np.argmax(model.predict(x.reshape(-1, X_test.shape[1], X_test.shape[2])), axis=1),\n",
    "        X_test_reshaped,\n",
    "        y_test_classes,\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame with importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': result.importances_mean,\n",
    "        'Std': result.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20])\n",
    "    plt.xlabel('Permutation Importance (decrease in accuracy)')\n",
    "    plt.title(f'Feature Importance for {symbol} (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{symbol}_feature_importance.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "def detect_market_regime(df, window=60):\n",
    "    \"\"\"Detect market regime based on volatility and trend\"\"\"\n",
    "    # Calculate rolling volatility\n",
    "    df['volatility'] = df['Returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Calculate rolling trend (slope of linear regression)\n",
    "    def rolling_slope(series, window):\n",
    "        slopes = []\n",
    "        for i in range(len(series) - window + 1):\n",
    "            x = np.arange(window)\n",
    "            y = series.iloc[i:i+window].values\n",
    "            slope, _, _, _, _ = linregress(x, y)\n",
    "            slopes.append(slope)\n",
    "        return pd.Series(np.nan, index=series.index).iloc[window-1:].combine_first(pd.Series(slopes, index=series.index[window-1:len(slopes)+window-1]))\n",
    "    \n",
    "    df['trend'] = rolling_slope(df['Close'], window)\n",
    "    \n",
    "    # Classify regimes\n",
    "    # High volatility, positive trend = 'Bull Volatile'\n",
    "    # High volatility, negative trend = 'Bear Volatile'\n",
    "    # Low volatility, positive trend = 'Bull Stable'\n",
    "    # Low volatility, negative trend = 'Bear Stable'\n",
    "    \n",
    "    volatility_threshold = df['volatility'].mean() + 0.5 * df['volatility'].std()\n",
    "    \n",
    "    df['regime'] = np.where(\n",
    "        df['volatility'] > volatility_threshold,\n",
    "        np.where(df['trend'] > 0, 'Bull Volatile', 'Bear Volatile'),\n",
    "        np.where(df['trend'] > 0, 'Bull Stable', 'Bear Stable')\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "def evaluate_signal_quality(signals, prediction_horizon=5):\n",
    "    \"\"\"Evaluate signal quality using future price movements\"\"\"\n",
    "    # Calculate future returns for different horizons\n",
    "    for days in range(1, prediction_horizon + 1):\n",
    "        signals[f'future_return_{days}d'] = signals['price'].pct_change(periods=days).shift(-days)\n",
    "    \n",
    "    # Evaluate buy signals\n",
    "    buy_signals = signals[signals['position'] == 1]\n",
    "    if len(buy_signals) > 0:\n",
    "        buy_accuracy = {}\n",
    "        for days in range(1, prediction_horizon + 1):\n",
    "            buy_accuracy[days] = np.mean(buy_signals[f'future_return_{days}d'] > 0)\n",
    "        \n",
    "        print(\"Buy Signal Quality:\")\n",
    "        for days, acc in buy_accuracy.items():\n",
    "            print(f\"  {days}-day accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Evaluate sell signals\n",
    "    sell_signals = signals[signals['position'] == -1]\n",
    "    if len(sell_signals) > 0:\n",
    "        sell_accuracy = {}\n",
    "        for days in range(1, prediction_horizon + 1):\n",
    "            sell_accuracy[days] = np.mean(sell_signals[f'future_return_{days}d'] < 0)\n",
    "        \n",
    "        print(\"Sell Signal Quality:\")\n",
    "        for days, acc in sell_accuracy.items():\n",
    "            print(f\"  {days}-day accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Calculate signal timeliness (how early signals are generated)\n",
    "    # For buy signals: how many days before a significant uptrend\n",
    "    # For sell signals: how many days before a significant downtrend\n",
    "    \n",
    "    # Define significant moves (e.g., 2% in 5 days)\n",
    "    threshold = 0.02\n",
    "    \n",
    "    signals['significant_up'] = np.max([signals[f'future_return_{days}d'] > threshold \n",
    "                                        for days in range(1, prediction_horizon + 1)], axis=0)\n",
    "    signals['significant_down'] = np.max([signals[f'future_return_{days}d'] < -threshold \n",
    "                                          for days in range(1, prediction_horizon + 1)], axis=0)\n",
    "    \n",
    "    # Return signal quality metrics\n",
    "    signal_metrics = {\n",
    "        'buy_precision': buy_accuracy[1] if len(buy_signals) > 0 else 0,\n",
    "        'sell_precision': sell_accuracy[1] if len(sell_signals) > 0 else 0,\n",
    "        'buy_signals_count': len(buy_signals),\n",
    "        'sell_signals_count': len(sell_signals),\n",
    "        'buy_win_rate': np.mean(buy_signals['strategy_return'] > 0) if len(buy_signals) > 0 else 0,\n",
    "        'sell_win_rate': np.mean(sell_signals['strategy_return'] > 0) if len(sell_signals) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return signal_metrics\n",
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Tune hyperparameters using grid search\"\"\"\n",
    "    param_grid = {\n",
    "        'filters': [[64, 128, 256], [128, 256, 512]],\n",
    "        'lstm_units': [[64, 32], [128, 64]],\n",
    "        'dropout_rates': [[0.3, 0.4, 0.5], [0.2, 0.3, 0.4]],\n",
    "        'learning_rates': [0.001, 0.0005, 0.0001]\n",
    "    }\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    # Simple grid search implementation\n",
    "    for filters in param_grid['filters']:\n",
    "        for lstm_units in param_grid['lstm_units']:\n",
    "            for dropout_rates in param_grid['dropout_rates']:\n",
    "                for lr in param_grid['learning_rates']:\n",
    "                    print(f\"Testing: filters={filters}, lstm_units={lstm_units}, \"\n",
    "                          f\"dropout={dropout_rates}, lr={lr}\")\n",
    "                    \n",
    "                    # Build model with current parameters\n",
    "                    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "                    model = build_model_with_params(\n",
    "                        input_shape, \n",
    "                        filters=filters,\n",
    "                        lstm_units=lstm_units,\n",
    "                        dropout_rates=dropout_rates,\n",
    "                        learning_rate=lr\n",
    "                    )\n",
    "                    \n",
    "                    # Train with early stopping\n",
    "                    early_stopping = EarlyStopping(\n",
    "                        monitor='val_accuracy', \n",
    "                        patience=10,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=50,  # Reduced epochs for hyperparameter search\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Get best validation accuracy\n",
    "                    val_accuracy = max(history.history['val_accuracy'])\n",
    "                    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "                    \n",
    "                    # Track results\n",
    "                    results.append({\n",
    "                        'filters': filters,\n",
    "                        'lstm_units': lstm_units,\n",
    "                        'dropout_rates': dropout_rates,\n",
    "                        'learning_rate': lr,\n",
    "                        'val_accuracy': val_accuracy\n",
    "                    })\n",
    "                    \n",
    "                    # Update best parameters\n",
    "                    if val_accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = val_accuracy\n",
    "                        best_params = {\n",
    "                            'filters': filters,\n",
    "                            'lstm_units': lstm_units,\n",
    "                            'dropout_rates': dropout_rates,\n",
    "                            'learning_rate': lr\n",
    "                        }\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    return best_params, pd.DataFrame(results)\n",
    "def optimize_decision_thresholds(model, X_val, y_val, df_val):\n",
    "    \"\"\"Optimize decision thresholds for trading signals\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_val)\n",
    "    \n",
    "    # Initialize variables\n",
    "    best_sharpe = 0\n",
    "    best_threshold_up = 0.5\n",
    "    best_threshold_down = 0.5\n",
    "    \n",
    "    # Grid search over thresholds\n",
    "    for threshold_up in np.arange(0.5, 0.95, 0.05):\n",
    "        for threshold_down in np.arange(0.5, 0.95, 0.05):\n",
    "            # Generate signals with current thresholds\n",
    "            signals = pd.DataFrame(index=range(len(y_pred_proba)))\n",
    "            signals['pred_down_prob'] = y_pred_proba[:, 0]\n",
    "            signals['pred_neutral_prob'] = y_pred_proba[:, 1]\n",
    "            signals['pred_up_prob'] = y_pred_proba[:, 2]\n",
    "            \n",
    "            # Apply thresholds\n",
    "            signals['position'] = 0  # Default is no position\n",
    "            signals.loc[signals['pred_up_prob'] > threshold_up, 'position'] = 1  # Long\n",
    "            signals.loc[signals['pred_down_prob'] > threshold_down, 'position'] = -1  # Short\n",
    "            \n",
    "            # Add price data\n",
    "            if len(df_val) >= len(signals):\n",
    "                signals['price'] = df_val['Close'].values[:len(signals)]\n",
    "                \n",
    "                # Calculate returns\n",
    "                signals['market_return'] = np.log(signals['price'] / signals['price'].shift(1))\n",
    "                signals['strategy_return'] = signals['position'].shift(1) * signals['market_return']\n",
    "                \n",
    "                # Calculate Sharpe ratio\n",
    "                sharpe_ratio = np.sqrt(252) * signals['strategy_return'].mean() / signals['strategy_return'].std() if signals['strategy_return'].std() > 0 else 0\n",
    "                \n",
    "                # Update best thresholds\n",
    "                if sharpe_ratio > best_sharpe:\n",
    "                    best_sharpe = sharpe_ratio\n",
    "                    best_threshold_up = threshold_up\n",
    "                    best_threshold_down = threshold_down\n",
    "    \n",
    "    print(f\"Best thresholds: Up={best_threshold_up:.2f}, Down={best_threshold_down:.2f}\")\n",
    "    print(f\"Best Sharpe ratio: {best_sharpe:.4f}\")\n",
    "    \n",
    "    return best_threshold_up, best_threshold_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for TSLA...\n",
      "Filtered data for TSLA. Count of rows: 0\n",
      "No data found for symbol TSLA.\n",
      "\n",
      "Trading Performance Metrics:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model, signals, performance \u001b[38;5;241m=\u001b[39m run_trading_system_for_symbol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp500_master_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSLA\u001b[39m\u001b[38;5;124m'\u001b[39m,confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.65\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrading Performance Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Return: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mperformance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnual Return: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperformance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_return\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSharpe Ratio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperformance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharpe_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Run the entire pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model, signals, performance = run_trading_system_for_symbol('sp500_master_data.csv','TSLA',confidence_threshold=0.65)\n",
    "    \n",
    "    print(\"\\nTrading Performance Metrics:\")\n",
    "    print(f\"Total Return: {performance['total_return']*100:.2f}%\")\n",
    "    print(f\"Annual Return: {performance['annual_return']*100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {performance['sharpe_ratio']:.2f}\")\n",
    "    print(f\"Maximum Drawdown: {performance['max_drawdown']*100:.2f}%\")\n",
    "    print(f\"Win Rate: {performance['win_rate']*100:.2f}%\")\n",
    "    print(f\"Profit Factor: {performance['profit_factor']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
