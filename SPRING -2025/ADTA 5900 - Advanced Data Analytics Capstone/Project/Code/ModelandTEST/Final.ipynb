{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced feature importance analysis\n",
    "def analyze_feature_importance_for_symbol(model, X_test, y_test, feature_names, symbol):\n",
    "    \"\"\"Analyze feature importance using permutation importance\"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Convert model predictions to class labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate baseline accuracy\n",
    "    baseline_accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    # Reshape X_test for sklearn compatibility\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(\n",
    "        lambda x: np.argmax(model.predict(x.reshape(-1, X_test.shape[1], X_test.shape[2])), axis=1),\n",
    "        X_test_reshaped,\n",
    "        y_test_classes,\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame with importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': result.importances_mean,\n",
    "        'Std': result.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20])\n",
    "    plt.xlabel('Permutation Importance (decrease in accuracy)')\n",
    "    plt.title(f'Feature Importance for {symbol} (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{symbol}_feature_importance.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "def detect_market_regime(df, window=60):\n",
    "    \"\"\"Detect market regime based on volatility and trend\"\"\"\n",
    "    # Calculate rolling volatility\n",
    "    df['volatility'] = df['Returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Calculate rolling trend (slope of linear regression)\n",
    "    def rolling_slope(series, window):\n",
    "        slopes = []\n",
    "        for i in range(len(series) - window + 1):\n",
    "            x = np.arange(window)\n",
    "            y = series.iloc[i:i+window].values\n",
    "            slope, _, _, _, _ = linregress(x, y)\n",
    "            slopes.append(slope)\n",
    "        return pd.Series(np.nan, index=series.index).iloc[window-1:].combine_first(pd.Series(slopes, index=series.index[window-1:len(slopes)+window-1]))\n",
    "    \n",
    "    df['trend'] = rolling_slope(df['Close'], window)\n",
    "    \n",
    "    # Classify regimes\n",
    "    # High volatility, positive trend = 'Bull Volatile'\n",
    "    # High volatility, negative trend = 'Bear Volatile'\n",
    "    # Low volatility, positive trend = 'Bull Stable'\n",
    "    # Low volatility, negative trend = 'Bear Stable'\n",
    "    \n",
    "    volatility_threshold = df['volatility'].mean() + 0.5 * df['volatility'].std()\n",
    "    \n",
    "    df['regime'] = np.where(\n",
    "        df['volatility'] > volatility_threshold,\n",
    "        np.where(df['trend'] > 0, 'Bull Volatile', 'Bear Volatile'),\n",
    "        np.where(df['trend'] > 0, 'Bull Stable', 'Bear Stable')\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "def evaluate_signal_quality(signals, prediction_horizon=5):\n",
    "    \"\"\"Evaluate signal quality using future price movements\"\"\"\n",
    "    # Calculate future returns for different horizons\n",
    "    for days in range(1, prediction_horizon + 1):\n",
    "        signals[f'future_return_{days}d'] = signals['price'].pct_change(periods=days).shift(-days)\n",
    "    \n",
    "    # Evaluate buy signals\n",
    "    buy_signals = signals[signals['position'] == 1]\n",
    "    if len(buy_signals) > 0:\n",
    "        buy_accuracy = {}\n",
    "        for days in range(1, prediction_horizon + 1):\n",
    "            buy_accuracy[days] = np.mean(buy_signals[f'future_return_{days}d'] > 0)\n",
    "        \n",
    "        print(\"Buy Signal Quality:\")\n",
    "        for days, acc in buy_accuracy.items():\n",
    "            print(f\"  {days}-day accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Evaluate sell signals\n",
    "    sell_signals = signals[signals['position'] == -1]\n",
    "    if len(sell_signals) > 0:\n",
    "        sell_accuracy = {}\n",
    "        for days in range(1, prediction_horizon + 1):\n",
    "            sell_accuracy[days] = np.mean(sell_signals[f'future_return_{days}d'] < 0)\n",
    "        \n",
    "        print(\"Sell Signal Quality:\")\n",
    "        for days, acc in sell_accuracy.items():\n",
    "            print(f\"  {days}-day accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Calculate signal timeliness (how early signals are generated)\n",
    "    # For buy signals: how many days before a significant uptrend\n",
    "    # For sell signals: how many days before a significant downtrend\n",
    "    \n",
    "    # Define significant moves (e.g., 2% in 5 days)\n",
    "    threshold = 0.02\n",
    "    \n",
    "    signals['significant_up'] = np.max([signals[f'future_return_{days}d'] > threshold \n",
    "                                        for days in range(1, prediction_horizon + 1)], axis=0)\n",
    "    signals['significant_down'] = np.max([signals[f'future_return_{days}d'] < -threshold \n",
    "                                          for days in range(1, prediction_horizon + 1)], axis=0)\n",
    "    \n",
    "    # Return signal quality metrics\n",
    "    signal_metrics = {\n",
    "        'buy_precision': buy_accuracy[1] if len(buy_signals) > 0 else 0,\n",
    "        'sell_precision': sell_accuracy[1] if len(sell_signals) > 0 else 0,\n",
    "        'buy_signals_count': len(buy_signals),\n",
    "        'sell_signals_count': len(sell_signals),\n",
    "        'buy_win_rate': np.mean(buy_signals['strategy_return'] > 0) if len(buy_signals) > 0 else 0,\n",
    "        'sell_win_rate': np.mean(sell_signals['strategy_return'] > 0) if len(sell_signals) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return signal_metrics\n",
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Tune hyperparameters using grid search\"\"\"\n",
    "    param_grid = {\n",
    "        'filters': [[64, 128, 256], [128, 256, 512]],\n",
    "        'lstm_units': [[64, 32], [128, 64]],\n",
    "        'dropout_rates': [[0.3, 0.4, 0.5], [0.2, 0.3, 0.4]],\n",
    "        'learning_rates': [0.001, 0.0005, 0.0001]\n",
    "    }\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    # Simple grid search implementation\n",
    "    for filters in param_grid['filters']:\n",
    "        for lstm_units in param_grid['lstm_units']:\n",
    "            for dropout_rates in param_grid['dropout_rates']:\n",
    "                for lr in param_grid['learning_rates']:\n",
    "                    print(f\"Testing: filters={filters}, lstm_units={lstm_units}, \"\n",
    "                          f\"dropout={dropout_rates}, lr={lr}\")\n",
    "                    \n",
    "                    # Build model with current parameters\n",
    "                    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "                    model = build_model_with_params(\n",
    "                        input_shape, \n",
    "                        filters=filters,\n",
    "                        lstm_units=lstm_units,\n",
    "                        dropout_rates=dropout_rates,\n",
    "                        learning_rate=lr\n",
    "                    )\n",
    "                    \n",
    "                    # Train with early stopping\n",
    "                    early_stopping = EarlyStopping(\n",
    "                        monitor='val_accuracy', \n",
    "                        patience=10,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=50,  # Reduced epochs for hyperparameter search\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Get best validation accuracy\n",
    "                    val_accuracy = max(history.history['val_accuracy'])\n",
    "                    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "                    \n",
    "                    # Track results\n",
    "                    results.append({\n",
    "                        'filters': filters,\n",
    "                        'lstm_units': lstm_units,\n",
    "                        'dropout_rates': dropout_rates,\n",
    "                        'learning_rate': lr,\n",
    "                        'val_accuracy': val_accuracy\n",
    "                    })\n",
    "                    \n",
    "                    # Update best parameters\n",
    "                    if val_accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = val_accuracy\n",
    "                        best_params = {\n",
    "                            'filters': filters,\n",
    "                            'lstm_units': lstm_units,\n",
    "                            'dropout_rates': dropout_rates,\n",
    "                            'learning_rate': lr\n",
    "                        }\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    return best_params, pd.DataFrame(results)\n",
    "def optimize_decision_thresholds(model, X_val, y_val, df_val):\n",
    "    \"\"\"Optimize decision thresholds for trading signals\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_val)\n",
    "    \n",
    "    # Initialize variables\n",
    "    best_sharpe = 0\n",
    "    best_threshold_up = 0.5\n",
    "    best_threshold_down = 0.5\n",
    "    \n",
    "    # Grid search over thresholds\n",
    "    for threshold_up in np.arange(0.5, 0.95, 0.05):\n",
    "        for threshold_down in np.arange(0.5, 0.95, 0.05):\n",
    "            # Generate signals with current thresholds\n",
    "            signals = pd.DataFrame(index=range(len(y_pred_proba)))\n",
    "            signals['pred_down_prob'] = y_pred_proba[:, 0]\n",
    "            signals['pred_neutral_prob'] = y_pred_proba[:, 1]\n",
    "            signals['pred_up_prob'] = y_pred_proba[:, 2]\n",
    "            \n",
    "            # Apply thresholds\n",
    "            signals['position'] = 0  # Default is no position\n",
    "            signals.loc[signals['pred_up_prob'] > threshold_up, 'position'] = 1  # Long\n",
    "            signals.loc[signals['pred_down_prob'] > threshold_down, 'position'] = -1  # Short\n",
    "            \n",
    "            # Add price data\n",
    "            if len(df_val) >= len(signals):\n",
    "                signals['price'] = df_val['Close'].values[:len(signals)]\n",
    "                \n",
    "                # Calculate returns\n",
    "                signals['market_return'] = np.log(signals['price'] / signals['price'].shift(1))\n",
    "                signals['strategy_return'] = signals['position'].shift(1) * signals['market_return']\n",
    "                \n",
    "                # Calculate Sharpe ratio\n",
    "                sharpe_ratio = np.sqrt(252) * signals['strategy_return'].mean() / signals['strategy_return'].std() if signals['strategy_return'].std() > 0 else 0\n",
    "                \n",
    "                # Update best thresholds\n",
    "                if sharpe_ratio > best_sharpe:\n",
    "                    best_sharpe = sharpe_ratio\n",
    "                    best_threshold_up = threshold_up\n",
    "                    best_threshold_down = threshold_down\n",
    "    \n",
    "    print(f\"Best thresholds: Up={best_threshold_up:.2f}, Down={best_threshold_down:.2f}\")\n",
    "    print(f\"Best Sharpe ratio: {best_sharpe:.4f}\")\n",
    "    \n",
    "    return best_threshold_up, best_threshold_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import linregress\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dynamic Class Weight Callback (assumed from original code)\n",
    "class DynamicClassWeightCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train, initial_weights, adjustment_factor=0.05):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.class_weights = initial_weights.copy()\n",
    "        self.adjustment_factor = adjustment_factor\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_train)\n",
    "        y_true = np.argmax(self.y_train, axis=1)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        errors = y_true != y_pred_classes\n",
    "        for cls in self.class_weights:\n",
    "            class_mask = (y_true == cls)\n",
    "            error_rate = np.mean(errors[class_mask]) if np.any(class_mask) else 0\n",
    "            self.class_weights[cls] *= (1 + self.adjustment_factor * error_rate)\n",
    "        self.model.class_weight = self.class_weights\n",
    "\n",
    "# Preprocess Stock Data\n",
    "def preprocess_stock_data(df):\n",
    "    \"\"\"Preprocess stock data by handling missing values and creating target labels.\"\"\"\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    threshold = 0.003  # Threshold for classifying Up/Down movements\n",
    "    df['target'] = np.where(df['Returns'].shift(-1) > threshold, 2,  # Up\n",
    "                           np.where(df['Returns'].shift(-1) < -threshold, 0, 1))  # Down, Neutral\n",
    "    df = df.dropna(subset=['target'])\n",
    "    df['target'] = df['target'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Feature Selection with Mutual Information\n",
    "def select_features_with_mi(df, features, target_col='target', n_select=30):\n",
    "    \"\"\"Select top features using mutual information.\"\"\"\n",
    "    data = df.dropna(subset=[target_col])\n",
    "    mi_scores = mutual_info_classif(data[features], data[target_col])\n",
    "    mi_df = pd.DataFrame({'Feature': features, 'MI Score': mi_scores})\n",
    "    mi_df = mi_df.sort_values('MI Score', ascending=False)\n",
    "    print(\"Top 15 features by mutual information:\")\n",
    "    print(mi_df.head(15))\n",
    "    return mi_df.head(n_select)['Feature'].tolist()\n",
    "\n",
    "# Prepare Multi-Sequence Data\n",
    "def prepare_multi_sequence_data(df, sequence_lengths, target_column, feature_columns):\n",
    "    \"\"\"Prepare sequences of multiple lengths for model input.\"\"\"\n",
    "    max_length = max(sequence_lengths)\n",
    "    X_dict = {length: [] for length in sequence_lengths}\n",
    "    y = []\n",
    "    for i in range(max_length, len(df)):\n",
    "        for length in sequence_lengths:\n",
    "            X_dict[length].append(df[feature_columns].iloc[i - length:i].values)\n",
    "        y.append(df[target_column].iloc[i])\n",
    "    for length in sequence_lengths:\n",
    "        X_dict[length] = np.array(X_dict[length])\n",
    "    y = np.array(y)\n",
    "    return X_dict, y\n",
    "\n",
    "# Build Multi-Timeframe CNN-BiLSTM Model\n",
    "def build_multi_timeframe_model(sequence_lengths, num_features, num_classes=3):\n",
    "    \"\"\"Build a model with separate branches for each sequence length.\"\"\"\n",
    "    inputs = []\n",
    "    branches = []\n",
    "    for length in sequence_lengths:\n",
    "        input_layer = Input(shape=(length, num_features))\n",
    "        inputs.append(input_layer)\n",
    "        # CNN Block 1\n",
    "        conv1 = Conv1D(filters=128, kernel_size=3, padding='same')(input_layer)\n",
    "        bn1 = BatchNormalization()(conv1)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(act1)\n",
    "        drop1 = Dropout(0.2)(pool1)\n",
    "        # CNN Block 2\n",
    "        conv2 = Conv1D(filters=256, kernel_size=3, padding='same')(drop1)\n",
    "        bn2 = BatchNormalization()(conv2)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(act2)\n",
    "        drop2 = Dropout(0.3)(pool2)\n",
    "        # BiLSTM Layers\n",
    "        lstm1 = Bidirectional(LSTM(units=128, return_sequences=True))(drop2)\n",
    "        drop_lstm1 = Dropout(0.4)(lstm1)\n",
    "        lstm2 = Bidirectional(LSTM(units=64, return_sequences=False))(drop_lstm1)\n",
    "        drop_lstm2 = Dropout(0.4)(lstm2)\n",
    "        branches.append(drop_lstm2)\n",
    "    # Concatenate branches\n",
    "    if len(branches) > 1:\n",
    "        combined = concatenate(branches)\n",
    "    else:\n",
    "        combined = branches[0]\n",
    "    # Dense Layers\n",
    "    dense1 = Dense(64, activation='relu')(combined)\n",
    "    drop_dense = Dropout(0.5)(dense1)\n",
    "    outputs = Dense(num_classes, activation='softmax')(drop_dense)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train Model with Advanced Techniques\n",
    "def train_model_with_advanced_techniques(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, symbol=None):\n",
    "    \"\"\"Train the model with dynamic class weights and callbacks.\"\"\"\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    class_counts = np.bincount(y_train_labels)\n",
    "    total_samples = len(y_train_labels)\n",
    "    class_weights = {i: (total_samples / (len(np.unique(y_train_labels)) * count)) * 1.5 if i != 1 else (total_samples / (len(np.unique(y_train_labels)) * count))\n",
    "                     for i, count in enumerate(class_counts)}\n",
    "    dynamic_weights = DynamicClassWeightCallback(X_train, y_train, initial_weights=class_weights, adjustment_factor=0.05)\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n",
    "        dynamic_weights\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history, model\n",
    "\n",
    "# Generate Advanced Trading Signals\n",
    "def generate_advanced_trading_signals(model, X_test, df_test, confidence_threshold=0.6):\n",
    "    \"\"\"Generate trading signals based on model predictions.\"\"\"\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    signals = pd.DataFrame(index=df_test.index[:len(y_pred_proba)])\n",
    "    signals['pred_down_prob'] = y_pred_proba[:, 0]\n",
    "    signals['pred_neutral_prob'] = y_pred_proba[:, 1]\n",
    "    signals['pred_up_prob'] = y_pred_proba[:, 2]\n",
    "    signals['predicted_class'] = np.argmax(y_pred_proba, axis=1)\n",
    "    signals['confidence'] = np.max(y_pred_proba, axis=1)\n",
    "    signals['position'] = 0\n",
    "    signals.loc[(signals['predicted_class'] == 2) & (signals['confidence'] > confidence_threshold), 'position'] = 1  # Long\n",
    "    signals.loc[(signals['predicted_class'] == 0) & (signals['confidence'] > confidence_threshold), 'position'] = -1  # Short\n",
    "    signals['position_size'] = signals['position'] * (signals['confidence'] - 0.5) * 2\n",
    "    signals.loc[signals['position_size'] < 0, 'position_size'] = signals['position_size'].abs()\n",
    "    signals['price'] = df_test['Close'].values[:len(signals)]\n",
    "    if 'Volatility_20d' in df_test.columns:\n",
    "        signals['volatility'] = df_test['Volatility_20d'].values[:len(signals)]\n",
    "        signals['position_size'] = signals['position_size'] / (1 + signals['volatility'] * 10)\n",
    "    signals['market_return'] = np.log(signals['price'] / signals['price'].shift(1))\n",
    "    signals['strategy_return'] = signals['position'].shift(1) * signals['market_return']\n",
    "    signals['sized_strategy_return'] = signals['position_size'].shift(1) * signals['market_return']\n",
    "    signals['cumulative_market_return'] = np.exp(signals['market_return'].cumsum()) - 1\n",
    "    signals['cumulative_strategy_return'] = np.exp(signals['sized_strategy_return'].cumsum()) - 1\n",
    "    signals['drawdown'] = signals['cumulative_strategy_return'] - signals['cumulative_strategy_return'].cummax()\n",
    "    total_return = np.exp(signals['sized_strategy_return'].sum()) - 1\n",
    "    annual_return = np.exp(signals['sized_strategy_return'].mean() * 252) - 1\n",
    "    sharpe_ratio = np.sqrt(252) * signals['sized_strategy_return'].mean() / signals['sized_strategy_return'].std()\n",
    "    max_drawdown = signals['drawdown'].min()\n",
    "    win_rate = len(signals[signals['sized_strategy_return'] > 0]) / len(signals[signals['sized_strategy_return'] != 0])\n",
    "    gross_profits = signals.loc[signals['sized_strategy_return'] > 0, 'sized_strategy_return'].sum()\n",
    "    gross_losses = abs(signals.loc[signals['sized_strategy_return'] < 0, 'sized_strategy_return'].sum())\n",
    "    profit_factor = gross_profits / gross_losses if gross_losses != 0 else float('inf')\n",
    "    performance_metrics = {\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "        'profit_factor': profit_factor\n",
    "    }\n",
    "    return signals, performance_metrics\n",
    "\n",
    "\n",
    "# Visualize Trading Performance\n",
    "def visualize_trading_performance(signals, performance_metrics, symbol_name):\n",
    "    \"\"\"Visualize trading performance metrics and signals.\"\"\"\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    signals['cumulative_market_return'].plot(ax=ax1, label=f'{symbol_name} Return', color='blue', alpha=0.7)\n",
    "    signals['cumulative_strategy_return'].plot(ax=ax1, label='Strategy Return', color='green')\n",
    "    ax1.set_title(f'Cumulative Returns Comparison - {symbol_name}')\n",
    "    ax1.set_ylabel('Return (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    signals['drawdown'].plot(ax=ax2, color='red')\n",
    "    ax2.set_title('Strategy Drawdown')\n",
    "    ax2.set_ylabel('Drawdown (%)')\n",
    "    ax2.grid(True)\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.plot(signals.index, signals['price'], color='black', alpha=0.7)\n",
    "    buy_signals = signals[signals['position'].diff() > 0]\n",
    "    sell_signals = signals[signals['position'].diff() < 0]\n",
    "    ax3.scatter(buy_signals.index, buy_signals['price'], marker='^', color='green', s=100, label='Buy')\n",
    "    ax3.scatter(sell_signals.index, sell_signals['price'], marker='v', color='red', s=100, label='Sell')\n",
    "    ax3.set_title(f'Trading Signals - {symbol_name}')\n",
    "    ax3.set_ylabel('Price')\n",
    "    ax3.legend()\n",
    "    plt.figtext(0.01, 0.01, f\"\"\"\n",
    "    {symbol_name} Performance Metrics:\n",
    "    - Total Return: {performance_metrics['total_return']*100:.2f}%\n",
    "    - Annual Return: {performance_metrics['annual_return']*100:.2f}%\n",
    "    - Sharpe Ratio: {performance_metrics['sharpe_ratio']:.2f}\n",
    "    - Max Drawdown: {performance_metrics['max_drawdown']*100:.2f}%\n",
    "    - Win Rate: {performance_metrics['win_rate']*100:.2f}%\n",
    "    - Profit Factor: {performance_metrics['profit_factor']:.2f}\n",
    "    \"\"\", fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_trading_system_for_symbol(data_path, symbol, confidence_threshold=0.6, sequence_lengths=[30]):\n",
    "    \"\"\"Run the trading system for a given symbol with multiple sequence lengths.\"\"\"\n",
    "    print(f\"Loading data for {symbol}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "        print(f\"Filtered data for {symbol}. Count of rows: {len(df)}\")\n",
    "    else:\n",
    "        print(f\"No Symbol column found. Assuming data is already for {symbol}.\")\n",
    "    if len(df) == 0:\n",
    "        print(f\"No data found for symbol {symbol}.\")\n",
    "        return None, None, None\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    df = preprocess_stock_data(df)\n",
    "    print(f\"Missing values after imputation: {df.isna().sum().sum()}\")\n",
    "\n",
    "    # Define feature categories\n",
    "    price_indicators = ['Close', 'Returns', 'Log_Returns', 'Price_Range', 'Price_Range_Pct']\n",
    "    moving_averages = [col for col in df.columns if col.startswith(('MA_', 'EMA_', 'Returns_'))]\n",
    "    volatility_metrics = [col for col in df.columns if col.startswith(('Volatility_', 'Volume_MA_', 'BB_Width_'))]\n",
    "    technical_indicators = ['RSI_9', 'RSI_14', 'RSI_25', 'MACD', 'Signal_Line', 'MACD_Histogram',\n",
    "                            'Momentum_14', 'ROC_14', 'MFI_14', 'MFI_28'] + \\\n",
    "                           [col for col in df.columns if col.startswith('Channel_Width_')]\n",
    "    volume_indicators = ['OBV', 'Volume_Ratio', 'Volume_StdDev']\n",
    "    fundamental_features = ['PE_Ratio', 'PB_Ratio', 'Dividend_Yield', 'Profit_Margin', 'Beta', \n",
    "                            'Enterprise_Value', 'Forward_EPS', 'Trailing_EPS']\n",
    "    market_features = ['Market_Return', 'Market_Volatility', 'Rolling_Beta', 'VIX', 'VIX_MA_10']\n",
    "    all_features = (price_indicators + moving_averages + volatility_metrics +\n",
    "                    technical_indicators + volume_indicators + fundamental_features + market_features)\n",
    "    features = [f for f in all_features if f in df.columns]\n",
    "    print(f\"Using {len(features)} features\")\n",
    "\n",
    "    # Feature selection\n",
    "    selected_features = select_features_with_mi(df, features, 'target', n_select=30)\n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "    # Prepare sequence data\n",
    "    print(\"Preparing sequence data...\")\n",
    "    X_dict, y = prepare_multi_sequence_data(df, sequence_lengths, 'target', selected_features)\n",
    "    print(f\"X_dict shapes: {{ {', '.join(f'{length}: {X_dict[length].shape}' for length in sequence_lengths)} }}, y shape: {y.shape}\")\n",
    "    y_onehot = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "    # Split data chronologically\n",
    "    print(\"Splitting data...\")\n",
    "    num_samples = len(y)\n",
    "    train_size = int(0.7 * num_samples)\n",
    "    val_size = int(0.15 * num_samples)\n",
    "    X_train_dict = {length: X_dict[length][:train_size] for length in sequence_lengths}\n",
    "    X_val_dict = {length: X_dict[length][train_size:train_size + val_size] for length in sequence_lengths}\n",
    "    X_test_dict = {length: X_dict[length][train_size + val_size:] for length in sequence_lengths}\n",
    "    y_train = y_onehot[:train_size]\n",
    "    y_val = y_onehot[train_size:train_size + val_size]\n",
    "    y_test = y_onehot[train_size + val_size:]\n",
    "    print(f\"Training set: {{ {', '.join(f'{length}: {X_train_dict[length].shape}' for length in sequence_lengths)} }}, {y_train.shape}\")\n",
    "    print(f\"Validation set: {{ {', '.join(f'{length}: {X_val_dict[length].shape}' for length in sequence_lengths)} }}, {y_val.shape}\")\n",
    "    print(f\"Testing set: {{ {', '.join(f'{length}: {X_test_dict[length].shape}' for length in sequence_lengths)} }}, {y_test.shape}\")\n",
    "    class_counts = np.sum(y_train, axis=0)\n",
    "    print(f\"Class distribution in training set: Down: {class_counts[0]}, Neutral: {class_counts[1]}, Up: {class_counts[2]}\")\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    print(\"Applying SMOTE to training data...\")\n",
    "    num_features = X_dict[sequence_lengths[0]].shape[2]\n",
    "    total_features = sum(length * num_features for length in sequence_lengths)\n",
    "    X_train_flattened = np.hstack([X_train_dict[length].reshape(train_size, -1) for length in sequence_lengths])\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flattened, y_train_labels)\n",
    "\n",
    "    # Reshape back to sequences\n",
    "    X_train_resampled_dict = {}\n",
    "    start = 0\n",
    "    for length in sequence_lengths:\n",
    "        seq_features = length * num_features\n",
    "        X_train_resampled_dict[length] = X_train_resampled[:, start:start + seq_features].reshape(-1, length, num_features)\n",
    "        start += seq_features\n",
    "    y_train_resampled_onehot = tf.keras.utils.to_categorical(y_train_resampled, num_classes=3)\n",
    "    print(f\"Resampled training set: {{ {', '.join(f'{length}: {X_train_resampled_dict[length].shape}' for length in sequence_lengths)} }}, {y_train_resampled_onehot.shape}\")\n",
    "\n",
    "    # Build and train model\n",
    "    print(\"Building model...\")\n",
    "    model = build_multi_timeframe_model(sequence_lengths, num_features, num_classes=3)\n",
    "    model.summary()\n",
    "    print(\"Training model...\")\n",
    "    history, model = train_model_with_advanced_techniques(\n",
    "        model,\n",
    "        [X_train_resampled_dict[length] for length in sequence_lengths],\n",
    "        y_train_resampled_onehot,\n",
    "        [X_val_dict[length] for length in sequence_lengths],\n",
    "        y_val,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        symbol=symbol\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    y_pred_proba = model.predict([X_test_dict[length] for length in sequence_lengths])\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    precision = precision_score(y_test_labels, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_labels, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_labels, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Down', 'Neutral', 'Up'], yticklabels=['Down', 'Neutral', 'Up'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {symbol}')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model Accuracy - {symbol}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model Loss - {symbol}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Generate trading signals\n",
    "    print(\"Generating trading signals...\")\n",
    "    max_length = max(sequence_lengths)\n",
    "    start_index = max_length + train_size + val_size\n",
    "    df_test = df.iloc[start_index:start_index + len(y_test)]\n",
    "    signals, performance_metrics = generate_advanced_trading_signals(\n",
    "        model, \n",
    "        [X_test_dict[length] for length in sequence_lengths], \n",
    "        df_test, \n",
    "        confidence_threshold\n",
    "    )\n",
    "    visualize_trading_performance(signals, performance_metrics, symbol)\n",
    "\n",
    "    return model, signals, performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def _run_trading_system_for_symbol(data_path, symbol, confidence_threshold=0.6, sequence_lengths=[30], train_on_all_data=False):\n",
    "    \"\"\"\n",
    "    Run the trading system for a given symbol with multiple sequence lengths.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the CSV file containing stock data.\n",
    "    - symbol (str): Stock symbol to analyze (e.g., 'AAPL').\n",
    "    - confidence_threshold (float): Threshold for making trading decisions.\n",
    "    - sequence_lengths (list): List of sequence lengths for multi-timeframe analysis.\n",
    "    - train_on_all_data (bool): If True, train the model on all available data without splitting.\n",
    "                                If False, split into training (70%), validation (15%), and testing (15%) sets.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained TensorFlow model.\n",
    "    - signals: Trading signals DataFrame (or None if train_on_all_data=True).\n",
    "    - performance_metrics: Dictionary of performance metrics (or None if train_on_all_data=True).\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    print(f\"Loading data for {symbol}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Assume preprocess_stock_data and feature selection are defined elsewhere\n",
    "    df = preprocess_stock_data(df)\n",
    "    selected_features = select_features_with_mi(df, df.columns, 'target', n_select=30)\n",
    "\n",
    "    # Prepare sequence data\n",
    "    print(\"Preparing sequence data...\")\n",
    "    X_dict, y = prepare_multi_sequence_data(df, sequence_lengths, 'target', selected_features)\n",
    "    y_onehot = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "    # Data splitting based on train_on_all_data\n",
    "    if train_on_all_data:\n",
    "        print(\"Training on all available data...\")\n",
    "        X_train_dict = X_dict\n",
    "        y_train = y_onehot\n",
    "        X_val_dict = None\n",
    "        y_val = None\n",
    "        X_test_dict = None\n",
    "        y_test = None\n",
    "    else:\n",
    "        print(\"Splitting data into train, validation, and test sets...\")\n",
    "        num_samples = len(y)\n",
    "        train_size = int(0.7 * num_samples)\n",
    "        val_size = int(0.15 * num_samples)\n",
    "        X_train_dict = {length: X_dict[length][:train_size] for length in sequence_lengths}\n",
    "        X_val_dict = {length: X_dict[length][train_size:train_size + val_size] for length in sequence_lengths}\n",
    "        X_test_dict = {length: X_dict[length][train_size + val_size:] for length in sequence_lengths}\n",
    "        y_train = y_onehot[:train_size]\n",
    "        y_val = y_onehot[train_size:train_size + val_size]\n",
    "        y_test = y_onehot[train_size + val_size:]\n",
    "\n",
    "    # Build model (assume this function is defined elsewhere)\n",
    "    num_features = X_dict[sequence_lengths[0]].shape[2]\n",
    "    model = build_multi_timeframe_model(sequence_lengths, num_features, num_classes=3)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    if train_on_all_data:\n",
    "        history, model = train_model_with_advanced_techniques(\n",
    "            model,\n",
    "            [X_train_dict[length] for length in sequence_lengths],\n",
    "            y_train,\n",
    "            None,  # No validation data\n",
    "            None,  # No validation labels\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            symbol=symbol\n",
    "        )\n",
    "        print(\"Training complete. Returning model only (no evaluation or signals).\")\n",
    "        return model, None, None\n",
    "    else:\n",
    "        history, model = train_model_with_advanced_techniques(\n",
    "            model,\n",
    "            [X_train_dict[length] for length in sequence_lengths],\n",
    "            y_train,\n",
    "            [X_val_dict[length] for length in sequence_lengths],\n",
    "            y_val,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            symbol=symbol\n",
    "        )\n",
    "\n",
    "        # Evaluate model and generate signals (assume these functions are defined)\n",
    "        y_pred_proba = model.predict([X_test_dict[length] for length in sequence_lengths])\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "        print(f\"Test accuracy: {accuracy_score(y_test_labels, y_pred):.4f}\")\n",
    "\n",
    "        max_length = max(sequence_lengths)\n",
    "        start_index = max_length + train_size + val_size\n",
    "        df_test = df.iloc[start_index:start_index + len(y_test)]\n",
    "        signals, performance_metrics = generate_advanced_trading_signals(\n",
    "            model,\n",
    "            [X_test_dict[length] for length in sequence_lengths],\n",
    "            df_test,\n",
    "            confidence_threshold\n",
    "        )\n",
    "        return model, signals, performance_metrics\n",
    "\n",
    "# Example helper function adjustment (must handle None validation data)\n",
    "def _train_model_with_advanced_techniques(model, X_train, y_train, X_val, y_val, epochs, batch_size, symbol):\n",
    "    if X_val is not None and y_val is not None:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1\n",
    "        )\n",
    "    return history, model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model, signals, performance = run_trading_system_for_symbol(\n",
    "        'stock_data.csv',\n",
    "        'AAPL',\n",
    "        train_on_all_data=True  # Train on all data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the trading system for TSLA with multiple sequence lengths\n",
    "    model, signals, performance = run_trading_system_for_symbol(\n",
    "        'sp500_master_data.csv', \n",
    "        'AAPL', \n",
    "        confidence_threshold=0.85, \n",
    "        sequence_lengths=[7, 30, 60]  # Short-term, medium-term, long-term\n",
    "    )\n",
    "    if performance:\n",
    "        print(\"\\nTrading Performance Metrics:\")\n",
    "        print(f\"Total Return: {performance['total_return']*100:.2f}%\")\n",
    "        print(f\"Annual Return: {performance['annual_return']*100:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {performance['sharpe_ratio']:.2f}\")\n",
    "        print(f\"Maximum Drawdown: {performance['max_drawdown']*100:.2f}%\")\n",
    "        print(f\"Win Rate: {performance['win_rate']*100:.2f}%\")\n",
    "        print(f\"Profit Factor: {performance['profit_factor']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17034\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 179ms/step - accuracy: 0.4559 - loss: 0.6993 - val_accuracy: 0.5163 - val_loss: 0.6938\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.5363 - loss: 0.6925 - val_accuracy: 0.5163 - val_loss: 0.6931\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.5042 - loss: 0.6948 - val_accuracy: 0.5163 - val_loss: 0.6937\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.5107 - loss: 0.6960 - val_accuracy: 0.5359 - val_loss: 0.6932\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.5622 - loss: 0.6928 - val_accuracy: 0.5098 - val_loss: 0.6935\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5373 - loss: 0.6937 - val_accuracy: 0.5163 - val_loss: 0.6937\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.5166 - loss: 0.6940 - val_accuracy: 0.4902 - val_loss: 0.6940\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5319 - loss: 0.6913 - val_accuracy: 0.4902 - val_loss: 0.6945\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.5108 - loss: 0.6943 - val_accuracy: 0.5359 - val_loss: 0.6936\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.5274 - loss: 0.6927 - val_accuracy: 0.4575 - val_loss: 0.6946\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.5439 - loss: 0.6904 - val_accuracy: 0.5033 - val_loss: 0.6937\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.5277 - loss: 0.6922 - val_accuracy: 0.4706 - val_loss: 0.6938\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5341 - loss: 0.6911 - val_accuracy: 0.4575 - val_loss: 0.6945\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5364 - loss: 0.6909 - val_accuracy: 0.4575 - val_loss: 0.6943\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5378 - loss: 0.6904 - val_accuracy: 0.4902 - val_loss: 0.6946\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5514 - loss: 0.6917 - val_accuracy: 0.4641 - val_loss: 0.6950\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5668 - loss: 0.6905 - val_accuracy: 0.4706 - val_loss: 0.6951\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5530 - loss: 0.6906 - val_accuracy: 0.4771 - val_loss: 0.6953\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5473 - loss: 0.6903 - val_accuracy: 0.5033 - val_loss: 0.6958\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5510 - loss: 0.6899 - val_accuracy: 0.4902 - val_loss: 0.6955\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5565 - loss: 0.6901 - val_accuracy: 0.4706 - val_loss: 0.6952\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5760 - loss: 0.6874 - val_accuracy: 0.4706 - val_loss: 0.6962\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5455 - loss: 0.6904 - val_accuracy: 0.4771 - val_loss: 0.6963\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5216 - loss: 0.6928 - val_accuracy: 0.4967 - val_loss: 0.6967\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5645 - loss: 0.6869 - val_accuracy: 0.4902 - val_loss: 0.6958\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5545 - loss: 0.6899 - val_accuracy: 0.4771 - val_loss: 0.6955\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5629 - loss: 0.6880 - val_accuracy: 0.4771 - val_loss: 0.6963\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.5373 - loss: 0.6888 - val_accuracy: 0.4837 - val_loss: 0.6961\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5530 - loss: 0.6902 - val_accuracy: 0.4902 - val_loss: 0.6978\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5467 - loss: 0.6922 - val_accuracy: 0.4902 - val_loss: 0.6962\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5219 - loss: 0.6917 - val_accuracy: 0.4706 - val_loss: 0.6966\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5516 - loss: 0.6904 - val_accuracy: 0.4837 - val_loss: 0.6961\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5485 - loss: 0.6894 - val_accuracy: 0.4771 - val_loss: 0.6972\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.5756 - loss: 0.6883 - val_accuracy: 0.4641 - val_loss: 0.7010\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.5528 - loss: 0.6871 - val_accuracy: 0.4510 - val_loss: 0.7014\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5468 - loss: 0.6892 - val_accuracy: 0.4444 - val_loss: 0.7019\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5413 - loss: 0.6899 - val_accuracy: 0.4902 - val_loss: 0.6976\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5553 - loss: 0.6870 - val_accuracy: 0.4771 - val_loss: 0.6968\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5406 - loss: 0.6874 - val_accuracy: 0.4706 - val_loss: 0.6986\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5713 - loss: 0.6853 - val_accuracy: 0.4444 - val_loss: 0.7024\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5500 - loss: 0.6865 - val_accuracy: 0.4837 - val_loss: 0.6963\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5449 - loss: 0.6873 - val_accuracy: 0.4837 - val_loss: 0.6963\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5488 - loss: 0.6870 - val_accuracy: 0.4771 - val_loss: 0.6979\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5574 - loss: 0.6871 - val_accuracy: 0.4771 - val_loss: 0.7005\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5630 - loss: 0.6862 - val_accuracy: 0.4706 - val_loss: 0.7009\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5335 - loss: 0.6890 - val_accuracy: 0.4641 - val_loss: 0.7025\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5434 - loss: 0.6861 - val_accuracy: 0.4706 - val_loss: 0.7043\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5466 - loss: 0.6873 - val_accuracy: 0.4314 - val_loss: 0.7059\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5548 - loss: 0.6847 - val_accuracy: 0.4706 - val_loss: 0.7127\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5732 - loss: 0.6824 - val_accuracy: 0.4902 - val_loss: 0.7222\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5600 - loss: 0.6843 - val_accuracy: 0.4510 - val_loss: 0.7250\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5873 - loss: 0.6813 - val_accuracy: 0.4706 - val_loss: 0.7152\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5519 - loss: 0.6812 - val_accuracy: 0.4314 - val_loss: 0.7385\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5690 - loss: 0.6824 - val_accuracy: 0.4641 - val_loss: 0.7259\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5898 - loss: 0.6813 - val_accuracy: 0.4510 - val_loss: 0.7333\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5190 - loss: 0.6894 - val_accuracy: 0.4967 - val_loss: 0.6925\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5771 - loss: 0.6882 - val_accuracy: 0.5033 - val_loss: 0.6933\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5089 - loss: 0.6885 - val_accuracy: 0.5490 - val_loss: 0.6929\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5589 - loss: 0.6874 - val_accuracy: 0.5294 - val_loss: 0.6933\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5273 - loss: 0.6875 - val_accuracy: 0.5490 - val_loss: 0.6928\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5316 - loss: 0.6867 - val_accuracy: 0.5359 - val_loss: 0.6932\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.5418 - loss: 0.6850 - val_accuracy: 0.4967 - val_loss: 0.6928\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5720 - loss: 0.6841 - val_accuracy: 0.4837 - val_loss: 0.6933\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5658 - loss: 0.6803 - val_accuracy: 0.5163 - val_loss: 0.6930\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5437 - loss: 0.6862 - val_accuracy: 0.5163 - val_loss: 0.6929\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5489 - loss: 0.6850 - val_accuracy: 0.5163 - val_loss: 0.6937\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5625 - loss: 0.6814 - val_accuracy: 0.5229 - val_loss: 0.6939\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5418 - loss: 0.6833 - val_accuracy: 0.4706 - val_loss: 0.6952\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5574 - loss: 0.6785 - val_accuracy: 0.4837 - val_loss: 0.7029\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5752 - loss: 0.6841 - val_accuracy: 0.4771 - val_loss: 0.7205\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5571 - loss: 0.6792 - val_accuracy: 0.4902 - val_loss: 0.7419\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5859 - loss: 0.6744 - val_accuracy: 0.4379 - val_loss: 0.7554\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5757 - loss: 0.6764 - val_accuracy: 0.4575 - val_loss: 0.7545\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5792 - loss: 0.6821 - val_accuracy: 0.4052 - val_loss: 0.7540\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5921 - loss: 0.6765 - val_accuracy: 0.4706 - val_loss: 0.7454\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5791 - loss: 0.6764 - val_accuracy: 0.4314 - val_loss: 0.7565\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5769 - loss: 0.6761 - val_accuracy: 0.4575 - val_loss: 0.7775\n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5740 - loss: 0.6718 - val_accuracy: 0.4575 - val_loss: 0.7483\n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5726 - loss: 0.6680 - val_accuracy: 0.4575 - val_loss: 0.7465\n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5587 - loss: 0.6706 - val_accuracy: 0.4706 - val_loss: 0.7216\n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5805 - loss: 0.6660 - val_accuracy: 0.4510 - val_loss: 0.7947\n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5697 - loss: 0.6742 - val_accuracy: 0.4444 - val_loss: 0.7641\n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5926 - loss: 0.6699 - val_accuracy: 0.4575 - val_loss: 0.7591\n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5851 - loss: 0.6644 - val_accuracy: 0.4771 - val_loss: 0.7719\n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5881 - loss: 0.6695 - val_accuracy: 0.4706 - val_loss: 0.7633\n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.5989 - loss: 0.6668 - val_accuracy: 0.4575 - val_loss: 0.7805\n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5881 - loss: 0.6741 - val_accuracy: 0.4510 - val_loss: 0.7701\n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5937 - loss: 0.6712 - val_accuracy: 0.4248 - val_loss: 0.8048\n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5848 - loss: 0.6783 - val_accuracy: 0.4641 - val_loss: 0.7565\n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.5995 - loss: 0.6650 - val_accuracy: 0.4510 - val_loss: 0.7540\n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6155 - loss: 0.6628 - val_accuracy: 0.4575 - val_loss: 0.7532\n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.5867 - loss: 0.6670 - val_accuracy: 0.4771 - val_loss: 0.7504\n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.5713 - loss: 0.6668 - val_accuracy: 0.4248 - val_loss: 0.8024\n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.6154 - loss: 0.6692 - val_accuracy: 0.4444 - val_loss: 0.7936\n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.6018 - loss: 0.6586 - val_accuracy: 0.3922 - val_loss: 0.7997\n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.5908 - loss: 0.6630 - val_accuracy: 0.4641 - val_loss: 0.8135\n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.5888 - loss: 0.6707 - val_accuracy: 0.4575 - val_loss: 0.8025\n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5661 - loss: 0.6600 - val_accuracy: 0.4052 - val_loss: 0.8004\n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6030 - loss: 0.6609 - val_accuracy: 0.4510 - val_loss: 0.7910\n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5977 - loss: 0.6602 - val_accuracy: 0.4314 - val_loss: 0.8158\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021926B47740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 801ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021926B47740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step\n",
      "Test Accuracy: 0.4323\n",
      "Entering position at 221.4763336181641 on 2024-08-14 00:00:00\n",
      "Stop-loss triggered at 216.082275390625 on 2024-09-16 00:00:00, return: -2.44%\n",
      "Entering position at 227.9492034912109 on 2024-09-20 00:00:00\n",
      "Stop-loss triggered at 221.4463653564453 on 2024-10-07 00:00:00, return: -2.85%\n",
      "Entering position at 236.8500061035156 on 2025-01-10 00:00:00\n",
      "Stop-loss triggered at 228.25999450683597 on 2025-01-16 00:00:00, return: -3.63%\n",
      "Entering position at 229.97999572753903 on 2025-01-17 00:00:00\n",
      "Stop-loss triggered at 222.63999938964844 on 2025-01-21 00:00:00, return: -3.19%\n",
      "Entering position at 223.8300018310547 on 2025-01-22 00:00:00\n",
      "Take-profit triggered at 238.25999450683597 on 2025-01-28 00:00:00, return: 6.45%\n",
      "Entering position at 239.3600006103516 on 2025-01-29 00:00:00\n",
      "\n",
      "Performance Metrics: {'total_return': -0.058707369805684355, 'sharpe_ratio': -0.8105142694141303, 'max_drawdown': 0.11571587342621624}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Technical Indicator Functions\n",
    "def calculate_moving_averages(df, windows=[50, 200]):\n",
    "    \"\"\"Calculate moving averages for specified windows.\"\"\"\n",
    "    for window in windows:\n",
    "        df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "    return df\n",
    "\n",
    "def calculate_rsi(df, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index (RSI).\"\"\"\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    return df\n",
    "\n",
    "def calculate_macd(df, short_window=12, long_window=26, signal_window=9):\n",
    "    \"\"\"Calculate Moving Average Convergence Divergence (MACD).\"\"\"\n",
    "    short_ema = df['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = df['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "    df['MACD'] = short_ema - long_ema\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=signal_window, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "def calculate_bollinger_bands(df, window=20):\n",
    "    \"\"\"Calculate Bollinger Bands.\"\"\"\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=window).mean()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + 2 * df['Close'].rolling(window=window).std()\n",
    "    df['BB_Lower'] = df['BB_Middle'] - 2 * df['Close'].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "# Load and Preprocess Data\n",
    "def load_and_preprocess_data(file_path, symbol):\n",
    "    \"\"\"Load and preprocess stock data with technical indicators.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df = calculate_moving_averages(df)\n",
    "    df = calculate_rsi(df)\n",
    "    df = calculate_macd(df)\n",
    "    df = calculate_bollinger_bands(df)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.ffill(inplace=True)\n",
    "    \n",
    "    # Define target: 1 if next day's return > 0, else 0\n",
    "    df['Return'] = df['Close'].pct_change().shift(-1)\n",
    "    df['Target'] = np.where(df['Return'] > 0, 1, 0)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Select features\n",
    "    features = ['Close', 'Volume', 'Return', 'MA_50', 'MA_200', 'RSI', 'MACD', 'Signal_Line', 'BB_Middle', 'BB_Upper', 'BB_Lower']\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    return scaled_data, df['Target'].values, features, df\n",
    "\n",
    "# Create Sequences\n",
    "def create_sequences(data, targets, seq_length):\n",
    "    \"\"\"Create sequences for LSTM input.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i - seq_length:i])\n",
    "        y.append(targets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build Model\n",
    "def build_model(seq_length, num_features):\n",
    "    \"\"\"Build a three-layer LSTM model.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, input_shape=(seq_length, num_features), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Simulate Trades\n",
    "def simulate_trades(signals, df, stop_loss_pct=0.02, take_profit_pct=0.05, max_holding_days=30):\n",
    "    \"\"\"Simulate trades with stop-loss and take-profit.\"\"\"\n",
    "    portfolio = 10000  # Initial capital\n",
    "    position = 0  # 0: no position, 1: long position\n",
    "    entry_price = 0\n",
    "    days_held = 0\n",
    "    trade_returns = []\n",
    "    portfolio_values = [portfolio]\n",
    "\n",
    "    for date, row in signals.iterrows():\n",
    "        signal = row['Signal']\n",
    "        close_price = df.loc[date, 'Close']\n",
    "\n",
    "        if position == 0:  # No position\n",
    "            if signal == 'Buy':\n",
    "                position = 1\n",
    "                entry_price = close_price\n",
    "                days_held = 0\n",
    "                print(f\"Entering position at {close_price} on {date}\")\n",
    "        elif position == 1:  # Holding position\n",
    "            days_held += 1\n",
    "            if close_price <= entry_price * (1 - stop_loss_pct):\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Stop-loss triggered at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "            elif close_price >= entry_price * (1 + take_profit_pct):\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Take-profit triggered at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "            elif days_held >= max_holding_days:\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Max holding period reached, exiting at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "\n",
    "        portfolio_values.append(portfolio)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    daily_returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    total_return = (portfolio - 10000) / 10000\n",
    "    sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252) if daily_returns.std() != 0 else 0\n",
    "    max_drawdown = (pd.Series(portfolio_values).cummax() - pd.Series(portfolio_values)).max() / pd.Series(portfolio_values).cummax().max()\n",
    "\n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Main Trading System Function\n",
    "def run_trading_system(file_path, symbol='AAPL', seq_length=30, confidence_threshold=0.6):\n",
    "    \"\"\"Run the trading system with updated features and strategy.\"\"\"\n",
    "    # Load and preprocess data\n",
    "    scaled_data, targets, feature_cols, df = load_and_preprocess_data(file_path, symbol)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data, targets, seq_length)\n",
    "    \n",
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    num_samples = len(X)\n",
    "    train_size = int(0.7 * num_samples)\n",
    "    val_size = int(0.15 * num_samples)\n",
    "    X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]\n",
    "    y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_model(seq_length, len(feature_cols))\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > confidence_threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generate trading signals\n",
    "    start_index = seq_length + train_size + val_size\n",
    "    test_dates = df.index[start_index:start_index + len(y_test)]\n",
    "    signals = pd.DataFrame(index=test_dates)\n",
    "    signals['Signal'] = np.where(y_pred == 1, 'Buy', 'Hold')\n",
    "    \n",
    "    # Simulate trades\n",
    "    performance_metrics = simulate_trades(signals, df)\n",
    "    \n",
    "    return model, signals, performance_metrics\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model, signals, performance = run_trading_system(\n",
    "        file_path='sp500_master_data.csv',\n",
    "        symbol='AAPL',\n",
    "        seq_length=30,\n",
    "        confidence_threshold=0.6\n",
    "    )\n",
    "    print(\"\\nPerformance Metrics:\", performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17034\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 84ms/step - accuracy: 0.4598 - loss: 0.6946 - val_accuracy: 0.5163 - val_loss: 0.6928\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5044 - loss: 0.6941 - val_accuracy: 0.5163 - val_loss: 0.6923\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.4932 - loss: 0.6942 - val_accuracy: 0.5163 - val_loss: 0.6925\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5020 - loss: 0.6945 - val_accuracy: 0.5163 - val_loss: 0.6931\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5025 - loss: 0.6935 - val_accuracy: 0.5163 - val_loss: 0.6932\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5012 - loss: 0.6941 - val_accuracy: 0.5163 - val_loss: 0.6942\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5037 - loss: 0.6933 - val_accuracy: 0.5163 - val_loss: 0.6926\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5074 - loss: 0.6931 - val_accuracy: 0.5163 - val_loss: 0.6923\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5012 - loss: 0.6930 - val_accuracy: 0.5163 - val_loss: 0.6928\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4985 - loss: 0.6925 - val_accuracy: 0.5163 - val_loss: 0.6930\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.5049 - loss: 0.6921 - val_accuracy: 0.5163 - val_loss: 0.6929\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5057 - loss: 0.6920 - val_accuracy: 0.5163 - val_loss: 0.6934\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5078 - loss: 0.6933 - val_accuracy: 0.5163 - val_loss: 0.6934\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5003 - loss: 0.6918 - val_accuracy: 0.5163 - val_loss: 0.6939\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5196 - loss: 0.6925 - val_accuracy: 0.5163 - val_loss: 0.6943\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5107 - loss: 0.6912 - val_accuracy: 0.5163 - val_loss: 0.6941\n",
      "Epoch 17/50\n",
      "\u001b[1m12/23\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5095 - loss: 0.6923"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path, symbol, seq_length):\n",
    "    \"\"\"\n",
    "    Load stock data from a CSV file, preprocess it, and create sequences for the specified symbol.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing stock data.\n",
    "        symbol (str): Stock symbol to filter (e.g., 'AAPL').\n",
    "        seq_length (int): Number of time steps in each input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Input sequences (X).\n",
    "        np.array: Target values (y).\n",
    "        list: List of feature column names.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "    else:\n",
    "        raise ValueError(f\"No data found for symbol '{symbol}' in the dataset.\")\n",
    "    \n",
    "    # Convert 'Date' to datetime and set as index\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Handle missing values with forward fill\n",
    "    df.ffill(inplace=True)\n",
    "    \n",
    "    # Define target: 1 if next day's return > 0, else 0\n",
    "    df['Return'] = df['Close'].pct_change().shift(-1)\n",
    "    df['Target'] = np.where(df['Return'] > 0, 1, 0)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Select features (customize as needed)\n",
    "    features = ['Close', 'Volume', 'Return']\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(scaled_data)):\n",
    "        X.append(scaled_data[i - seq_length:i])\n",
    "        y.append(df['Target'].iloc[i])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "# Function to build the CNN-BiLSTM model\n",
    "def build_cnn_bilstm_model(seq_length, num_features, num_classes=1):\n",
    "    \"\"\"\n",
    "    Build a CNN-BiLSTM model for time series forecasting in a trading system.\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): Number of time steps in each input sequence.\n",
    "        num_features (int): Number of features in the input data.\n",
    "        num_classes (int): Number of output classes (default: 1 for binary classification).\n",
    "    \n",
    "    Returns:\n",
    "        Sequential: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # CNN layers for feature extraction\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_length, num_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    # BiLSTM layers for temporal dependencies\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Dense layers for prediction\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy' if num_classes == 1 else 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split the data into training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Input sequences.\n",
    "        y (np.array): Target values.\n",
    "        train_ratio (float): Proportion of data for training.\n",
    "        val_ratio (float): Proportion of data for validation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    num_samples = len(X)\n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = int(val_ratio * num_samples)\n",
    "    X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]\n",
    "    y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Simulate Trades\n",
    "def simulate_trades(signals, df, stop_loss_pct=0.02, take_profit_pct=0.05, max_holding_days=30):\n",
    "    \"\"\"Simulate trades with stop-loss and take-profit.\"\"\"\n",
    "    portfolio = 10000  # Initial capital\n",
    "    position = 0  # 0: no position, 1: long position\n",
    "    entry_price = 0\n",
    "    days_held = 0\n",
    "    trade_returns = []\n",
    "    portfolio_values = [portfolio]\n",
    "\n",
    "    for date, row in signals.iterrows():\n",
    "        signal = row['Signal']\n",
    "        close_price = df.loc[date, 'Close']\n",
    "\n",
    "        if position == 0:  # No position\n",
    "            if signal == 'Buy':\n",
    "                position = 1\n",
    "                entry_price = close_price\n",
    "                days_held = 0\n",
    "                print(f\"Entering position at {close_price} on {date}\")\n",
    "        elif position == 1:  # Holding position\n",
    "            days_held += 1\n",
    "            if close_price <= entry_price * (1 - stop_loss_pct):\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Stop-loss triggered at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "            elif close_price >= entry_price * (1 + take_profit_pct):\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Take-profit triggered at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "            elif days_held >= max_holding_days:\n",
    "                position = 0\n",
    "                trade_return = (close_price - entry_price) / entry_price\n",
    "                portfolio *= (1 + trade_return)\n",
    "                trade_returns.append(trade_return)\n",
    "                print(f\"Max holding period reached, exiting at {close_price} on {date}, return: {trade_return:.2%}\")\n",
    "\n",
    "        portfolio_values.append(portfolio)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    daily_returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    total_return = (portfolio - 10000) / 10000\n",
    "    sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252) if daily_returns.std() != 0 else 0\n",
    "    max_drawdown = (pd.Series(portfolio_values).cummax() - pd.Series(portfolio_values)).max() / pd.Series(portfolio_values).cummax().max()\n",
    "\n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Main function to run the trading system\n",
    "def run_trading_system(file_path, symbol='AAPL', seq_length=30, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Run the trading system for a given stock symbol using a CNN-BiLSTM model.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file with stock data.\n",
    "        symbol (str): Stock symbol (default: 'AAPL').\n",
    "        seq_length (int): Number of days in each sequence (default: 30).\n",
    "        confidence_threshold (float): Threshold for generating buy signals (default: 0.6).\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained CNN-BiLSTM model.\n",
    "        signals (pd.DataFrame): Generated trading signals.\n",
    "        performance_metrics (dict): Basic performance metrics.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    X, y, feature_cols = load_and_preprocess_data(file_path, symbol, seq_length)\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "    \n",
    "    # Build the CNN-BiLSTM model\n",
    "    num_features = X.shape[2]\n",
    "    model = build_cnn_bilstm_model(seq_length, num_features)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > confidence_threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generate trading signals\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'Symbol' in df.columns:\n",
    "        df = df[df['Symbol'] == symbol].copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    signals = pd.DataFrame(index=df.index[seq_length + len(X_train) + len(X_val): seq_length + len(X_train) + len(X_val) + len(y_test)])\n",
    "    signals['Signal'] = np.where(y_pred == 1, 'Buy', 'Hold')\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    performance_metrics = simulate_trades(signals, df)\n",
    "    \n",
    "    return model, signals, performance_metrics\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'sp500_master_data.csv' with the path to your stock data CSV\n",
    "    model, signals, performance = run_trading_system(\n",
    "        file_path='sp500_master_data.csv',\n",
    "        symbol='AAPL',\n",
    "        seq_length=30,\n",
    "        confidence_threshold=0.6\n",
    "    )\n",
    "    print(\"\\nPerformance Metrics:\", performance)\n",
    "    print(\"\\nFirst few trading signals:\\n\", signals.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
