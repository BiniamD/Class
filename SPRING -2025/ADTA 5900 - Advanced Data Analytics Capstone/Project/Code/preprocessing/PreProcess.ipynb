{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structure\n",
    "#preprocessing/\n",
    "#── __init__\n",
    "#├── data_loader\n",
    "#├── feature_engineer\n",
    "#├── data_validator\n",
    "#├── sequence_creator\n",
    "#├── utils\n",
    "#└── main\n",
    "\n",
    "# data_loader.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Union\n",
    "\n",
    "# feature_engineer\n",
    "from typing import List\n",
    "import logging\n",
    "import ta\n",
    "\n",
    "# main.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Recommended feature engineering enhancements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and perform initial cleaning of data\"\"\"\n",
    "        try:\n",
    "            # Read the data\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert date to datetime\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "            \n",
    "            # Sort by date and symbol\n",
    "            df = df.sort_values(['Symbol', 'Date'])\n",
    "            \n",
    "            self.logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, train_end: str = '2023-12-31',\n",
    "                   val_end: str = '2024-06-30') -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "        try:\n",
    "            train = df[df['Date'] <= train_end]\n",
    "            val = df[(df['Date'] > train_end) & (df['Date'] <= val_end)]\n",
    "            test = df[df['Date'] > val_end]\n",
    "            \n",
    "            splits = {\n",
    "                'train': train,\n",
    "                'val': val,\n",
    "                'test': test\n",
    "            }\n",
    "            \n",
    "            for split_name, split_data in splits.items():\n",
    "                self.logger.info(f\"{split_name} set shape: {split_data.shape}\")\n",
    "                \n",
    "            return splits\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error splitting data: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def calculate_features(self, df: pd.DataFrame,missing_columns) -> pd.DataFrame:\n",
    "        \"\"\"Calculate all technical indicators and features\"\"\"\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            \n",
    "            # only calculate features for the missing columns \n",
    "            for feature in missing_columns:\n",
    "                if feature == 'SMA':\n",
    "                    df['SMA'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
    "                elif feature == 'EMA':\n",
    "                    df['EMA'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
    "                elif feature == 'RSI':\n",
    "                    df['RSI'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "                elif feature == 'MACD':\n",
    "                    df['MACD'] = ta.trend.macd_diff(df['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "                elif feature == 'STOCH':\n",
    "                    df['STOCH'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'], window=14)\n",
    "                elif feature == 'WILLIAMS':\n",
    "                    df['WILLIAMS'] = ta.momentum.wr(df['High'], df['Low'], df['Close'], lbp=14)\n",
    "                elif feature == 'CCI':\n",
    "                    df['CCI'] = ta.trend.cci(df['High'], df['Low'], df['Close'], window=20)\n",
    "                elif feature == 'ATR':\n",
    "                    df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=14)\n",
    "                elif feature == 'ADX':\n",
    "                    df['ADX'] = ta.trend.adx(df['High'], df['Low'], df['Close'], window=14)\n",
    "                elif feature == 'OBV':\n",
    "                    df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
    "                elif feature == 'AD':\n",
    "                    df['AD'] = ta.volume.acc_dist_index(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "                elif feature == 'MA20':\n",
    "                    df['MA_20'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20).mean())\n",
    "                elif feature == 'MA50':\n",
    "                    df['MA_50'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=50).mean())\n",
    "                elif feature == 'MA200':\n",
    "                    df['MA_200'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=200).mean())\n",
    "                elif feature == 'BB_Width_20':\n",
    "                    df['BB_Width_20'] = (df['Upper_Band'] - df['Lower_Band']) / df['MA20']\n",
    "                elif feature == 'ROC':\n",
    "                    df['ROC_14'] = df.groupby('Symbol')['Close'].transform(lambda x: x.pct_change(periods=14) * 100)\n",
    "                elif feature == 'Momentum':\n",
    "                    df['Momentum'] = df.groupby('Symbol')['Close'].transform(lambda x: x - x.shift(10))  \n",
    "                elif feature == 'MACD_Histogram':\n",
    "                    df['MACD_Histogram'] = ta.trend.macd_signal(df['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "                elif feature == 'RSI_MACD_Signal':\n",
    "                    df['RSI_MACD_Signal'] = df['RSI_14'] * df['MACD_Histogram'].apply(  lambda x: 1 if x > 0 else -1)\n",
    "                elif feature == 'Trend_Strength':\n",
    "                    df['Trend_Strength'] = (df['Close'] - df['MA_50']) / df['MA_50'] * 100\n",
    "\n",
    "                else:\n",
    "                    self.logger.warning(f\"Feature {feature} not recognized\")              \n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating features: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        features_to_normalize = ['Close', 'ATR', 'BB_Width_20', 'Volume']\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        df_scaled = df.copy()\n",
    "        for symbol in df['Symbol'].unique():\n",
    "            mask = df['Symbol'] == symbol\n",
    "            for feature in features_to_normalize:\n",
    "                df_scaled.loc[mask, f'{feature}_Norm'] = scaler.fit_transform(\n",
    "                    df.loc[mask, feature].values.reshape(-1, 1)\n",
    "                )\n",
    "        \n",
    "        return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_creator.py\n",
    "class SequenceCreator:\n",
    "    def __init__(self, sequence_length: int = 20):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def create_sequences(self, df: pd.DataFrame, \n",
    "                        feature_columns: List[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create sequences for deep learning model\"\"\"\n",
    "        try:\n",
    "            sequences = []\n",
    "            targets = []\n",
    "            \n",
    "            # Create sequences for each symbol\n",
    "            for symbol in df['Symbol'].unique():\n",
    "                symbol_data = df[df['Symbol'] == symbol]\n",
    "                \n",
    "                for i in range(len(symbol_data) - self.sequence_length):\n",
    "                    # Extract sequence\n",
    "                    sequence = symbol_data[feature_columns].iloc[i:(i + self.sequence_length)].values\n",
    "                    target = symbol_data['Returns'].iloc[i + self.sequence_length]\n",
    "                    \n",
    "                    sequences.append(sequence)\n",
    "                    targets.append(target)\n",
    "            \n",
    "            return np.array(sequences), np.array(targets)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating sequences: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 13:42:38,586 - __main__ - INFO - Loading data...\n",
      "2025-03-22 13:42:39,380 - __main__ - INFO - Loaded data shape: (43460, 77)\n",
      "2025-03-22 13:42:39,381 - __main__ - INFO - Calculating features...\n",
      "2025-03-22 13:42:39,755 - __main__ - INFO - Splitting data...\n",
      "2025-03-22 13:42:39,768 - __main__ - INFO - train set shape: (33136, 80)\n",
      "2025-03-22 13:42:39,769 - __main__ - INFO - val set shape: (4238, 80)\n",
      "2025-03-22 13:42:39,770 - __main__ - INFO - test set shape: (6086, 80)\n",
      "2025-03-22 13:42:39,770 - __main__ - INFO - Creating sequences for train split...\n",
      "2025-03-22 13:42:58,681 - __main__ - INFO - train sequences shape: X=(32436, 20, 11), y=(32436,)\n",
      "2025-03-22 13:42:58,682 - __main__ - INFO - Creating sequences for val split...\n",
      "2025-03-22 13:43:00,793 - __main__ - INFO - val sequences shape: X=(3538, 20, 11), y=(3538,)\n",
      "2025-03-22 13:43:00,794 - __main__ - INFO - Creating sequences for test split...\n",
      "2025-03-22 13:43:03,945 - __main__ - INFO - test sequences shape: X=(5406, 20, 11), y=(5406,)\n",
      "2025-03-22 13:43:03,946 - __main__ - INFO - Saving preprocessed data...\n",
      "2025-03-22 13:43:04,005 - __main__ - INFO - Preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize components\n",
    "        data_loader = DataLoader()\n",
    "        feature_engineer = FeatureEngineer()\n",
    "        sequence_creator = SequenceCreator(sequence_length=20)\n",
    "        \n",
    "        # Load data\n",
    "        logger.info(\"Loading data...\")\n",
    "        df = data_loader.load_data('sp500_master_data.csv')\n",
    "        \n",
    "        # Calculate features\n",
    "        logger.info(\"Calculating features...\")\n",
    "        required_columns = [ 'Close', 'Returns', 'RSI_14', 'MACD', 'MA_20',  'BB_Width_20', 'ATR', 'ROC_14', 'Volume',    'RSI_MACD_Signal', 'Trend_Strength'\n",
    "]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        df = feature_engineer.calculate_features(df,missing_columns)\n",
    "        \n",
    "        # Split data\n",
    "        logger.info(\"Splitting data...\")\n",
    "        splits = data_loader.split_data(df)\n",
    "        \n",
    "        # Create sequences for each split\n",
    "        sequences = {}\n",
    "        for split_name, split_data in splits.items():\n",
    "            logger.info(f\"Creating sequences for {split_name} split...\")\n",
    "            \n",
    "            # Normalize features\n",
    "            split_data = feature_engineer.normalize_features(split_data)\n",
    "            \n",
    "            # Create sequences\n",
    "            X, y = sequence_creator.create_sequences(\n",
    "                split_data,\n",
    "                feature_columns=required_columns\n",
    "            )\n",
    "            \n",
    "            sequences[split_name] = {\n",
    "                'X': X,\n",
    "                'y': y\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"{split_name} sequences shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Save preprocessed data\n",
    "        logger.info(\"Saving preprocessed data...\")\n",
    "        output_dir = Path('preprocessed_data')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for split_name, split_sequences in sequences.items():\n",
    "            np.save(output_dir / f'X_{split_name}.npy', split_sequences['X'])\n",
    "            np.save(output_dir / f'y_{split_name}.npy', split_sequences['y'])\n",
    "        \n",
    "        logger.info(\"Preprocessing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main preprocessing pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:21:53,045 - __main__ - INFO - Loaded data shape: (623756, 77)\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "df = data_loader.load_data('../stock_data/sp500_master_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
       "       'Stock Splits', 'Symbol', 'Sector', 'Industry', 'Market_Cap', 'Returns',\n",
       "       'Log_Returns', 'Price_Range', 'Price_Range_Pct', 'MA_5', 'EMA_5',\n",
       "       'Returns_5d', 'MA_10', 'EMA_10', 'Returns_10d', 'MA_20', 'EMA_20',\n",
       "       'Returns_20d', 'MA_50', 'EMA_50', 'Returns_50d', 'MA_200', 'EMA_200',\n",
       "       'Returns_200d', 'Volatility_5d', 'Volume_MA_5d', 'Volatility_20d',\n",
       "       'Volume_MA_20d', 'Volatility_60d', 'Volume_MA_60d', 'RSI_9', 'RSI_14',\n",
       "       'RSI_25', 'MACD', 'Signal_Line', 'MACD_Histogram', 'STD_20d',\n",
       "       'BB_Upper_20', 'BB_Lower_20', 'BB_Width_20', 'STD_50d', 'BB_Upper_50',\n",
       "       'BB_Lower_50', 'BB_Width_50', 'Momentum_14', 'ROC_14', 'MFI_14',\n",
       "       'MFI_28', 'OBV', 'Volume_Ratio', 'Volume_StdDev', 'Upper_Channel_20',\n",
       "       'Lower_Channel_20', 'Channel_Width_20', 'Upper_Channel_50',\n",
       "       'Lower_Channel_50', 'Channel_Width_50', 'PE_Ratio', 'PB_Ratio',\n",
       "       'Dividend_Yield', 'Profit_Margin', 'Beta', 'Enterprise_Value',\n",
       "       'Forward_EPS', 'Trailing_EPS', 'Market_Return', 'Market_Volatility',\n",
       "       'Rolling_Beta', 'VIX', 'VIX_MA_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_columns=['Close', 'Returns', 'RSI_14', 'MACD', 'MA_20',  'BB_Width_20', 'ATR', 'ROC_14', 'Volume'] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
