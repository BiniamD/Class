{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chapter 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.RandomState(20)\n",
    "\n",
    "# Load the dataset\n",
    "loblollyPine = pd.read_csv('loblollyPineSample.csv')\n",
    "\n",
    "# Set proportions of train-validate-test split\n",
    "trainPropPercent = 0.65\n",
    "validatePropPercent = 0.1\n",
    "testPropPercent = 0.25\n",
    "\n",
    "# Split dataset into training/validation data and testing data\n",
    "#Split the dataset into training/validation and test datasets using train_test_split() with the following parameters:\n",
    "#the original dataframe\n",
    "#the test_size parameter set to the test proportion\n",
    "#the random_state parameter set to rng\n",
    "\n",
    "# Split dataset into training/validation data and testing data\n",
    "\n",
    "trainAndValidate, testDataPercent =  train_test_split(\n",
    "    loblollyPine, \n",
    "    test_size=testPropPercent,\n",
    "    random_state=rng\n",
    ")\n",
    "\n",
    "# Split training/validation data into training data and validation data\n",
    "trainDataPercent, validateDataPercent = train_test_split(\n",
    "    trainAndValidate, \n",
    "    train_size=trainPropPercent/(trainPropPercent+validatePropPercent),\n",
    "    random_state=rng\n",
    ")\n",
    "\n",
    "# Print split sizes and test dataset\n",
    "print('original dataset:', len(loblollyPine), \n",
    "    '\\ntrain_data:', len(trainDataPercent), \n",
    "    '\\nvalidation_data:', len(validateDataPercent), \n",
    "    '\\n\\ntest_data:', len(testDataPercent),\n",
    "    '\\n', testDataPercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblollySample.csv')\n",
    "\n",
    "# Set proportions of train-validate-test split\n",
    "trainPropPercent = 0.7\n",
    "validatePropPercent = 0.1\n",
    "testPropPercent = 0.2\n",
    "\n",
    "# Split dataset into training/validation data and testing data\n",
    "trainAndValidate, testDataPercent = train_test_split(\n",
    "    loblolly, \n",
    "    test_size=testPropPercent,\n",
    "    random_state=rng\n",
    ")\n",
    "\n",
    "# Split training/validation data into training data and validation data\n",
    "\n",
    "trainDataPercent, validateDataPercent =  train_test_split(\n",
    "    trainAndValidate, \n",
    "    train_size=trainPropPercent/(trainPropPercent+validatePropPercent),\n",
    "    random_state=rng\n",
    ")\n",
    "\n",
    "# Print split sizes and test dataset\n",
    "print('original dataset:', len(loblolly), \n",
    "    '\\ntrain_data:', len(trainDataPercent), \n",
    "    '\\nvalidation_data:', len(validateDataPercent), \n",
    "    '\\n\\ntest_data:', len(testDataPercent),\n",
    "    '\\n', testDataPercent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "loblollyPine = pd.read_csv('loblollyPine_sample.csv')\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = loblollyPine['age']\n",
    "y = loblollyPine['height']\n",
    "\n",
    "# Initialize the model -- polynomial regression model\n",
    "polyFeatures = PolynomialFeatures(degree=2, include_bias=False)\n",
    "xPoly = polyFeatures.fit_transform(X.values.reshape(-1, 1))\n",
    "polyModel = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "polyModel = polyModel.fit(xPoly, y)\n",
    "\n",
    "# Make predictions\n",
    "yPredicted = polyModel.predict(xPoly)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# Your code goes here \n",
    "MSE = mean_squared_error(y, yPredicted) \n",
    "RMSE = np.sqrt(MSE) \n",
    "\n",
    "# Print metrics\n",
    "print('Mean squared error =', MSE)\n",
    "print('Root mean squared error =', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblolly_sample.csv')\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = loblolly['age']\n",
    "y = loblolly['height']\n",
    "\n",
    "# Initialize the model -- polynomial regression model\n",
    "polyFeatures = PolynomialFeatures(degree=3, include_bias=False)\n",
    "xPoly = polyFeatures.fit_transform(X.values.reshape(-1, 1))\n",
    "polyModel = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "polyModel = polyModel.fit(xPoly, y)\n",
    "\n",
    "# Make predictions\n",
    "yPrediction = polyModel.predict(xPoly)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# Your code goes here \n",
    "MAE = mean_absolute_error(y, yPrediction)\n",
    "\n",
    "# Print metric\n",
    "print('Mean absolute error =', MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "${meaningful.data_name} = pd.read_csv('${meaningful.data_name}Sample.csv')\n",
    "\n",
    "# Convert Sex to 0 and 1.\n",
    "${meaningful.data_name}.loc[${meaningful.data_name}['Sex'] == 'F', 'Sex'] = 0\n",
    "${meaningful.data_name}.loc[${meaningful.data_name}['Sex'] == 'M', 'Sex'] = 1\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = ${meaningful.data_name}[['${meaningful.x_name}']].values.reshape(-1, 1)\n",
    "y = ${meaningful.data_name}[['Sex']].values.reshape(-1, 1).astype(int)\n",
    "\n",
    "# Initialize and fit the model\n",
    "logisticModel = LogisticRegression()\n",
    "logisticModel = logisticModel.fit(X, np.ravel(y.astype(int)))\n",
    "\n",
    "# Make predictions\n",
    "yPred = logisticModel.predict(X).reshape(-1, 1).astype(int)\n",
    "\n",
    "# Evaluate accuracy -- accuracy score\n",
    "score =  accuracy_score(y, yPred)\n",
    "\n",
    "print('Accuracy score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Load the dataset\n",
    "cats = pd.read_csv('catsSample.csv')\n",
    "\n",
    "# Convert Sex to 0 and 1.\n",
    "cats.loc[cats['Sex'] == 'F', 'Sex'] = 0\n",
    "cats.loc[cats['Sex'] == 'M', 'Sex'] = 1\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = cats[['Body_weight']].values.reshape(-1, 1)\n",
    "y = cats[['Sex']].values.reshape(-1, 1).astype(int)\n",
    "\n",
    "# Initialize and fit the model\n",
    "logisticModel = LogisticRegression()\n",
    "logisticModel = logisticModel.fit(X, np.ravel(y.astype(int)))\n",
    "\n",
    "# Make predictions\n",
    "yPredicted = logisticModel.predict(X).reshape(-1, 1).astype(int)\n",
    "\n",
    "# Evaluate accuracy -- precision score\n",
    "score =  precision_score(y, yPredicted)\n",
    "\n",
    "print('Precision score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Load the dataset\n",
    "cats = pd.read_csv('catsSample.csv')\n",
    "\n",
    "# Convert Sex to 0 and 1.\n",
    "cats.loc[cats['Sex'] == 'F', 'Sex'] = 0\n",
    "cats.loc[cats['Sex'] == 'M', 'Sex'] = 1\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = cats[['Heart_weight']].values.reshape(-1, 1)\n",
    "y = cats[['Sex']].values.reshape(-1, 1).astype(int)\n",
    "\n",
    "# Initialize and fit the model\n",
    "logisticModel = LogisticRegression()\n",
    "logisticModel = logisticModel.fit(X, np.ravel(y.astype(int)))\n",
    "\n",
    "# Make predictions\n",
    "yPredicted = logisticModel.predict(X).reshape(-1, 1).astype(int)\n",
    "\n",
    "# Evaluate accuracy -- recall score\n",
    "score =     recall_score(y, yPredicted)\n",
    "\n",
    "print('Recall score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "rng = np.random.RandomState(36)\n",
    "\n",
    "# Load the dataset\n",
    "pines = pd.read_csv('pinesSample.csv')\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "trainingDataName, testingDataName =  train_test_split(\n",
    "    pines, \n",
    "    test_size=0.30,\n",
    "    random_state=rng\n",
    ")\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = trainingDataName[['age']]\n",
    "y = trainingDataName[['height']]\n",
    "\n",
    "# Initialize the model -- quadratic polynomial regression model\n",
    "polyFeatures = PolynomialFeatures(degree=2, include_bias=False)\n",
    "xPoly = polyFeatures.fit_transform(X)\n",
    "polyRegModel = LinearRegression()\n",
    "\n",
    "# Evaluate accuracy\n",
    "# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.\n",
    "tenFoldScores = -cross_val_score(polyRegModel, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "print('ten-fold average MSE =', np.mean(tenFoldScores), '\\n', tenFoldScores)\n",
    "\n",
    "# neg_mean_square_error is the negative MSE, so append a minus so the scores are positive.\n",
    "LOOCVScores = -cross_val_score(polyRegModel, X, y, scoring='neg_mean_squared_error', cv=len(X))\n",
    "\n",
    "print('\\nk-fold average MSE =', np.mean(LOOCVScores), '\\n', LOOCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "rng = np.random.RandomState(46)\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblollySample.csv')\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "trainDatasetName, testDatasetName = train_test_split(loblolly, test_size=0.25, random_state=rng)\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = trainDatasetName[['age']]\n",
    "y = trainDatasetName[['height']]\n",
    "\n",
    "# Initialize the model -- quadratic polynomial regression model\n",
    "polyFeatures = PolynomialFeatures(degree=2, include_bias=False)\n",
    "xPoly = polyFeatures.fit_transform(X)\n",
    "quadPolyRegModel = LinearRegression()\n",
    "\n",
    "# Evaluate accuracy\n",
    "# neg_mean_square_error is the negative MSE, so append a minus so the scores are positive.\n",
    "tenFoldScores = -cross_val_score(quadPolyRegModel, X, y, scoring='neg_mean_squared_error', cv=10) \n",
    "\n",
    "print('ten-fold average MSE =', np.mean(tenFoldScores), '\\n', tenFoldScores)\n",
    "\n",
    "# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.\n",
    "LOOCVScores = -cross_val_score(quadPolyRegModel, X, y, scoring='neg_mean_squared_error', cv=len(X))\n",
    "\n",
    "print('\\nk-fold average MSE =', np.mean(LOOCVScores), '\\n', LOOCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblollySample.csv')\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "trainDatasetName, testDatasetName = train_test_split(loblolly, test_size=0.3, random_state=rng)\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = trainDatasetName[['age']]\n",
    "y = trainDatasetName[['height']]\n",
    "\n",
    "# Initialize the model -- quadratic polynomial regression model\n",
    "polyFeatures = PolynomialFeatures(degree=2, include_bias=False)\n",
    "xPoly = polyFeatures.fit_transform(X)\n",
    "polyRegModel = LinearRegression()\n",
    "\n",
    "# Evaluate accuracy\n",
    "# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.\n",
    "tenFoldScores = -cross_val_score(polyRegModel, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "print('ten-fold average MSE =', np.mean(tenFoldScores), '\\n', tenFoldScores)\n",
    "\n",
    "# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.\n",
    "LOOCVScores = -cross_val_score(polyRegModel, X, y, scoring='neg_mean_squared_error', cv=len(X))\n",
    "\n",
    "print('\\nk-fold average MSE =', np.mean(LOOCVScores), '\\n', LOOCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rng = np.random.RandomState(27)\n",
    "\n",
    "# Load the dataset\n",
    "pines = pd.read_csv('pinesSample.csv')\n",
    "\n",
    "# Initialize the errors array\n",
    "bootstrapErrors = []\n",
    "\n",
    "# Generate the bootstrap samples\n",
    "for i in range(0, 39):\n",
    "    # Create the bootstrap sample and the out-of-bag Generate 39 bootstrap samples of size 26 using the resample() method with the following parameters: the dataset, replace set to True, n_samples set to the specified bootstrap sample size, and random_state set to rng.\n",
    "    bootstrap =  resample(pines, replace=True, n_samples=26, random_state=rng)\n",
    "    oob = pines[~pines.index.isin(bootstrap.index)]\n",
    "    \n",
    "    # Fit a linear model to the bootstrap sample\n",
    "    XBoot = bootstrap[['age']]\n",
    "    yBoot = bootstrap[['height']]\n",
    "    linModel = LinearRegression()\n",
    "    linModel.fit(XBoot, yBoot)\n",
    "    \n",
    "    # Make predictions\n",
    "    # Predict y values for the out-of-bag sample\n",
    "    XOob = oob[['age']]\n",
    "    YOob = oob[['height']]\n",
    "    YOobPredicted = linModel.predict(XOob)\n",
    "    \n",
    "    # Calculate and store the error\n",
    "    bootError = mean_squared_error(YOob, YOobPredicted)\n",
    "    bootstrapErrors.append(bootError)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "print('Mean of bootstrap errors =', np.mean(bootstrapErrors))\n",
    "print('Standard deviation of bootstrap errors =', np.std(bootstrapErrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rng = np.random.RandomState(19)\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblollySample.csv')\n",
    "\n",
    "# Initialize the errors array\n",
    "bootstrapErrors = []\n",
    "\n",
    "# Generate the bootstrap samples\n",
    "for i in range(0, 34):\n",
    "    # Create the bootstrap sample and the out-of-bag sample  The format is: datasetName[NOT datasetName.index.isin(bootstrapName.index)].\n",
    "    bootstrap = resample(loblolly, replace=True, n_samples=38, random_state=rng)\n",
    "    oob =   loblolly[~loblolly.index.isin(bootstrap.index)]\n",
    "    \n",
    "    # Fit a linear model to the bootstrap sample\n",
    "    XBoot = bootstrap[['age']]\n",
    "    yBoot = bootstrap[['height']]\n",
    "    linModel = LinearRegression()\n",
    "    linModel.fit(XBoot, yBoot)\n",
    "    \n",
    "    # Make predictions\n",
    "    # Predict y values for the out-of-bag sample\n",
    "    XOob = oob[['age']]\n",
    "    YOob = oob[['height']]\n",
    "    YOobPredicted = linModel.predict(XOob)\n",
    "    \n",
    "    # Calculate and store the error\n",
    "    bootError = mean_squared_error(YOob, YOobPredicted)\n",
    "    bootstrapErrors.append(bootError)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "print('Mean of bootstrap errors =', np.mean(bootstrapErrors))\n",
    "print('Standard deviation of bootstrap errors =', np.std(bootstrapErrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rng = np.random.RandomState(36)\n",
    "\n",
    "# Load the dataset\n",
    "loblolly = pd.read_csv('loblollySample.csv')\n",
    "\n",
    "# Initialize the errors array\n",
    "bootstrapErrors = []\n",
    "\n",
    "# Generate the bootstrap samples\n",
    "for i in range(0, 49):\n",
    "    # Create the bootstrap sample and the out-of-bag sample Generate 49 bootstrap samples of size 35 using the resample() method with the following parameters: the dataset, replace set to True, n_samples set to the specified bootstrap sample size, and random_state set to rng.\n",
    "    \n",
    "    # Your code goes here \n",
    "    bootSample = resample(loblolly, replace=True, n_samples=35, random_state=rng) \n",
    "    oob = loblolly[~loblolly.index.isin(bootSample.index)]\n",
    "    \n",
    "    # Fit a linear model to the bootstrap sample\n",
    "    XBoot = bootSample[['age']]\n",
    "    yBoot = bootSample[['height']]\n",
    "    linModel = LinearRegression()\n",
    "    linModel.fit(XBoot, yBoot)\n",
    "    \n",
    "    # Make predictions\n",
    "    # Predict y values for the out-of-bag sample\n",
    "    XOob = oob[['age']]\n",
    "    YOob = oob[['height']]\n",
    "    YOobPredicted = linModel.predict(XOob)\n",
    "    \n",
    "    # Calculate and store the error\n",
    "    bootError = mean_squared_error(YOob, YOobPredicted)\n",
    "    bootstrapErrors.append(bootError)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "print('Mean of bootstrap errors =', np.mean(bootstrapErrors))\n",
    "print('Standard deviation of bootstrap errors =', np.std(bootstrapErrors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nbaallelo_slr dataset contains information on 126315 NBA games between 1947 and 2015. The columns report the points made by one team, the Elo rating of that team coming into the game, the Elo rating of the team after the game, and the points made by the opposing team. The Elo rating measures the relative skill of teams in a league.\n",
    "\n",
    "The code creates a new column y in the data frame that is the difference between pts and opp_pts.\n",
    "Split the data into 70 percent training set and 30 percent testing set using sklearn's train_test_split function. Set random_state=0.\n",
    "Store elo_i and y from the training data as the variables X and y.\n",
    "The code performs a simple linear regression on X and y.\n",
    "Perform 10-fold cross-validation with the default scorer using scikit-learn's cross_val_score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "nba = pd.read_csv(\"nbaallelo_slr.csv\")\n",
    "\n",
    "#The code creates a new column y in the data frame that is the difference between pts and opp_pts.\n",
    "#Split the data into 70 percent training set and 30 percent testing set using sklearn's train_test_split function. Set random_state=0.\n",
    "#Store elo_i and y from the training data as the variables X and y.\n",
    "#The code performs a simple linear regression on X and y.\n",
    "#Perform 10-fold cross-validation with the default scorer using scikit-learn's cross_val_score function.\n",
    "#Ex: If random_state=1 is used, the output is:#\n",
    "# Create a new column in the data frame that is the difference between pts and opp_pts\n",
    "nba['y'] = nba['pts'] - nba['opp_pts']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train, test =  train_test_split(nba, test_size=0.25, random_state=42)\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X =   train[['elo_i']].values.reshape(-1, 1)\n",
    "y =   train[['y']].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the linear regression model\n",
    "SLRModel = LinearRegression()\n",
    "# Fit the model on X and y\n",
    "SLRModel.fit(X,y)\n",
    "\n",
    "# Perform 10-fold cross-validation with the default scorer\n",
    "tenFoldScores =  cross_val_score(SLRModel, X, y, cv=10)\n",
    "print('The cross-validation scores are', tenFoldScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Seed the random number generator\n",
    "rng = np.random.RandomState(32)\n",
    "\n",
    "# Read in the data\n",
    "hawkExample = pd.read_csv('hawkExample.csv')\n",
    "\n",
    "# Encode sex as a dummy variable\n",
    "hawkExampleWithDummy = pd.get_dummies(hawkExample, drop_first=True)\n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = hawkExampleWithDummy['Hallux']\n",
    "X = hawkExampleWithDummy.drop('Hallux', axis=1)\n",
    "\n",
    "# Define model\n",
    "hawkRT = DecisionTreeRegressor(max_depth=2, min_samples_leaf=3, random_state=rng)\n",
    "\n",
    "# Fit the model\n",
    "hawkRT.fit(X, y)\n",
    "\n",
    "# Print regression tree\n",
    "print(export_text(hawkRT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Seed the random number generator\n",
    "rng = np.random.RandomState(8)\n",
    "\n",
    "# Read in the data\n",
    "hawkExample = pd.read_csv('hawkExample.csv')\n",
    "\n",
    "# Encode sex as a dummy variable\n",
    "hawkExampleWithDummy = pd.get_dummies(hawkExample, drop_first=True)\n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = hawkExampleWithDummy['Culmen']\n",
    "X = hawkExampleWithDummy.drop('Culmen', axis=1)\n",
    "\n",
    "# Initialize the model\n",
    "hawkRT = DecisionTreeRegressor(max_depth=3, min_samples_leaf=2, random_state=rng)\n",
    "\n",
    "# Fit the model\n",
    "hawkRT.fit(X, y)\n",
    "\n",
    "# Print regression tree info\n",
    "print('max_depth = ' + str(hawkRT.max_depth) + ', ' + str(hawkRT.random_state))\n",
    "\n",
    "# Print regression tree\n",
    "print(export_text(hawkRT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "penguins = pd.read_csv('raptorExample.csv')\n",
    "penguinsWithDummies = pd.get_dummies(penguins, drop_first=True)\n",
    "\n",
    "# Assign outcome to y and features to X  Define the outcome and features to predict Wing. Assign the outcome to y and features to X.\n",
    "\n",
    "# Your code goes here\n",
    "y =  penguinsWithDummies['Wing']\n",
    "X =  penguinsWithDummies.drop('Wing', axis=1)\n",
    "\n",
    "# Initialize the model\n",
    "raptorRT = DecisionTreeRegressor(max_depth=3, min_samples_leaf=2, random_state=rng)\n",
    "\n",
    "# Fit the model\n",
    "raptorRT.fit(X, y)\n",
    "\n",
    "\n",
    "# Print regression tree info\n",
    "print('max_depth = ' + str(raptorRT.max_depth) + ', ' + str(raptorRT.random_state))\n",
    "\n",
    "# Print regression tree\n",
    "print(export_text(raptorRT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(8)\n",
    "\n",
    "# Load the dataset\n",
    "birdOfPrey = pd.read_csv('birdOfPrey_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = birdOfPrey['Age']\n",
    "X = birdOfPrey.drop('Age', axis=1)\n",
    "\n",
    "# Your code goes here\n",
    "birdOfPreyCT = DecisionTreeClassifier(max_depth=4, random_state=rng)\n",
    "\n",
    "# Fit the model\n",
    "birdOfPreyCT.fit(X,y)\n",
    "\n",
    "# Print classification tree\n",
    "print(export_text(birdOfPreyCT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(19)\n",
    "\n",
    "# Load the dataset\n",
    "birdOfPrey = pd.read_csv('birdOfPrey_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = birdOfPrey['Age']\n",
    "X = birdOfPrey.drop('Age', axis=1)\n",
    "\n",
    "# Initialize the model -- decision tree classifier Initialize the model using \n",
    "#the DecisionTreeClassifier() type of classification tree with min_samples_split of 2, min_samples_leaf of 5,\n",
    "# max_depth of 3, and the random number generator random_state set to rng.\n",
    "\n",
    "\n",
    "# Your code goes here\n",
    "birdOfPreyCT = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=rng)\n",
    "\n",
    "# Fit the model\n",
    "birdOfPreyCT.fit(X,y)\n",
    "\n",
    "\n",
    "# Print classification tree\n",
    "print(export_text(birdOfPreyCT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) A random forest regressor gives the average result from all the trees as the predicted value of an instance, not the majority result.\n",
    "\n",
    "(2) The classification that a majority of the trees report for an instance is given as the result of the entire random forest.\n",
    "\n",
    "(3), (4) A random forest selects a sample the same size as the training set, but chooses random instances from the training set with replacement, meaning that the sample will have many duplicate values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "predictions = [10661, 10942, 5861, 13935, 10376, 7186, 10664]\n",
    "random_forest_prediction = round(sum(predictions) / len(predictions))\n",
    "random_forest_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(38)\n",
    "\n",
    "# Load the dataset\n",
    "raptor = pd.read_csv('raptor_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = raptor['Species']\n",
    "X = raptor.drop('Species', axis=1)\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=.3, random_state=rng)\n",
    "\n",
    "# Initialize the model -- random forest classification trees\n",
    "#Initialize the model using the RandomForestClassifier() type of classification tree. The model's parameters should be set as follows:\n",
    "#n_estimators to 62\n",
    "#max_features to at most the square root of the number of features (sqrt) considered for the best split\n",
    "##criterion to the gini quality measure for splitting\n",
    "#bootstrap to True to build trees with sample data\n",
    "#random_state to rng\n",
    "\n",
    "raptorRFC =     RandomForestClassifier(n_estimators=62, max_features='sqrt', criterion='gini',\n",
    "                                        bootstrap=True, random_state=rng)\n",
    "\n",
    "# Fit the model with training data\n",
    "raptorRFC = raptorRFC.fit(XTrain, yTrain)\n",
    "\n",
    "# Print first and last random trees generated in the forest\n",
    "print('First tree:')\n",
    "print(export_text(raptorRFC[0], feature_names=X.columns.to_list()))\n",
    "print('Last tree:')\n",
    "print(export_text(raptorRFC[62-1], feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(6)\n",
    "\n",
    "# Load the dataset\n",
    "hawk = pd.read_csv('hawk_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = hawk['Species']\n",
    "X = hawk.drop('Species', axis=1)\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=.3, random_state=rng)\n",
    "\n",
    "# Initialize the model -- random forest classifier\n",
    "hawkRFC = RandomForestClassifier(n_estimators=68, max_features='sqrt', criterion='gini', bootstrap=True, random_state=rng)\n",
    "\n",
    "# Fit the model with training data\n",
    "hawkRFC = hawkRFC.fit(XTrain, yTrain)\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "# Print first and last random trees generated in the forest\n",
    "print('First tree:')\n",
    "print(export_text(hawkRFC[0], feature_names=X.columns.to_list()))\n",
    "print('Last tree:')\n",
    "print(export_text(hawkRFC[68-1], feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(19)\n",
    "\n",
    "# Load the dataset\n",
    "raptor = pd.read_csv('raptor_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = raptor['Species']\n",
    "X = raptor.drop('Species', axis=1)\n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.25, random_state=rng)\n",
    "\n",
    "# Your code goes here\n",
    "# Initialize the model -- random forest classifier\n",
    "raptorRFC =  RandomForestClassifier(n_estimators=51, max_features='sqrt', criterion='gini',\n",
    "                                        bootstrap=True, random_state=rng)\n",
    "\n",
    "# Fit the model with training data\n",
    "raptorRFC = raptorRFC.fit(XTrain, yTrain)\n",
    "\n",
    "# Print first and last random trees generated in the forest\n",
    "print('First tree:')\n",
    "print(export_text(raptorRFC[0], feature_names=X.columns.to_list()))\n",
    "print('Last tree:')\n",
    "print(export_text(raptorRFC[51-1], feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_text  \n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(35)\n",
    "\n",
    "# Load the dataset\n",
    "birdOfPrey = pd.read_csv('birdOfPrey_Example.csv') \n",
    "\n",
    "# Assign outcome to y and features to X \n",
    "y = birdOfPrey['Hallux']\n",
    "X = birdOfPrey.drop('Hallux', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True) # categorical maps to numerical \n",
    "\n",
    "# Split dataset into training data and testing data\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.25, random_state=rng)\n",
    "\n",
    "# Your code goes here\n",
    "# Initialize the model -- random forest regression trees\n",
    "birdOfPreyRFR = RandomForestRegressor(n_estimators=26, max_features='sqrt', random_state=rng)\n",
    "\n",
    "# Fit the model to training data\n",
    "birdOfPreyRFR = birdOfPreyRFR.fit(XTrain, yTrain)\n",
    "\n",
    "\n",
    "# Print a random tree generated in the forest\n",
    "print('Random tree:')\n",
    "print(export_text(birdOfPreyRFR[12], feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Load the mpg dataset Load the mpg.csv dataset\n",
    "mpg = pd.read_csv('mpg.csv')\n",
    "\n",
    "# Subset the data containing weight and model_year\n",
    "X = mpg[['weight', 'model_year']]\n",
    "\n",
    "# Subset the data containing mpg\n",
    "y = mpg['mpg']\n",
    "\n",
    "# Initialize a regression tree with random_state = 100\n",
    "# that has depth 3 and a minimum number of samples in each leaf of 5\n",
    "mpgRT = DecisionTreeRegressor(max_depth=3, min_samples_leaf=5, random_state=100)\n",
    "\n",
    "# Fit the X and y data\n",
    "mpgRT = mpgRT.fit(X, y)\n",
    "\n",
    "# Print regression tree\n",
    "print(\"max_depth = %s, %s\"% (mpgRT.max_depth, mpgRT.random_state))\n",
    "# Print tree\n",
    "print(export_text(mpgRT, feature_names=X.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05038266246564986"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "points_mean = 1300.18 \n",
    "points_std_dev =  16.27544\n",
    "points = 1301\n",
    "\n",
    "standardized_points = (points - points_mean) / points_std_dev\n",
    "standardized_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Read in the data\n",
    "wines = pd.read_csv('whitewine.csv')\n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(46)\n",
    "\n",
    "# Initialize k-means clustering model\n",
    "kmeansModel = KMeans(n_clusters=5, random_state=rng)\n",
    "\n",
    "# Your code goes here\n",
    "# Fit the model Fit the k-means clustering model to cluster wines based on pH (pH) and alcohol concentration (alcohol).\n",
    "kmeansModel = kmeansModel.fit(wines[['pH', 'alcohol']])\n",
    "\n",
    "print(kmeansModel.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Read in the data\n",
    "wines = pd.read_csv('whitewine.csv')\n",
    "\n",
    "# Seed random number generator\n",
    "rng = np.random.RandomState(23)\n",
    "\n",
    "#Initialize a k-means clustering model with n_clusters=5 and random_state=rng.\n",
    "kmeansModel = KMeans(n_clusters=5, random_state=rng)\n",
    "#Fit the model to cluster wines based on chlorides (chlorides) and quality (quality).\n",
    "kmeansModel = kmeansModel.fit(wines[['chlorides', 'quality']])\n",
    "#Print the within cluster sums of squares for the fitted model.\n",
    "print(kmeansModel.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A cluster contains points (1, -9) and (-7, -1).Centroid's location: \n",
    "x = (1-7)/2 = -3\n",
    "y = (-9-1)/2 = -5\n",
    "#A third point (-1, 8) is added to the cluster.\n",
    "#Updated centroid's location:\n",
    "x = (1-7-1)/3 = -7/3 = -2.33\n",
    "y = (-9-1+8)/3 = -2/3 = -0.67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the given graph, the centroid of cluster 0 is (16, 3) \n",
    "#and the centroid of cluster 1 is (41, 21.5). \n",
    "#Point A is at (34, 22).\n",
    "#What is the distance between the centroid of cluster 0 and point A?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A cluster contains points (9, -3) and (5, -10).Centroid's location:\n",
    "x = (9+5)/2 = 7\n",
    "y = (-3-10)/2 = -6.5\n",
    "#A third point (1, 6) is added to the cluster.Centroid's updated location: \n",
    "x = (9+5+1)/3 = 5\n",
    "y = (-3-10+6)/3 = -7/3 = -2.33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.515301344262525\n",
      "9.219544457292887\n"
     ]
    }
   ],
   "source": [
    "#In the given graph, the centroid of cluster 0 is (12, 5) and the centroid of cluster 1 is (38, 19). \n",
    "#Point A is at (36, 10).\n",
    "# Create a functon that calcule at the distance between two points given the coordinates as (x1, y1)\n",
    "# and (x2, y2)\n",
    "\n",
    "import numpy as np\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "print(euclidean_distance(12, 5, 36, 10))\n",
    "print(euclidean_distance(38, 19, 36, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17034\\AppData\\Local\\Temp\\ipykernel_6204\\54601630.py:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  distance = np.sqrt((38-25)^2 + (19-22)^2) #= sqrt(13^2 + 3^2) = sqrt(178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Researchers studying chemical properties of wines collected data on a sample of white wines in Northern Portugal. A research goal was to cluster wines based on similar chemical properties.\n",
    "\n",
    "#Cluster wines with single linkage.\n",
    "#The code provided creates a dataframe with two features (density and free_sulfur_dioxide), normalizes the dataframe, creates a distance matrix, and displays the cluster membership of each data point.\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine = pd.read_csv('wine1.csv')\n",
    "\n",
    "# Calculate a distance matrix with selected variables\n",
    "X = wine[['density', 'free_sulfur_dioxide']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# pdist() calculates pairs of distances between each instance in the dataset\n",
    "dist = pdist(X)\n",
    "\n",
    "# linkage() calculates the distance between clusters\n",
    "linkage_matrix = linkage(dist, method='single')\n",
    "\n",
    "\n",
    "print(clusterModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Researchers studying chemical properties of wines collected data on a sample of white wines in Northern Portugal. A research goal was to cluster wines based on similar chemical properties.\n",
    "#Using pdist(), calculate a distance matrix for wines. The matrix of input features, X, has already been created.\n",
    "\n",
    "#Use the distance matrix to cluster the wines with centroid linkage.\n",
    "\n",
    "#The code provided creates a dataframe with two features (volatile_acidity and sulphates), normalizes the dataframe, and displays the cluster membership of each data point.\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine = pd.read_csv('wine1.csv')\n",
    "\n",
    "# Calculate a distance matrix with selected variables\n",
    "X = wine[['volatile_acidity', 'sulphates']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "print(clusterModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine = pd.read_csv('wine1.csv')\n",
    "\n",
    "# Create an input matrix with selected features\n",
    "X = wine[['fixed_acidity', 'citric_acid']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# Cluster using DBSCAN with default options\n",
    "dbscanModel = DBSCAN()\n",
    "\n",
    "# Fit the model\n",
    "dbscanModel = dbscanModel.fit(X)\n",
    "\n",
    "print(dbscanModel.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine = pd.read_csv('wine1.csv')\n",
    "\n",
    "# Create an input matrix with selected features\n",
    "X = wine[['chlorides', 'sulphates']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# Your code goes hereUse the DBSCAN clustering function to cluster wines. Set eps=0.79 and min_samples=5.\n",
    "dbscanModel = DBSCAN(eps=0.79, min_samples=5)\n",
    "#Fit the DBSCAN model to cluster wines.\n",
    "dbscanModel = dbscanModel.fit(X)\n",
    "\n",
    "\n",
    "print(dbscanModel.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8586 0.009800000000000031 -0.014799999999999924 0.5942000000000001\n",
      "0.44210000000000005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07830000000000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create function for communality is the sum of the squares of the factor loadings for a given feature.\n",
    "\n",
    "#X1 - μ₁ = -0.26F1 - 0.27F2 + 0.03F3\n",
    "#X2 - μ2 = 0.29F1 -0.95F2 + 0.06F3\n",
    "#X3 - μ3 = 0.98F1 - 0.12F2 - 0.20F3\n",
    "#X4 - μ₁ = -0.41F₁ - 0.21F2 - 0.44F3\n",
    "\n",
    "#Which factor has the most influence on hours worked?\n",
    "#Which factor has the least influence on hours worked?\n",
    "#Which factor has the most influence on hours worked?\n",
    "\n",
    "#The communality of a feature is the sum of the squares of the factor loadings for that feature.\n",
    "#The code provided calculates the communality of each feature based on the factor loadings.\n",
    "import numpy as np\n",
    "\n",
    "#-0.26F1 - 0.27F2 + 0.03F3\n",
    "#0.29F1 -0.95F2 + 0.06F3\n",
    "#0.98F1 - 0.12F2 - 0.20F3\n",
    "#-0.41F₁ - 0.21F2 - 0.44F3\n",
    "communality1 = 1-  ((-0.26)**2 + (-0.27)**2 + (0.03)**2)\n",
    "communality2 = 1- ((0.29)**2 + (-0.95)**2 + (0.06)**2)\n",
    "communality3 = 1- ( (0.98)**2 + (-0.12)**2 + (-0.20)**2)\n",
    "communality4 = 1- ((-0.41)**2 + (-0.21)**2 + (-0.44)**2)\n",
    "\n",
    "print(communality1, communality2, communality3, communality4)\n",
    "\n",
    "#X1 - μ₁ = 0.65F1+0.14F2\n",
    "\n",
    "Communality = (0.65**2 + 0.14**2)\n",
    "print(Communality)\n",
    "\n",
    "#0.36F1 + 0.89F2\n",
    "\n",
    "Communality = (0.36**2 + 0.89**2)\n",
    "1-Communality\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083333333333332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A dataset contains information on six lifestyle measures for 44 major cities around the world. Features in the dataset include cost of a water bottle, life expectancy, pollution, hours worked, obesity, and happiness levels.\n",
    "\n",
    "#A principal components model was fitted to the dataset. The eigenvalues for each factor are listed in the following table. How much of the total variance is explained by the second factor?\n",
    "#Eigenvalue\t3.42\t1.11\t0.82\t0.38\t0.25\t0.02\n",
    "\n",
    "#The total variance is the sum of the eigenvalues.\n",
    "#The variance explained by the second factor is the second eigenvalue.\n",
    "#The proportion of the total variance explained by the second factor is the second eigenvalue divided by the total variance.\n",
    "#The code provided calculates the proportion of the total variance explained by the second factor.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(3.47 +\t1.38)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine_data = pd.read_csv('wine_data.csv')\n",
    "\n",
    "#Create a dataframe, X, that contains three features in the following order: residual_sugar, density, and fixed_acidity.\n",
    "X = wine_data[['residual_sugar', 'density', 'fixed_acidity']]\n",
    "#Create a dataframe, y, that contains the quality feature.\n",
    "\n",
    "\n",
    "print(X.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wines = pd.read_csv('wines.csv')\n",
    "\n",
    "X = wines[['chlorides', 'pH', 'free_sulfur_dioxide', 'citric_acid']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "\n",
    "# Your code goes here Fit the principal components model to the dataframe X.\n",
    "model = model.fit(X)\n",
    "\n",
    "print(model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "mammalSleep =  pd.read_csv('mammalSleep.csv')\n",
    "\n",
    "# Clean the data\n",
    "mammalSleep = mammalSleep.dropna()\n",
    "\n",
    "# Create a dataframe with the columns sleep_total and sleep_cycle\n",
    "X =  mammalSleep[['sleep_total', 'sleep_cycle']]\n",
    "\n",
    "# Initialize a k-means clustering model with 4 clusters and random_state = 0\n",
    "km =  KMeans(n_clusters=4, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "mammalSleepKm = km.fit(X)\n",
    "\n",
    "# Find the centroids of the clusters\n",
    "mammalSleepCentroids = \n",
    "\n",
    "print(mammalSleepCentroids)\n",
    "\n",
    "# Predict the cluster for each data point in mammal_sleep\n",
    "mammalSleep['cluster'] =  km.predict(X)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Graph the clusters\n",
    "plt.scatter(mammalSleep['sleep_total'], mammalSleep['sleep_cycle'], c=mammalSleep['cluster'], cmap='viridis')\n",
    "\n",
    "plt.xlabel('Total sleep', fontsize=14)\n",
    "plt.ylabel('Length of sleep cycle',fontsize=14)\n",
    "plt.savefig('msleep_clusters.png')\n",
    "\n",
    "WCSS = []\n",
    "k = [1,2,3,4,5]\n",
    "for j in k:\n",
    "    km = KMeans(n_clusters = j)\n",
    "    mammalSleepKmWCSS = km.fit(X)\n",
    "    intermediateWCSS = # find the within-cluster sum of squares\n",
    "    WCSS.append(round(intermediateWCSS,1))\n",
    "    \n",
    "print(WCSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
