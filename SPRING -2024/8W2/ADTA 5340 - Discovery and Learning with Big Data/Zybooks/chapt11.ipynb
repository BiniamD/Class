{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "mammalSleep =pd.read_csv('msleep.csv')\n",
    "\n",
    "# Clean the data\n",
    "mammalSleep = mammalSleep.dropna()\n",
    "\n",
    "# Create a dataframe with the columns sleep_total and sleep_cycle\n",
    "X = mammalSleep[['sleep_total', 'sleep_cycle']]\n",
    "\n",
    "# Initialize a k-means clustering model with 4 clusters and random_state = 0\n",
    "km =KMeans(n_clusters=4, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "mammalSleepKm = km.fit(X)\n",
    "\n",
    "# Find the centroids of the clusters\n",
    "mammalSleepCentroids = mammalSleepKm.cluster_centers_\n",
    "print(mammalSleepCentroids)\n",
    "\n",
    "# Predict the cluster for each data point in mammal_sleep\n",
    "mammalSleep['cluster'] = mammalSleepKm.predict(x)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Graph the clusters\n",
    "plt.scatter(mammalSleep['sleep_total'], mammalSleep['sleep_cycle'], c=mammalSleep['cluster'], cmap='viridis')\n",
    "\n",
    "plt.xlabel('Total sleep', fontsize=14)\n",
    "plt.ylabel('Length of sleep cycle',fontsize=14)\n",
    "plt.savefig('msleep_clusters.png')\n",
    "\n",
    "WCSS = []\n",
    "k = [1,2,3,4,5]\n",
    "for j in k:\n",
    "    km = KMeans(n_clusters = j)\n",
    "    mammalSleepKmWCSS = km.fit(X)\n",
    "    intermediateWCSS =  km.inertia_\n",
    "    WCSS.append(round(intermediateWCSS,1))\n",
    "    \n",
    "print(WCSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Read in forestfires.csv\n",
    "fires = pd.read_csv('forestfires.csv')\n",
    "\n",
    "# Create a new data frame with the columns FFMC, DMC, DC, ISI, temp, RH, wind, and rain, in that order\n",
    "X = fires[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']]\n",
    "# Calculate the correlation matrix for the data in the data frame X\n",
    "XCorr = X.corr()\n",
    "print(XCorr)\n",
    "\n",
    "# Scale the data.\n",
    "scaler =  StandardScaler()\n",
    "firesScaled =  scaler.fit_transform(X)\n",
    "\n",
    "# Perform four-component factor analysis on the scaled data.\n",
    "pca = PCA(n_components=4)\n",
    "firesPCA = pca.fit(firesScaled)\n",
    "\n",
    "# Print the factors and the explained variance.\n",
    "print(\"Factors: \", firesPCA.components_)\n",
    "      \n",
    "\n",
    "print(\"Explained variance: \", firesPCA.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine = pd.read_csv('wine1.csv')\n",
    "\n",
    "# Calculate a distance matrix with selected variables\n",
    "X = wine[['total_sulfur_dioxide', 'sulphates']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# pdist() calculates pairs of distances between each instance in the dataset\n",
    "dist = pdist(X)\n",
    "\n",
    "# linkage() performs hierarchical clustering on the condensed distance matrix\n",
    "clusterModel = linkage(dist)\n",
    "\n",
    "print(clusterModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-.5*12\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "heart = pd.read_csv('heart.csv')\n",
    "\n",
    "# Slices the features of the dataset\n",
    "X = heart[['trestbps', 'age', 'thalach']]\n",
    "y = heart[['target']]\n",
    "\n",
    "# Scales the features\n",
    "scaler = StandardScaler()\n",
    "XScaled = pd.DataFrame(scaler.fit_transform(X), columns=['trestbps','age','thalach'])\n",
    "\n",
    "# Splits the data into train and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(XScaled, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initializes and fits a perceptron model Initialize a perceptron model percModel with a maximum of 2000 epochs and a learning rate of 0.1.\n",
    "percModel =  Perceptron(max_iter=2000, eta0=0.1)\n",
    "percModel.fit(XTrain, np.ravel(yTrain))\n",
    "\n",
    "print(percModel.coef_)\n",
    "print(percModel.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "heart = pd.read_csv('heart.csv')\n",
    "\n",
    "# Slices the features of the dataset\n",
    "X = heart[['trestbps', 'age', 'thalach']]\n",
    "y = heart[['target']]\n",
    "\n",
    "# Scales the features\n",
    "scaler = StandardScaler()\n",
    "XScaled = pd.DataFrame(scaler.fit_transform(X), columns=['trestbps', 'age', 'thalach'])\n",
    "\n",
    "# Splits the data into train and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(XScaled, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initializes a perceptron model\n",
    "clf = Perceptron(eta0=0.15, max_iter=1500)\n",
    "\n",
    "# Fits a perceptron model\n",
    "clf.fit(XTrain, np.ravel(yTrain))\n",
    "\n",
    "# Finds the weights\n",
    "print( clf.coef_)\n",
    "print(  clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "heart = pd.read_csv('heart.csv')\n",
    "\n",
    "# Slices the features of the dataset\n",
    "X = heart[['trestbps', 'age', 'thalach']]\n",
    "y = heart[['target']]\n",
    "\n",
    "# Scales the features\n",
    "scaler = StandardScaler()\n",
    "XScaled = pd.DataFrame(scaler.fit_transform(X), columns=['trestbps', 'age', 'thalach'])\n",
    "\n",
    "# Splits the data into train and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(XScaled, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initializes a perceptron model\n",
    "pModel = Perceptron(eta0=0.15, max_iter=2000)\n",
    "\n",
    "# Fits a perceptron model\n",
    "pModel.fit(XTrain, np.ravel(yTrain))\n",
    "\n",
    "# Creates a list of predictions from the test features\n",
    "yPredicted = pModel.predict(XTest)\n",
    "\n",
    "# Prints accuracy score\n",
    "print(accuracy_score(yTest, yPredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "# Load input into a dataframe\n",
    "NBA = pd.read_csv(input())\n",
    "\n",
    "# Hot encode the game_result variable as a numeric variable with 0 for L and 1 for W\n",
    "NBA.loc[NBA['game_result']=='L','game_result']=0\n",
    "NBA.loc[NBA['game_result']=='W','game_result']=1\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = NBA[['pts','elo_i','win_equiv']]\n",
    "y = NBA[['game_result']].astype(int)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "XScaled =  pd.DataFrame(scaler.fit_transform(X), columns=['pts','elo_i','win_equiv'])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(XScaled, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Initialize a perceptron model with a learning rate of 0.05 and 20000 epochs\n",
    "classifyNBA =  Perceptron(eta0=0.05, max_iter=20000)\n",
    "# Fit the perceptron model\n",
    "classifyNBA.fit(XTrain, np.ravel(yTrain))\n",
    "\n",
    "# Create a list of predictions from the test features\n",
    "yPred = classifyNBA.predict(XTest)\n",
    "\n",
    "# Find the weights for the input variables\n",
    "weightVar = classifyNBA.coef_\n",
    "print(weightVar)\n",
    "\n",
    "# Find the weights for the bias term\n",
    "weightBias =  classifyNBA.intercept_\n",
    "print(weightBias)\n",
    "\n",
    "# Find the accuracy score\n",
    "score = accuracy_score(yTest, yPred)\n",
    "print('%.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37750000000000006"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.84 +0.2+0.07+0.4)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('avila.csv').dropna()\n",
    "\n",
    "# Create input features X and output feature y\n",
    "X = data[['ic_distance', 'upper', 'lower', 'exploitation', 'rows',\n",
    "         'modular', 'line_spacing', 'weight', 'peak_num', \n",
    "         'modular_line']]\n",
    "y = data[['class']]\n",
    "\n",
    "# Initialize the model\n",
    "boostModel = AdaBoostClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "boostModel = boostModel.fit(X, np.ravel(y))\n",
    "boostModel.score(X, y)\n",
    "\n",
    "print('Classification accuracy:', boostModel.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('avila.csv').dropna()\n",
    "\n",
    "# Create input features X and output feature y\n",
    "X = data[['ic_distance', 'upper', 'lower', 'exploitation', 'rows',\n",
    "         'modular', 'line_spacing', 'weight', 'peak_num', \n",
    "         'modular_line']]\n",
    "y = data[['class']]\n",
    "\n",
    "# Initialize the model Initialize an adaptive boosting classification ensemble, boostModel, with up to 70 Gaussian Naive Bayes base models and for predicting whether or not a certain copyist transcribed a page.\n",
    "# Your code goes here\n",
    "boostModel = AdaBoostClassifier(base_estimator=GaussianNB(), n_estimators=70, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "boostModel = boostModel.fit(X, np.ravel(y))\n",
    "boostModel.score(X, y)\n",
    "\n",
    "print('Classification accuracy:', boostModel.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('avila.csv').dropna()\n",
    "\n",
    "# Create input features X and output feature y\n",
    "X = data[['ic_distance', 'upper', 'lower', 'exploitation', 'rows',\n",
    "         'modular', 'line_spacing', 'weight', 'peak_num',           \n",
    "         'modular_line']]\n",
    "y = data[['class']]\n",
    "\n",
    "# Initialize the model\n",
    "boostModel = AdaBoostClassifier()\n",
    "\n",
    "# Fit the model and calculate predicted probabilities\n",
    "boostModel = boostModel.fit(X, np.ravel(y))\n",
    "probs = boostModel.predict_proba(X)\n",
    "\n",
    "print('Predicted probabilities:', probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('avila.csv').dropna()\n",
    "\n",
    "# Create input features X and output feature y\n",
    "X = data[['ic_distance', 'upper', 'lower', 'exploitation', 'rows',\n",
    "         'modular', 'line_spacing', 'weight', 'peak_num', \n",
    "         'modular_line']]\n",
    "y = data[['class']]\n",
    "\n",
    "# Initialize the model\n",
    "baggingModel = BaggingClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "baggingModel = baggingModel.fit(X, np.ravel(y))\n",
    "baggingModel.score(X, y)\n",
    "\n",
    "print('Classification accuracy:', baggingModel.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
